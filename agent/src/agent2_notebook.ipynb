{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk--lxq9ReUmWxgxJhWPPwRNg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import litellm\n",
    "from litellm import completion\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load API keys\n",
    "load_dotenv(\".env\")\n",
    "apikey = os.getenv('OPENAI_API_KEY')\n",
    "print(apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Request Successful\n",
      "Response: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3ARetrieval%20Augmented%20Generation%26id_list%3D%26start%3D0%26max_results%3D5\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=all:Retrieval Augmented Generation&amp;id_list=&amp;start=0&amp;max_results=5</title>\n",
      "  <id>http://arxiv.org/api/UOKvU43HASfzk48jVrFyBsmll1A</id>\n",
      "  <updated>2024-05-23T00:00:00-04:00</updat\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Example query to the arXiv API directly\n",
    "url = \"http://export.arxiv.org/api/query\"\n",
    "params = {\n",
    "    \"search_query\": \"all:Retrieval Augmented Generation\",\n",
    "    \"start\": 0,\n",
    "    \"max_results\": 5\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"API Request Successful\")\n",
    "    print(\"Response:\", response.text[:500])  # Print first 500 characters of the response\n",
    "else:\n",
    "    print(\"Failed to fetch data from arXiv:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='1\\nPulmonary Disease Classiﬁcation Using Globally\\nCorrelated Maximum Likelihood:\\nan Auxiliary Attention mechanism for\\nConvolutional Neural Networks\\nEdward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, and Faraz Hussain\\nAbstract—Convolutional neural networks (CNN) are now being\\nwidely used for classiﬁying and detecting pulmonary abnormal-\\nities in chest radiographs. Two complementary generalization\\nproperties of CNNs, translation invariance and equivariance,\\nare particularly useful in detecting manifested abnormalities\\nassociated with pulmonary disease, regardless of their spatial\\nlocations within the image. However, these properties also come\\nwith the loss of exact spatial information and global relative\\npositions of abnormalities detected in local regions. Global\\nrelative positions of such abnormalities may help distinguish\\nsimilar conditions, such as COVID-19 and viral pneumonia. In\\nsuch instances, a global attention mechanism is needed, which\\nCNNs do not support in their traditional architectures that\\naim for generalization afforded by translation invariance and\\nequivariance. Vision Transformers provide a global attention\\nmechanism, but lack translation invariance and equivariance,\\nrequiring signiﬁcantly more training data samples to match\\ngeneralization of CNNs. To address the loss of spatial information\\nand global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that\\nserves as an auxiliary attention mechanism to existing CNN\\narchitectures, in order to extract global correlations between\\nsalient features.\\nImpact Statement—We improve sensitivity of Convolutional', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='serves as an auxiliary attention mechanism to existing CNN\\narchitectures, in order to extract global correlations between\\nsalient features.\\nImpact Statement—We improve sensitivity of Convolutional\\nNeural Networks (CNNs) using an auxiliary global attention\\nmechanism (GCML) that enables CNNs to utilize global spatial\\ninformation similar to Vision Transformers (ViTs). Our tech-\\nnique retains the beneﬁts of spatial invariance and equivariance\\ninherent to CNNs, while allowing spatial information of features\\nto be used as discriminators. GCML retains these inductive\\nbiases in data starved environments, which ViTs lack due their\\narchitecture, and hence require signiﬁcantly more training data\\nto achieve a similar level of generalization. Finally, we show\\nimpirically, that GCML improves the sensitivity of standard\\nCNNs when classifying pulmonary conditions in chest X-rays. We\\nprovide all associated code, data, and models for reproducibility\\nand improvement through further research.\\nIndex Terms—COVID-19 detection, convolutional neural net-\\nworks, global attention mechanism, data starved environment.\\nI. INTRODUCTION\\nW\\nITH the emergence of the COVID-19 pandemic, the\\nuse of Convolutional Neural Networks (CNNs) to\\ndetect presence of pulmonary diseases in medical imagery\\nhas quickly gained momentum, where a signiﬁcant majority\\nof approaches center around ﬁne-tuning pre-trained CNNs on', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='detect presence of pulmonary diseases in medical imagery\\nhas quickly gained momentum, where a signiﬁcant majority\\nof approaches center around ﬁne-tuning pre-trained CNNs on\\nnew data, as reported by Roberts et al. [1]. The beneﬁts of\\nutilizing such techniques are intuitive, including the ability\\nof CNNs to detect features that are difﬁcult for humans to\\nidentify, learning certain correlations between positive cases,\\nand the speed with which such predictions can be made in\\norder to aid in timely diagnosis. CNNs have been shown\\nto outperform individual radiologists in detecting pneumonia\\nin chest X-rays as reported by Rajpurkar et al. [2]. In that\\nwork, X-ray based pneumonia diagnoses made by a group\\nof four radiologists were compared to a custom CNN, using\\nthe F1 metric, where only one radiologist performed better\\nthan the model. This result prompted us to explore a possible\\nmechasism of visual analysis of X-rays by a radiologist who\\noutperformed the CNN, and whether that might translate into\\nmore accurate CNNs for classiﬁcation of pulmonary diseases.\\nOur hypothesis is that while human radiologists may not\\nbe as effective as CNNs at identifying individual salient fea-\\ntures, their ability to quickly consider global spatial relations\\nbetween those features may play a factor. Our intution for\\nthis came from recent work by Borghesi et al. [3], where\\nan experimental scoring system for chest X-rays was used\\nby radiologists to quantify and monitor disease progression', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='this came from recent work by Borghesi et al. [3], where\\nan experimental scoring system for chest X-rays was used\\nby radiologists to quantify and monitor disease progression\\nin COVID-19 patients. The scoring worked by dividing the\\nfrontal X-ray of the lungs into six zones, where each zone\\nwas assessed with a quantitative score ranging from 0, rep-\\nresenting no lung abnormalities, through 3, representing the\\nhighest severity of abnormalities. To obtain the ﬁnal score,\\neach region’s scores were summed, and used as a measure\\nof COVID-19 severity on the lungs. Our idea is to enable\\nthe CNN model to track abnormalities and their relations to\\neach other across regions, and base our predictions on these\\nlearned spatial relationships and not just a cummulative score,\\nsimilar to what a human subject matter expert might do to\\ndistinguish different diseases.\\nIn the remainder of this section, we brieﬂy describe how im-\\nage classiﬁcation using CNNs works, including their strengths\\nand limitations, with the goal to introduce the notion of\\nadditive classiﬁcation that we argue standard CNNs perform.\\nWe then discuss how certain positive generalization properties\\nof CNNs limit their ability to account for global spatial\\ncorrelations between regions of an image, thereby lacking the\\nability to utilize positional information as a discriminating\\nfactor.\\narXiv:2109.00573v1  [cs.CV]  1 Sep 2021\\n2\\n(a) CNN Equavariance\\n(b) CNN Invariance', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='factor.\\narXiv:2109.00573v1  [cs.CV]  1 Sep 2021\\n2\\n(a) CNN Equavariance\\n(b) CNN Invariance\\n(d) GCML Attention\\n(c) CNN Multi-object\\nFig. 1: Examples of inductive biases inherent to Convolutional Neural Networks (a,b,c). Small translational shifts in a target\\nobject (black circle) within the receptive ﬁeld of the kernel (dotted square) will shift the activation equally, and will still\\nclassify the target correctly due to (a) translation equivariance. Major translational shift of the target object (black circle) to\\na new spatial position within the image will also result in the correct classiﬁcation, regardless of its global position, due to\\n(b) translational invariance. Due to the scoring function, presence of multiple target objects (c) within the image will result\\nin a classiﬁcation that the object is present in the image, regardless of quantity or spatial locations of the objects within the\\nimage. Our GCML attention mechansim (d) uses spatial information of features, and their global interrelations, to distinguish\\nbetween classes that can exhibit the same features in different locations, similar to Vision Transformers (ViTs) [4], but with\\nsigniﬁcantly fewer training samples needed by ViTs to achieve similar generalization. GCML achieves this by not performing\\ntokenization of input images, but rather tokenizing class activation maps generated by the CNN with respect to the class of\\ninput images.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='tokenization of input images, but rather tokenizing class activation maps generated by the CNN with respect to the class of\\ninput images.\\nA. Image Classiﬁcation with CNNs\\nYamashita et al. [5] provide an excellent overview of\\nCNNs and their application in radiology, while we provide\\na general summary. In a feed forward Convolutional Neural\\nNetwork, image classiﬁcation is performed by propogating\\ninput through a series of convolutional layers, where ﬁlters\\nthat were previously trained to recognize speciﬁc features of an\\nimage get activated and are used as input to subsequent layers.\\nTypically, this process involves downsampling or reducing the\\nmapping resolution of activated output with respect to input, as\\nimage data moves further through the layers. This is done by\\nutilizing convolutional ﬁlters of size k > 1, but signiﬁcantly\\nless than the size of the input. By learning local correlation\\nstructures within the bounds of the window deﬁned by k,\\nparticular convolutional layers learn speciﬁc local features.\\nDuring inference, this allows the network to recognize objects\\nor larger features, that are compositions of smaller features,\\nwhich were recognized by earlier layers. After the last convo-\\nlutional layer, 2 dimensional activation maps are ﬂattened to\\n1 dimensional data and are passed to a fully connected layer\\nthat tallies the contributions of activated ﬁlters with respect to', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='lutional layer, 2 dimensional activation maps are ﬂattened to\\n1 dimensional data and are passed to a fully connected layer\\nthat tallies the contributions of activated ﬁlters with respect to\\nclasses represented in the ﬁnal connected layer. A classiﬁcation\\ncan then be made by taking the maximum tally, or top K\\ntallies for top K classiﬁcation. Additional layers, such as a\\nlayer representing a SoftMax function, can be utilized after\\nthe ﬁnal connected layer to obtain class probabilities from the\\njoint distribution of represented classes, but the core process\\ndescribed above is the same.\\nThe use of convolutions in CNNs provides two important\\ngeneralization properties with respect to image classiﬁcation,\\ntranslation equavarience and invariance, where a convolution\\nis equavarient to translation if an object in an image is spatially\\nshifted, convolution’s output is equally shifted. Invariance is a\\nresult of a position-independent pooling operation that follows\\na convolution, and results in a loss of absolute location, but\\nenables the visual inductive prior of convolutional operators\\n[6]. At a higher level, they enable features to be reliably\\ndetected regardless of their spatial location within an input\\nimage.\\nCNNs became the go-to method for image classiﬁcation\\nafter Krizhevsky et al. [7] published their results on ImageNet\\n[8]. Within the medical domain, CNNs started to be em-', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='after Krizhevsky et al. [7] published their results on ImageNet\\n[8]. Within the medical domain, CNNs started to be em-\\nployed for diagnostic assistance of pulmonary diseases through\\nclassiﬁcation of medical imagery, including CT scans and\\nX-rays [9] [10]. With the rise of the COVID-19 pandemic,\\nmuch work has been done in applying CNNs for image\\nclassiﬁcation of medical imagery in order to rapidly detect\\npulmonary manifestations of COVID-19 in chest X-rays [11].\\nMuch of this work, which we further discuss in Section II,\\nutilizes CNNs for classifying chest X-rays in a manner we\\ndescribed above. For the purposes of this work, we refer to\\nthis method of image classiﬁcation as standard or additive,\\nbecause classiﬁcation is performed by summing contributions\\nof various ﬁlters at the ﬁnal connected layer of the network.\\nB. Limitations of Image Classiﬁcation with CNNs\\nEven with state-of-the-art results in image classiﬁcation and\\nobject detection, CNNs have certain limitations relevant to\\nimage classiﬁcation in certain domains. For example, Hosseini\\net al. [12] report degraded image classiﬁcation performance on\\nimages with reversed brightness, or negative images. This may\\nbe mitigated by proper pre-processing of data, but it may signal\\nthat color channel information or texture may be learned by', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='images with reversed brightness, or negative images. This may\\nbe mitigated by proper pre-processing of data, but it may signal\\nthat color channel information or texture may be learned by\\nthe network, along with shapes. Although not a limitation in\\nitself, it introduces additional considerations when evaluating\\nnetwork performance on new and out-of-distribution data.\\nThe main limitation that we address in this work has to\\ndo with the localized perceptive ﬁelds of convolutional ﬁlters.\\nAlthough CNNs exhibit translational invariance and equivari-\\nance, which improves generalization in many instances, it can\\n3\\nalso hurt generalization through loss of spatial information\\nwithin the pooling layers of the CNN [13]. The canonical\\nexample of this problem relates to classifying an image of\\na human face, where features such as nose, eyes, lips are\\nidentiﬁed, yet placing them in different parts of the image\\nin a manner that does not resemble a face can still yield\\na face classiﬁcation [14]. Relating this to the chest radio-\\ngraphs domain, small manifestations of pulmonary disease are\\nidentiﬁed by a CNN, but their spatial interrelationships are\\nmostly lost. In the natural imagery and radiology domains, the\\nloss of spatial information, due to the contraints imposed by\\nconvolutional ﬁlter size and pooling, can result in incorrect\\nclassiﬁcations. Figure 1 provides a visual intuition to the\\ninductive biases that CNNs exhibit in terms of identiﬁying', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='classiﬁcations. Figure 1 provides a visual intuition to the\\ninductive biases that CNNs exhibit in terms of identiﬁying\\ntarget objects or certain features, for example abnormalities\\nin X-rays. While CNNs are particularly good at detecting\\nabnormalities regardless of their spatial locations, the scoring\\nfunction used for classiﬁcation in standard CNNs, shown in\\nEq 1, does not consider spatial locations nor spatial relations\\nbetween features as discriminators. We show in this work, that\\naccounting for global spatial interrelation of features improves\\nclassiﬁcation of pulmonary conditions.\\nC. Contribution\\nOur main goal in this work is to provide empirical evidence\\nto the hypothesis that accounting for global correlations be-\\ntween activated regions of an image improves classiﬁcation of\\npulmonary conditions in chest radiographs. This improvement\\nalso extends to image classiﬁcation domains where discrim-\\ninating a target class involves accounting for global spatial\\ncorrelations between features. Our secondary goal is to show\\nthat the classiﬁcation approach, using our novel Globally\\nCorrelated Maximum Likelihood (GCML) auxiliary attention\\nmechanism, is competitive to standard CNN classiﬁcation,\\nwhile utilizing signiﬁcantly fewer model parameters, less\\ncomputational resources, and much fewer training samples.\\nTo that end, our contributions are the following:\\n• We developed a novel auxiliary attention mechanism,\\nGCML, that is utilized with existing Convolutional Neural\\nNetwork architectures in order to account for global spa-', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='To that end, our contributions are the following:\\n• We developed a novel auxiliary attention mechanism,\\nGCML, that is utilized with existing Convolutional Neural\\nNetwork architectures in order to account for global spa-\\ntial correlations between salient features of represented\\nclasses.\\n• Using GCML, we show competitive results for image\\nclassiﬁcation on a benchmark dataset, CIFAR-10, using\\nsigniﬁcantly less model parameters and computation,\\nwhile not utilizing any pre-training on samples in relevant\\ndomains.\\n• We show that by utilizing the GCML attention mechanism,\\nwe improve image classiﬁcation, speciﬁcally increasing\\nmodel sensitivity or recall of pulmonary diseases in chest\\nradiographs. This also includes effective generalization\\non a previously unseen dataset, suggesting that the GCML\\nattention mechanism improves and complements visual\\ninductive priors learned by CNNs by accounting for\\nspatial relations.\\n• Our results show that standard CNN and our GCML tech-\\nnique for image classiﬁcation have particular strengths,\\nand show potential utility when utilized as ensemble\\nmethods.\\n• Finally, we provide an open source1 reference implemen-\\ntation of our technique, allowing further research into its\\nimprovement and utility.\\nThe remainder of this paper is structured as follows. Section\\nII describes work related to image classiﬁcation of pulmonary\\ndiseases using medical imagery. Section III outlines our ap-\\nproach, while brieﬂy discussing work relevant to attention', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='II describes work related to image classiﬁcation of pulmonary\\ndiseases using medical imagery. Section III outlines our ap-\\nproach, while brieﬂy discussing work relevant to attention\\nmechanisms. We report results of our experiments in Section\\nIV, including a benchmark dataset of natural imagery and two\\nseparate datasets of chest radiographs. Finally we conclude by\\ndiscussing our results, limitations of our approach, and future\\nresearch directions.\\nII. RELATED WORK\\nIn this section we describe work related to pulmonary\\ndisease classiﬁcation using chest X-rays or CT scans. Some\\napproaches mentioned also include forms of weakly super-\\nvised localization, which in contrast to object detection in\\nimagery, does not require that training labels be accompanied\\nwith spatial coordinates of objects to be detected. These types\\nof training labels are also referred to as image level labels, as\\nthey only provide information on whether certain target classes\\nare present in the image, not their spatial locations. To the best\\nof our knowledge and at the time of this work, CNN attention\\nmechanisms have not been used for the purpose of improving\\nimage classiﬁcation of pulmonary diseases.\\nRahaman et al [15] employed transfer learning to ﬁne-tune\\nseveral CNN architectures to classify chest X-ray images into\\nthree classes: COVID-19, Healthy, and Pneumonia. A total\\nof 860 images were used in their study, which reported the\\nVGG19 [16] architecture having the best performance achiev-', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='three classes: COVID-19, Healthy, and Pneumonia. A total\\nof 860 images were used in their study, which reported the\\nVGG19 [16] architecture having the best performance achiev-\\ning an accuracy of 89.3 percent, average precision of 0.90, a\\nrecall of 0.89, and F1 score of 0.90. Khan et al. [17] proposed\\nCoroNet, a network based on the Xception [18] architecture\\nfor diagnosis of COVID-19 from chest X-rays. They used a\\ndataset of X-ray images [19] to train their network, achieving\\n95 percent accuracy when classifying images for COVID-\\n19, Normal, and Pneumonia. Tamal et al. [20] used their\\nradiology classiﬁcation model for COVID-19 classiﬁcation\\nwith low severity, as scored by radiologists, on new data from\\npatients at a local hospital, showing generalization to out-\\nof-distribution data and achieving an overall accuracy of 90\\npercent. Kim et al. [21] reported relevant ﬁndings on the effect\\nof dataset composition practices on classiﬁcation performance.\\nThe authors reported that higher classiﬁcation performance\\nwas observed on datasets where data composing each class\\ncame from different sources. Heidari et al. [22] showed\\nthat their image preprocessing scheme improved pulmonary\\ndisease classiﬁcation in X-ray images. Similar diagnostic aid', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='came from different sources. Heidari et al. [22] showed\\nthat their image preprocessing scheme improved pulmonary\\ndisease classiﬁcation in X-ray images. Similar diagnostic aid\\napproaches to classifying COVID-19 were reported in [23],\\n[24], and [25], where the use of transer learning using a pre-\\ntrained Convolutional Neural Network architecture to perform\\nmulti-class classiﬁcation was the common factor.\\n1https://gitlab.com/verenich/gcmlpub\\n4\\nIn addition to classifying images as representing pulmonary\\nconditions, work has been proposed to provide some explain-\\nability of those classiﬁcations. Tsiknakis et al. [26] utilized\\nthe Inception [27] architecture to ﬁrst classify X-ray images\\ninto speciﬁc diseases, and then applied a weakly supervised\\nlocalization technique GradCAM [28] to identify regions\\nwithin the image responsible for a particular classiﬁcation.\\nWang et al. [29] proposed a similar approach to diagnose and\\nlocalize disease maniﬁstation in X-rays. Verenich et al. [30]\\nproposed a method to reduce aleatoric uncertainty in weakly\\nsupervised localization that can arise from signiﬁcant class\\noverlap between features associated with similar pulmonary\\ndiseases. Gupta et al. [31] proposed an approach that classiﬁes\\nand performs weakly supervised localization using standard', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='overlap between features associated with similar pulmonary\\ndiseases. Gupta et al. [31] proposed an approach that classiﬁes\\nand performs weakly supervised localization using standard\\nClass Activation Maps [32].\\nRecently, Transformers, originally proposed by Vaswani et\\nal. [33], have shown state-of-the-art performance on natural\\nlanguage processing tasks. These ideas have been utilized in\\nVision Transformers (ViT) [4] to introduce positional attention\\nmechanisms for image classiﬁcation, attaining results com-\\nparable to state-of-the-art CNNs. However, ViTs lack trans-\\nlational invariance and equavarience of CNNs, thus require\\nsigniﬁcanly more training data to generalize [4]. To the best\\nof our knowledge, and at the time of this work, ViTs have\\nnot been used for classiﬁcation of pulmonary diseases using\\nX-rays. One possible reason for this is insufﬁcient amount of\\ntraining data, to achieve same levels of generalization as CNNs\\nwith the data that is currently available.\\nIII. APPROACH: GCML ATTENTION MECHANISM\\nThe goal of our attention mechanism is to preserve spatial\\ninterrelationships of activated regions in a given image I\\nrelative to target classes C learned by a convolutional neu-\\nral network G. Our hypothesis is that localized pulmonary\\nabnormalities, detected by the convolutional neural network\\nin different regions, can yield additional discriminative power', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='ral network G. Our hypothesis is that localized pulmonary\\nabnormalities, detected by the convolutional neural network\\nin different regions, can yield additional discriminative power\\nwhen their global interrelationships are considered. The main\\nintuition for this is that translation invariance and equivariance\\nproperties of the CNNs result in class discrimination that is\\nadditive, with respect to activation strengths of convolutional\\nﬁlters, but not their spatial relation to each other. This section\\ndescribes the architecture of our approach, that accounts for\\nspatial interrelations of salient features.\\nA. Attention Type\\nOur technique explores complementing convolutional neural\\nnetworks with attention mechanisms. The main difference\\nbetween prior work [34]–[37] and GCML is that we do not\\nalter the architecture of the CNN using self-attention layers,\\nbut instead provide a separate auxiliary structure that is created\\nusing a pre-trained network. In addition, the stochastic struc-\\nture of GCML does not require it to be trained simultaneously\\nwith the CNN using backpropagation as in [37], thus it does\\nnot have to be differentiable and is signiﬁcantly faster to\\ntrain. Finally, tokenization of input images into patches is\\nalso not required, instead we use downsampled output of the\\nlast convolutional layer, scaled by class weights from the ﬁnal\\nconnected layer, to generate class activation maps, as proposed\\nby Zhou et al. [32]. These class activation maps are used\\nas input to our GCML attention mechanism. In addition to', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='connected layer, to generate class activation maps, as proposed\\nby Zhou et al. [32]. These class activation maps are used\\nas input to our GCML attention mechanism. In addition to\\nbeing effective at performing weakly supervised localization,\\nclass activation maps were also used to distinguish overlapping\\nregions of images that may belong to different but overlapping\\nclasses [30].\\nThe work that is in closest alignment with our approach\\nis the UL-Hopﬁeld model [38], which uses pre-trained CNNs\\nwith an associative memory bank to perform image classiﬁ-\\ncation. Our approach differs in several ways: the UL-Hopﬁeld\\nauxiliary memory learns class-speciﬁc core patterns from a\\npre-trained CNN in an unsupervised manner, while we use\\nthe CNN to generate a pattern using training labels. Second,\\nthe type of input to the attention function that is used to train\\ntheir memory structure is extracted from the last pooling layer\\nwithout weighting activated feature maps by a speciﬁc class,\\nwhich is done in our approach during training. Finally, for\\nexperiments we use a CNN with 42x less parameters than the\\nCNN they use for feature extraction, as we discuss in Section\\nIV.\\nB. Input Features\\nFormally, to compute the attention function input tensor\\nMc, where c is a class represented in the CNN, we do the\\nfollowing: given an input image, let fk(x, y) be the activation\\nof ﬁlter k at the last convolutional layer of of a CNN G and', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='following: given an input image, let fk(x, y) be the activation\\nof ﬁlter k at the last convolutional layer of of a CNN G and\\n(x, y) be the spatial location. During classiﬁcation, for each\\nﬁlter k, a pooling layer outputs a global average Fk deﬁned\\nas P\\nx,y fk(x, y), which is then used as input to the fully\\nconnected layer, giving us the class score Sc in Eq 1, where\\nwc\\nk is a scalar weight indicating the importance of Fk to class\\nc.\\nSc =\\nX\\nk\\nwc\\nkFk\\n(1)\\nTo compute each spatial element of the class activation map\\n[32], or 2-dimensional tensor Mc, we use\\nMc(x, y) =\\nX\\nk\\nwc\\nkfk(x, y)\\n(2)\\nThe resulting tensor Mc effectively splits the input image I\\ninto regions or tokens, where entries M c\\ni,j represent activation\\nintensities of those regions for class c.\\nOur technique would be equally applicable to be used with\\nclass activation maps generated by another weakly supervised\\nlocalization technique called Grad-CAM, proposed by Sel-\\nvaraju et al. [28]. It requires that we compute the gradients of\\noutput of the network with respect to feature map activations\\nfor each class c, making it slower than the standard CAM\\nmethod for the purposes of training the GCML structure on\\nlarge datasets.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='output of the network with respect to feature map activations\\nfor each class c, making it slower than the standard CAM\\nmethod for the purposes of training the GCML structure on\\nlarge datasets.\\nC. Attention Function\\nSimilar to the original work on transformers [33], our\\nattention function is a mapping of a query Q, based on a\\n5\\nFig. 2: Example of a 2-dimensional tensor M, which is the\\nsame as the class activation map, that is used as an input to\\nthe attention function Q. The dimensions of M are dictated by\\nthe mapping resolution of the convolutional layer that we use\\nto compute the class activation map. Values at Mi,j represent\\nactivation intensities for class C after an image I is passed\\nthrough the network G.\\nFig. 3: Example of a 2-dimensional tensor M in its interme-\\ndiate state after normalization and application of threshold τ\\nas computed within attention function Q. Hyperparameter τ is\\nalso optimized during the training stage of the GCML structure\\nand its optimized value is persisted for inference.\\ndatastore key K to a retrieved value V . In our case, the query\\nfunction Q takes as input a real tensor M of size H×W, which\\nmaps it to a datastore key K, to retrieve likelihood value V . In\\ntraining mode, when learning the GCML structure, key K is\\nused to update the value at that index, while during inference\\nmode, it is used to retrieve V at position K. In both cases,\\nattention function Q remains the same. The states of input,', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='used to update the value at that index, while during inference\\nmode, it is used to retrieve V at position K. In both cases,\\nattention function Q remains the same. The states of input,\\nintermediate states, and output of Q are illustrated in Figures\\n2, 3, and 4. Parameter τ is used as a threshold to convert input\\nMc to its intermediate state shown in Fig 3 using Equation (3).\\nfτ(Mi,j) =\\n(\\n1,\\nif Mi,j ≥τ\\n0,\\notherwise\\n(3)\\nThe full algorithm for obtaining datastore key K from tensor\\nMc using the attention function Q is described in Algorithm\\n1.\\nFig. 4: Intermediate state of tensor M is ﬂattened to a vector\\nB where entries Bi represent bits that are set to 0 or 1.\\nConverting this vector of bits to an integer yields our datastore\\nkey K, which would be 144 in the vector displayed. This value\\nis the output of the attention function Q. Note that given the\\nlength L of vector B, the number of possible entries Ki is\\n2L. Also, either big or little endianness can be used for bit\\narrangment, as long as it is consistent throughout training and\\ninference.\\nAlgorithm 1 Attention function Q\\n1: procedure Q(Mc)\\n▷Computes K from Mc\\n2:\\nM ′\\nc ←normalize(Mc)\\n3:\\nM ′′\\nc ←fτ(M ′\\nc)\\n▷Threshold τ\\n4:', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='2:\\nM ′\\nc ←normalize(Mc)\\n3:\\nM ′′\\nc ←fτ(M ′\\nc)\\n▷Threshold τ\\n4:\\nB ←flatten(M ′′\\nc )\\n▷From 2-d to 1-d\\n5:\\nK ←binToInt(B)\\n6:\\nreturn K\\n▷Datastore key K\\n7: end procedure\\nD. Training GCML\\nThe GCML datastore S is a tensor with dimensions C × P,\\nwhere C is the number of classes represented in network G and\\nP is of size 2L, where L is the length of the ﬂattened vector B\\nfrom which key K is derived. For a given class y, and possible\\nactivations x of M y\\ni,j, being in on or off states, each row of S\\nrepresents a discrete Conditional Probability Distribution of\\nclass y given activations x, as shown in Equation (4).\\nS[y] = P(y|x0, y|x1, . . . , y|(x0, x1, . . . , xL))\\n(4)\\nTo compute these likelihood distributions for all classes, we\\nutilize a fully trained convolutional neural network G along\\nwith the training data set DT , which was used to train G.\\nAlgorithm 2 GCML training procedure (1 epoch)\\n1: procedure UPDATE(S, DT , G) ▷Update likelihoods in S\\n2:', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='with the training data set DT , which was used to train G.\\nAlgorithm 2 GCML training procedure (1 epoch)\\n1: procedure UPDATE(S, DT , G) ▷Update likelihoods in S\\n2:\\nfor i, lc in DT do\\n▷Image i, with label lc\\n3:\\nMc ←Gm(i, lc) ▷Compute Mc via G and Eq 2\\n4:\\nK ←Q(Mc)\\n▷Compute key K\\n5:\\nS[lc][K] ←+1\\n▷Increment state\\n6:\\nend for\\n7:\\nS ←normalize(S)\\n▷Optional normalization\\n8:\\nreturn S\\n▷GCML store S\\n9: end procedure\\nAlgorithm 2 shows the training procedure for the GCML\\ndatastore. For simplicity, we show the procedure for single\\nimages sequentially. In practice however, we implement these\\nprocedures on batches of images provided by a dataloader.\\nWe also note that none of the weights in network G are being\\nupdated during this procedure, and the network is set to eval-\\nuation mode. The procedure Gm(i, lc) involves propagating\\n6\\nthe image through network G, and computing Mc for that\\nimage using class label lc. Finally, the normalization procedure\\nof S is marked as optional because this normalization can\\nalso happen before inference using GCML is performed. One\\nreason to hold off on normalization, is to allow S to be further', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='of S is marked as optional because this normalization can\\nalso happen before inference using GCML is performed. One\\nreason to hold off on normalization, is to allow S to be further\\ntrained on additional data, as we will discuss later in this paper.\\nAnother consdideration when training the GCML structure\\nis to account for data transformations that were performed\\nwhile training the original network G. For example random\\ncrops or ﬂips along a horizontal or vertical axes may alter\\nthe class activation map, thus using more epochs with random\\ntransformations should improve the GCML structure in some\\ndomains, such as natural imagery of CIFAR-10. We note\\nhowever, that in a domain such as chest radiology, major trans-\\nformations such as ﬂips will actually degrade performance as\\norientation of X-ray images is consistent and generalizing to\\nsuch tranformations is not needed and is actually harmful.\\nTherefore, in order to improve generalization of attention\\nmechanisms, appropriate transformations should be considered\\nbased on the target domains.\\nE. Inference with GCML\\nTo perform inference using the GCML attention mecha-\\nnism we utilize a trained Convolutional Neural Network G\\nalong with the GCML datastore S. As mentioned earlier, the\\nattention function Q remains the same during training and\\ninference, the main difference is that during inference we\\ncompute Mc for all classes represented in G instead of just\\nthe class label provided with training data. Additionally, before\\ninference is performed, we must make sure that the datastore S', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='compute Mc for all classes represented in G instead of just\\nthe class label provided with training data. Additionally, before\\ninference is performed, we must make sure that the datastore S\\nis normalized, as the normalization step during training shown\\non line 7 in Algorithm 2 is optional, in order to enable further\\ntraining.\\nAlgorithm 3 GCML inference procedure\\n1: procedure PREDICT(I, G, S) ▷Predict class on image I\\n2:\\n⃗\\nMc ←G(I)\\n▷Tensors Mc for all classes in G\\n3:\\n⃗\\nKc ←Q( ⃗\\nMc)\\n▷Compute K for all classes in G\\n4:\\n⃗\\nVc ←lookup(S, ⃗\\nKc)\\n▷Get class likelihoods\\n5:\\nCP ←argmax( ⃗\\nVc)\\n▷Get Max Likelihood class\\n6:\\nreturn CP\\n▷Return predicted class\\n7: end procedure\\nAlgorithm 3 shows the inference procedure of our approach,\\nwhich expects datastore S to be normalized. For each image\\nI, we propagate it through the convolutional neural network\\nG, where we compute inputs ⃗\\nMc to the attention function Q,\\nwhere each item in ⃗\\nMc is a class activation map computed\\nfor each class represented in G, given input I. In other words,\\nsingle input I will generate N inputs to the attention function,\\nwhere N is the number of classes in G. The next step is', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='for each class represented in G, given input I. In other words,\\nsingle input I will generate N inputs to the attention function,\\nwhere N is the number of classes in G. The next step is\\nto compute vector ⃗\\nKc, where each entry Ki is a key to a\\nlikelihood value representing class i. Class likelihood values\\n⃗\\nVc are then retrieved from S and maximum value is taken to\\ndetermine the class that input I belongs to.\\nFigure 5 shows a full view of the inference procedure\\nusing the GCML attention mechanism, as well as the standard\\ninference process using the convolutional neural network.\\nAs shown in the diagram, in addition to two classiﬁcations,\\nlabeled (a) for standard CNN classiﬁcation and (b) for classiﬁ-\\ncation using the attention mechanism, Q function input tensors\\n⃗\\nMc can also be used to perform weakly supervised localization\\n(c) by upsampling them to the same size as the input image\\nto produce a heatmap of relevant regions for a given class.\\nF. On Attention Input Size\\nThe dimensions of the input Mc to the attention function\\nQ are determined by the ﬁnal mapping resolution of the last\\nconvolutional layer of network G. For example, ResNet50, a\\nversion of a widely used convolutional architecture utilizing\\nresidual layers [39], has a ﬁnal mapping resolution of 7 × 7,\\nwhen used with input images that are 224 × 224. This would', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='version of a widely used convolutional architecture utilizing\\nresidual layers [39], has a ﬁnal mapping resolution of 7 × 7,\\nwhen used with input images that are 224 × 224. This would\\nresult in GCML datastores with 249 entries for each class,\\nas this is the number of possible binary combinations of\\nthreshold activated regions within Mc of that size. Instead, we\\ndownsample the ﬁnal resolution layer to a more manageable\\nsize, 4 × 4 and 5 × 5 as we will discuss in Section IV.\\nIV. EXPERIMENTS\\nIn this section we report the results of using our GCML\\nattention mechanism to perform image classiﬁcation. We per-\\nformed experiments on three datasets: (1) dataset of natural\\nimagery CIFAR10 [40], (2) COVID-19 radiology dataset [41]\\ncontaining X-ray imagery of patients diagnosed with COVID-\\n19, viral pneumonia, and no ﬁndings, (3) dataset of COVID-\\n19, pneumonia, and no ﬁndings images taken from [19]. The\\npurpose of the third dataset is to assess generalization of our\\nmethod to new data.\\nThe CIFAR10 dataset contains 60000 32 × 32 images rep-\\nresenting 10 mutually exclusive classes, meaning each image\\nbelongs to only one class, which are: airplane, automobile,\\nbird, cat, deer, dog, frog, horse, ship, and truck. Dataset authors\\nnote that automobile and truck classes do not overlap. The', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='belongs to only one class, which are: airplane, automobile,\\nbird, cat, deer, dog, frog, horse, ship, and truck. Dataset authors\\nnote that automobile and truck classes do not overlap. The\\nmain reason we use CIFAR10 for our experiments is that\\nthe authors of the approach most similar to ours [38] provide\\nextensive evaluation results, as well as being the largest dataset\\nevaluated in that work.\\nAs the main data for our experiments, the COVID-19\\nradiology dataset [41] contains a total of 2905 images, where\\nCOVID-19 cases are only represented in 219 of them, with the\\nrest evenly distributed between viral pneumonia and images\\nwith no ﬁndings. This data set is particularly interesting to\\nus as it represents both class imbalance and a data starved\\nenvironment, as in such cases vision transformers do not out-\\nperform state-of-the-art CNN models [4] without pre-training\\non very large datasets in the similar domain.\\nTo remain within reasonably accessible hardware con-\\nstraints, all of our experiments were performed on a single\\nmachine with the following hardware characteristics: Intel\\nCore i7 CPU, 64 GiB of RAM, NVIDIA RTX 2080 GPU.\\nWe used Ubuntu 20 as the operating system, NVIDIA CUDA\\n11 GPU acceleration library, and Pytorch 1.9.0 as our model\\nimplementation framework.\\n7\\nFig. 5: Inference ﬂow using a Convolutional Neural Network G and the auxiliary attention mechanism GCML. For a given', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='implementation framework.\\n7\\nFig. 5: Inference ﬂow using a Convolutional Neural Network G and the auxiliary attention mechanism GCML. For a given\\ninput image (I), outputs (a) and (b) represent classiﬁcations made by the Convolutional Neural Network through its standard\\nclassiﬁcation layer (a), and using the GCML attention mechanism (b). Both classiﬁcations can be utlized as an ensemble in a\\nsingle forward pass during inference. In addition, input tensor Mc, to the attention function Q, can be upsampled using bilinear\\nsampling to obtain a heatmap that performs weakly supervised localization of relevant regions for a given class as shown with\\noutput (c). We note that we include path (c) as an example of weakly supervised localization that can be performed using\\nMc, which we do not perform in our experiments. The input to the attention function Q is marked as a vector because 2-d\\ntensors Mc are computed for every class represented in network G, hence a vector of 2-d tensors is passed to Q to compute a\\nvector of class keys ⃗\\nKc that is then used to retrieve a vector of class likelihood probabilities ⃗\\nV as shown in Algorithm 3. We\\nalso note that the block labeled as GCML represents GCML datastore tensor S as shown in Algorithm 2.\\nA. CIFAR-10 Results\\nHere we describe our experimental settings and results on\\nthe CIFAR-10 benchmark dataset. As mentioned earlier, the', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='A. CIFAR-10 Results\\nHere we describe our experimental settings and results on\\nthe CIFAR-10 benchmark dataset. As mentioned earlier, the\\nmodel we use as our feature extractor is signiﬁcantly smaller\\nthan that used in [38], where a pretrained ResNet50 architec-\\nture was used. We implemented a smaller version of a residual\\nnetwork architecture, which contains only 787,482 parameters\\ncompared to 33,554,432 for ResNet50. Additionally, we did\\nnot perform resizing of input images to 224×224, but instead\\nused the original input size of 32×32. The mapping resolution\\nof the last convolutional layer in our network is 4×4 compared\\nto 7×7 utilized in [38]. The authors [38] did not ﬁne-tune their\\nfeature extractor on the CIFAR-10 dataset, while we trained\\nours from scratch for 80 epochs on a single GPU (Nvidia RTX\\n2080) taking around 15 minutes or about 0.25 GPU hours.\\nThe only data augmentation we performed was padding input\\nimages by 4 pixels and randomly cropping at the original size\\nof 32×32. The training phase of the GCML auxiliary attention\\nmechanism took 28 minutes on the same machine with several\\nτ values. Training the GCML structure with different values\\nof τ does not require any retraining of the CNN feature\\nextractor. The test partition of the CIFAR-10 dataset (10,000\\nimages) was not used for either of training phases. We also', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='of τ does not require any retraining of the CNN feature\\nextractor. The test partition of the CIFAR-10 dataset (10,000\\nimages) was not used for either of training phases. We also\\ndid not utilize a validation set during training of our feature\\nextractor in order to replicate the training environment of [38]\\nas much as possible.\\nTable I shows our results marked with (*) along with\\nTABLE I: CIFAR-10 Results for image classiﬁcation using\\nour GCML approach, UL-Hopﬁeld network [38], and state-\\nof-the-art vision transformer model [4]. We include vision\\ntransformer models to illustrate efﬁciency of our approach in\\nterms of number of parameters and computational cost. The\\ntwo transformer models were pre-trained on 300M images\\nusing cloud TPUv3 with 8 cores for 8 days [4], while our\\nmethod took less than 1 hour on a single GPU. The numbers\\nof trainable parameters are reported for each method according\\nto standard practice.\\nModel\\n% Correct\\nParameters\\nExtra Training Data\\nViT-H/14 [4]\\n99.5\\n632M\\n300M\\nViT-L/16 [4]\\n99.42\\n307M\\n300M\\nUL-Hopﬁeld [38]\\n83.1\\n33.6M\\n1.28M\\nRes4Cif GCML*\\n85.5\\n0.79M\\n0\\nreported results in [38] and the state-of-the-art performance on', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='33.6M\\n1.28M\\nRes4Cif GCML*\\n85.5\\n0.79M\\n0\\nreported results in [38] and the state-of-the-art performance on\\nthe CIFAR-10 dataset reported in [4]. Only predictions made\\nusing the GCML data structure, marked (b) in Figure 5, were\\nused. Our methods outperforms [38], but we note that the train-\\ning of their memory module was unsupervised, even though\\ntheir pre-trained feature extractor was signiﬁcantly larger. One\\nobservation in these results is the signiﬁcant reduction in\\nparameters and extra training costs shown by our method,\\nwhere our model has 800x fewer parameters than ViT-H/14\\nand 42x fewer parameters than UL-Hopﬁeld while requiring no\\n8\\nFig. 6: Confusion matrix on the CIFAR-10 dataset reported in\\n[38] We note that they report true labels on the y-axis of the\\nconfusion matrix.\\nFig. 7: Confusion matrix on the CIFAR-10 dataset using our\\nRes4Cif and GCML with parameter τ = 0.001. Our method\\noutperforms UL-Hopﬁeld networks [38] with signiﬁcantly\\nfewer parameters and without pre-training. We also note that in\\nanother experiment using τ = 0.009, classiﬁcation improved,\\nas shown in Table II, implying further potential improvement.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='fewer parameters and without pre-training. We also note that in\\nanother experiment using τ = 0.009, classiﬁcation improved,\\nas shown in Table II, implying further potential improvement.\\nextra data to pre-train. Both ViT-H/14 and ViT-L/16 are vision\\ntranformer models, which are pretrained on large datasets. ViT-\\nL/16 model, is pre-trained on the ImageNet-21k dataset, and\\nis trained on cloud TPUv3 with 8 cores for approximately 8\\ndays [4]. Our method had a combined training time of less\\nthan 1 hour on a single machine with 1 GPU.\\nTable II shows the performance of our method on CIFAR-\\n10 at different values of the τ hyperparameter. Optimal values\\nof this parameter depend on the the type of data normalization\\nthat is performed on the original data, and normalization\\nmethods of activated feature maps that are used to compute\\nTABLE II: CIFAR-10 Results for classiﬁcation using GCML\\nand different values of the τ hyperparameter. This threshold\\nparameter is used to train the GCML structure as well as\\nperform inference.\\nHyperparameter τ value\\nPercent correct\\nTraining epochs\\n0.3\\n76.59\\n80\\n0.1\\n82.21\\n80\\n0.05\\n83.6\\n80\\n0.009\\n85.51\\n80\\n0.001\\n85.5\\n80\\nTABLE III: COVID-19 Dataset [41] partitions. The validation', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='80\\n0.05\\n83.6\\n80\\n0.009\\n85.51\\n80\\n0.001\\n85.5\\n80\\nTABLE III: COVID-19 Dataset [41] partitions. The validation\\npartition is used as a model selection criteria during ﬁne-tuning\\nof the feature extractor model.\\nData Class Samples\\nTraining\\nValidation\\nTest\\nCOVID-19\\n131\\n44\\n44\\nNo Finding\\n804\\n269\\n268\\nViral Pneumonia\\n807\\n269\\n268\\ninput to our attention function Q. In this experiment we did not\\nperform any normalization on the input data and used simple\\nmin-max normalization on activated feature maps. Figure 6\\nshows the confusion matrix on the CIFAR-10 dataset reported\\nin [38] and Figure 7 shows our method’s confusion matrix on\\nthe same data.\\nB. COVID-19 Radiology Results\\nThe main goal of this work was to evaluate the potential\\nbeneﬁt of attention mechanisms in identifying pulmonary\\nconditions in chest radiographs that may be missed by spatial\\ninvariance of convolutional neural networks. Vision transform-\\ners are an active area of research and convolutional neural nets\\nare currently the main approach for diagnostic assistance in\\nthe chest radiology domain [42]–[46], thus we focus on these\\ntechniques. Data in the radiology domain is not as well curated\\nas in the natural imagery domain, with new diseases like\\nCOVID-19 presenting new classes to identify. This presents', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='techniques. Data in the radiology domain is not as well curated\\nas in the natural imagery domain, with new diseases like\\nCOVID-19 presenting new classes to identify. This presents\\nboth, class imbalanced and data starved environments. Here\\nwe present our results of applying our method to the publicly\\navailable COVID-19 Radiology dataset [41].\\nFor this experiment we randomly split the COVID-19 Radi-\\nology dataset into train, validation, and test partitions as shown\\nin Table III. The validation set is used to select the best set\\nof weights during the model G ﬁne-tuning process, which is\\na standard practice in model training. Since validation data\\nis never used to update weights of G during training, in the\\nsecond part of our experiment we utilize the validation set to\\nfurther reﬁne the GCML structure to investigate the effect of\\nupdating just the attention mechanism without ﬁne-tuning the\\nfeature extractor model on this data.\\nSince we have relatively few training points, we utilized\\ntransfer learning by ﬁne-tuning a ResNet50 architecture that\\nwas pre-trained on the ImageNet dataset. The standard mod-\\niﬁcation that is done during transfer learning is to remove\\nthe last connected layer that represents original classes and\\nreplace it with new target classes before ﬁne-tuning with new\\n9\\nTABLE IV: COVID-19 dataset [41] combined results for\\naccuracy, F1 score, and sensitivity using standard CNN, our\\nGCML approach, and further tuned GCMLT structure using', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='9\\nTABLE IV: COVID-19 dataset [41] combined results for\\naccuracy, F1 score, and sensitivity using standard CNN, our\\nGCML approach, and further tuned GCMLT structure using\\nthe test partition of the COVID-19 Radiology dataset [41].\\nMETRIC (95% CI)\\nCNN\\nGCML\\nGCMLT\\nAccuracy\\n0.948 ±0.018\\n0.938 ±0.020\\n0.942 ±0.019\\nF1 Score\\n0.973 ±0.013\\n0.968 ±0.014\\n0.970 ±0.014\\nSensitivity\\n0.962 ±0.016\\n0.955 ±0.017\\n0.958 ±0.016\\ndata. One other modiﬁcation that we performed was to reduce\\nthe ﬁnal mapping resolution of the network from 7 × 7 to\\n5 × 5. This is done by adding a single convolutional layer\\nwith a kernel (ﬁlter) size of 3, padding of 0, and unit stride,\\nwhile preserving the same number of in and out channels as\\nthe previous convolutional layer. This follows from simple\\nouput resolution calculus for a given dimension of a CNN\\nconvolutional layer, where for any equilateral input i, kernel\\nsize k, padding p, and stride s = 1, the output resolution is\\ngiven by o = (i −k) + 2p + 1.\\nWe then ﬁne-tuned all parameters of this architecture using', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='size k, padding p, and stride s = 1, the output resolution is\\ngiven by o = (i −k) + 2p + 1.\\nWe then ﬁne-tuned all parameters of this architecture using\\nthe training partition of the COVID-19 dataset [41] for 30\\nepochs, while using the validation partition to keep track of\\nthe best performing weights, selecting them as our ﬁnal feature\\nextractor model. Fine-tuning was done using the following\\nsettings and hyperparameters:\\n• input was resized to 224 × 224 through a random resize\\ncrop and normalized using ImageNet mean and standard\\ndeviation values\\n• Stochastic Gradient Descent was used as our optimization\\nfunction\\n• Cross Entropy Loss was our training criterion\\n• learning rate was set to 0.001\\n• momentum was set to 0.9\\nFor our next step we trained the GCML structure using the\\ntraining partition of [41] and our convolutional neural network\\nfrom the previous step using several cam activation points or\\nτ values for 15 epochs each, with the best performing value on\\nthe test portion being τ = 0.05. Training for multiple epochs\\nwas done because we used the same random resize crop data\\ntransform that was done during training of our feature extractor\\nmodel, presenting slight positional variations. Finally, we run\\nour method on the test partition of the COVID-19 Radiology\\ndataset keeping track of both classiﬁcations, standard CNN\\nand GCML attention mechanism, as shown in paths (a) and\\n(b) of Figure 5.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='dataset keeping track of both classiﬁcations, standard CNN\\nand GCML attention mechanism, as shown in paths (a) and\\n(b) of Figure 5.\\nDuring the training phase of the GCML structure, we\\nexplicitly made the normalization step, shown on line 7\\nof Algorithm 2, optional. This allows us to further train a\\ngiven GCML structure on new data without having to track\\ndistribution statistics of the original training data. For our\\nnext experiment we evaluated the effect of further training\\nthe GCML structure on additional data points without further\\ntuning the convolutional neural network used as the feature\\nextractor. We used the validation portion of [41] to further train\\nonly the GCML structure with the same threshold τ = 0.05.\\nWe used 44 samples of COVID-19, 269 samples of No\\nTABLE V: COVID-19 per-class classiﬁcation accuracy (95%\\nCI) for standard CNN, our GCML approach, and further tuned\\nGCMLT structure using the test partition of the COVID-19\\nRadiology dataset [41]. We also note that GCMLT structure\\nwas further tuned on the validation partition, while our feature\\nextractor used with GCMLT was not further trained with the\\nvalidation partition.\\nMETHOD\\nCOVID-19\\nNo Finding\\nViral Pneumonia\\nCNN\\n1.0\\n0.978 ±0.018\\n0.911 ±0.034\\nGCML\\n1.0\\n0.929 ±0.031\\n0.937 ±0.029', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='CNN\\n1.0\\n0.978 ±0.018\\n0.911 ±0.034\\nGCML\\n1.0\\n0.929 ±0.031\\n0.937 ±0.029\\nGCMLT\\n1.0\\n0.937 ±0.029\\n0.937 ±0.029\\nFindings, and 269 samples of Viral Pneumonia images to train\\nfor 5 epochs. As we described earlier, forgoing the optinal\\nnormalization step in Algorithm 2 allows for this functionality.\\nWe then ran the test partition, which neither the convolutional\\nneural network nor the GCML structure have been trained\\non, to obtain results for combined accuracy, F1 score, and\\nsensitivity. Table IV shows our results using three models,\\nCNN or standard classiﬁcation using our convolutional neural\\nnetwork, GCML using our attention mechanism, and ﬁnally\\nGCMLT , where we further trained the attention mechanism\\nusing the validation partition.\\nThe combined results in Table IV show several encouraging\\nresults. First, the attention mechanism performed similarly\\nwell with a combined accuracy of 0.938 as compared to the\\ntraditional classiﬁcation method of convolutional neural net-\\nworks with a combined accuracy of 0.948. Second, by further\\ntuning the GCML structure, to obtain GCMLT , with a small\\nnumber of data points we were able to improve all metrics,\\nbringing combined accuracy to 0.942. This is encouraging\\nbecause tuning this structure is signiﬁcantly faster than tuning', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='number of data points we were able to improve all metrics,\\nbringing combined accuracy to 0.942. This is encouraging\\nbecause tuning this structure is signiﬁcantly faster than tuning\\na convolutional neural network. Finally, once we drilled down\\nto class level accuracy, we observe that GCML improves the\\nstandard CNN classiﬁcation on the Viral Pneumonia class by\\nabout 3 percent, as shown in Table V. This was signiﬁcant for\\ntwo reasons, ﬁrst it empirically veriﬁed our main hypothesis\\nthat our auxiliary attention mechanism could identify cases\\nof pulmonary conditions that the standard CNN classiﬁcation\\nmissed. Second, these results support our intuition of the\\nadditive nature of standard CNN classiﬁcation by showing\\nthat this method outperformed the attention method on the No\\nFindings class, as spatial relations between activated features\\nmatter signiﬁcantly less, as expected.\\nDue to the absense of object level labels, or bounding\\nboxes labeling localized manifestations of pulmonary condi-\\ntions in training data, it is difﬁcult to identify feature speciﬁc\\nintersections or lack thereof between images that CNN and\\nGCML classiﬁed correctly or incorrectly. But even with only\\nimage level labels for training, where we know that the\\ncondition is present but have no localized information of its\\nmaniﬁstation, the confusion matrices in Fig 8 clearly show\\nGCML’s improved performance on the Viral Pneumonia class,', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='condition is present but have no localized information of its\\nmaniﬁstation, the confusion matrices in Fig 8 clearly show\\nGCML’s improved performance on the Viral Pneumonia class,\\nwhile both methods performed the same on the COVID-19\\nclass.\\n10\\n(a)\\n(b)\\n(c)\\nFig. 8: Confusion matrices for evaluation results using the test partition of the COVID-19 Radiology dataset [41]. The three\\nresults represent the following: (a) classiﬁcation using our GCML attention mechanism, (b) classiﬁcation using the ﬁnal\\nconnected layer of our ﬁne-tuned Convolutional Neural Network, and (c) classiﬁcation using our GCMLT attention mechanism\\nfurther trained using the validation partition of [41]. Our ﬁrst observation is that our attention mechanism (a) is able to identify\\npulmonary conditions that the standard CNN (b) missed. This provides empirical evidence to our hypothesis for the utility\\nof attention mechanisms in the chest radiology domain. Second, by further training our GCML structure using the validation\\npartition, which was only used as a model selection criteria during the training of our feature extractor, performance was\\nfurther improved (c). This was done without further training of the CNN feature extractor. These results suggest that hybrid\\nCNN plus attention-based ensemble techniques, that utilize different inductive biases, provide a promising set of approaches\\nto incorporating global interactions between dispersed features of pulmonary diseases that are manifested in chest radiographs.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='CNN plus attention-based ensemble techniques, that utilize different inductive biases, provide a promising set of approaches\\nto incorporating global interactions between dispersed features of pulmonary diseases that are manifested in chest radiographs.\\nTABLE VI: Generalization test dataset for COVID-19 Radiol-\\nogy Data derived from [19]. Neither our feature extractor G,\\nnor the GCML structure S were trained on this data.\\nCOVID-19\\nNo Finding\\nPneumonia (Viral and Bacterial)\\n133\\n949\\n390\\nC. Radiology Generalization Performance\\nFor our ﬁnal experiment we examined our method’s ability\\nto generalize to a separate dataset [19] containing X-ray sam-\\nples of patients diagnosed with COVID-19, No Findings, and\\nPneumonia, both viral and bacterial. We created this dataset\\nby extracting images from a publicly available repository [19]\\nin order to create a test partition that was comparable in size\\nto the train partition we used in training our method. The main\\nmotivation for this experiment was an observation that many\\ninstances of work related to diagnosing COVID-19 and other\\npulmonary conditions using chest radiographs omit external\\ngeneralization experiments, as reported in Roberts et al. [1].\\nTable VI shows the generalization test partition that we\\ncreated using [19]. Neither our feature extractor nor the\\nGCML structure were trained using this data. We then tested\\nthis data using GCML classiﬁcation, using τ = 0.05. The\\nonly transformations that we performed during inference were', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='GCML structure were trained using this data. We then tested\\nthis data using GCML classiﬁcation, using τ = 0.05. The\\nonly transformations that we performed during inference were\\nresizing the image to 224 × 224, and normalization of the\\ntensorized image to ImageNet values for mean and standard\\ndeviation.\\nTable VII shows our results, including per class accuracy,\\nwhile Fig 9 shows the confusion matrix. Similar to our initial\\nresults on [41], we saw our attention mechanism perform\\nTABLE VII: Generalization dataset [19] results at 95% conﬁ-\\ndence interval. Combined accuracy, F1 score, and sensitivity\\nare shown using our GCML classiﬁcation approach. Last row\\nshows per-class accuracy for the same classiﬁer.\\n(95% CI)\\nGCML\\nAccuracy\\n0.940 ±0.012\\nF1 Score\\n0.969 ±0.009\\nSensitivity\\n0.955 ±0.011\\nCOVID-19\\nNo Finding\\nPneumonia\\nClass Accuracy\\n0.985 ±0.021\\n0.9294 ±0.016\\n0.951 ±0.021\\nwell at identifying pulmonary conditions, both COVID-19\\nand Pneumonia, and slightly worse on the No Findings class.\\nFurther examining per-class performance we see that sensitivy\\nrates for both COVID-19 and Pneumonia are high, making the\\napproach effective at detecting infected patients, represented\\nin data never seen by the model during training. These gener-', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='rates for both COVID-19 and Pneumonia are high, making the\\napproach effective at detecting infected patients, represented\\nin data never seen by the model during training. These gener-\\nalization results also increase our conﬁdence in that pertinent\\nfeatures of pulmonary conditions are being discovered by the\\nmodel, as opposed to spurious artifacts such as X-ray markings\\nrelated to the origin or medium of radiographs.\\nD. Conﬁdence Level on Hypothesis Test\\nTo evaluate our main hypothesis regarding our attention\\nmechanism outperforming the standard CNN when classifying\\npulmonary conditions, we perform a hypothesis test [47] on\\nthe two classifers as they pertain to pneumonia classiﬁcation.\\nBoth classiﬁers performed equally well on COVID-19 classi-\\nﬁcation. Let p1 = 0.911 be CNN accuracy and p2 = 0.937\\n11\\nFig. 9: Confusion matrix for predictions made on the radiology\\ngeneralization test dataset, created from [19], using the GCML\\nauxiliary attention mechanism. As noted, no training was\\nperformed using this dataset of neither the feature extractor\\nnor the GCML attention mechanism.\\nbe GCML accuracy shown in Table V. Let n = 268 be the\\nnumber of samples of viral pneumaonia images in the test\\npartition of [41]. Then, we calculate the test statistic Z as:\\nZ =\\np1 −p2\\np\\n2ˆ\\np(1 −ˆ\\np)/n\\n(5)', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='partition of [41]. Then, we calculate the test statistic Z as:\\nZ =\\np1 −p2\\np\\n2ˆ\\np(1 −ˆ\\np)/n\\n(5)\\nwhere ˆ\\np = (245 + 252)/2(268), with 245 and 252 being\\nnumbers of correct classiﬁcations from both classiﬁers, as\\nshown in the confusion matrices in Figure 8. To show that\\np1 < p2, or that p2 is better than p1, we need to show\\nZ < −zα, where zα is obtained from a standard normal\\ndistribution pertaining to a signiﬁcance level α. Given our\\nsample size, we compute Z = −1.1607. Our test statistic\\nis slightly better than the Z value of 1.15035 for a 75%\\nconﬁdence interval. Thus, we can state with 75% conﬁdence\\nlevel that GCML is more accurate than standard CNN for\\nclassifying pneumonia cases.\\nV. CONCLUSION\\nDetection of pulmonary disease using Artiﬁcial Intelligence\\ntechniques is an emergent ﬁeld [11], driven not just by\\nacademic curiosity, but a real need to improve accessibility\\nand speed of diagnosis. Well studied approaches to image\\nclassiﬁcation need to be analyzed and improved to be effective\\nin real world applications, where data can be scarce, noisy,\\nand novel. Emergence of new diseases, such as COVID-19,', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='classiﬁcation need to be analyzed and improved to be effective\\nin real world applications, where data can be scarce, noisy,\\nand novel. Emergence of new diseases, such as COVID-19,\\nexposed weaknesses in such applications of machine learning\\nmethods, but also highlighted potential beneﬁts and promise\\nto society if they were to be successfuly addressed [1].\\nIn this work, we developed a method to improve clas-\\nsiﬁcation performance of pulmonary diseases in chest X-\\nrays by expanding the perceptive awareness of convolutional\\nneural networks via GCML, our new attention mechanism.\\nWe showed that our auxiliary attention mechanism improved\\nsensitivity of pulmonary disease classifers by accounting for\\nspatial interrelations of features globally. Our initial results\\nshow a promising direction of research towards improving ex-\\nisting methods of image classiﬁcation for diagnostic assistance\\nof pulmonary diseases.\\nThere are two main limitations of our initial approach.\\nFirst, the current GCML datastore learns a conditional discrete\\nprobability distribution of a given class, more speciﬁcally the\\nuse of the τ parameter to threshold activation values of the\\ninput to attention function Q, results in a binomial distribution.\\nThis results in some information loss compared to a continuous\\ndistribution. Second, as described in Section III, we must\\nreduce the ﬁnal mapping resolution of our CNNs to manage\\nthe size of learned distributions, which essentially increases the\\narea of individual image patches for which we learn global\\ncorrelations. This can have a smoothing effect on multiple', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='the size of learned distributions, which essentially increases the\\narea of individual image patches for which we learn global\\ncorrelations. This can have a smoothing effect on multiple\\ndisease manifestations detected in a single region, causing\\nit to be treated as a single maniﬁstation. To address these\\nlimitations, we plan on improving the attention mechanism\\nby utilizing normalized continuous values of class activation\\nmaps without a threshold parameter by learning a continuous\\nmultivariate distribution, such as a Dirichlet distribution. This\\nwill allow for image patches with smaller areas, resulting in\\nmore patches, to be used in learning global relations among\\neven smaller manifestations.\\nAdditionally, we can improve positional probability distri-\\nbutions learned by GCML structure by reducing class noise\\ndue to class overlapping data in the chest X-ray domain.\\nSimilar classes result in noisy class activation maps that\\ncontain many overlapping activations [30]. By better extracting\\nclass relevant activations, we can learn tighter conditional\\nprobability distributions for appropriate classes of diseases.\\nWe believe that improving well established techniques\\nfor image classiﬁcation towards domain-speciﬁc applications,\\nsuch as pulmonary disease classiﬁcation, is well worth the\\neffort. By utilizing a strong technical foundation of CNNs,\\nprogress towards a useful diagnostic aid can be accelerated,\\nand outcomes of patients, especially in regions where access\\nto sophisticated laboratory diagnostics is limited, can be im-\\nproved. To that end, we make the reference implementation\\nof our attention mechanism freely available, including source\\ncode and models [48].\\nREFERENCES', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='to sophisticated laboratory diagnostics is limited, can be im-\\nproved. To that end, we make the reference implementation\\nof our attention mechanism freely available, including source\\ncode and models [48].\\nREFERENCES\\n[1] M. Roberts, D. Driggs, M. Thorpe, J. Gilbey, M. Yeung, S. Ursprung,\\nA. Aviles-Rivero, C. Etmann, C. McCague, L. Beer, J. Weir-McCall,\\nZ. Teng, E. Gkrania-Klotsas, AIX-COVNET, J. Rudd, E. Sala, and C.-B.\\nSchonlieb, “Common pitfalls and recommendations for using machine\\nlearning to detect and prognosticate for covid-19 using chest radiographs\\nand ct scans,” Nature Machine Intelligence, vol. 3, no. 3, pp. 199–217,\\n2021.\\n[2] P. Rajpurkar, J. A. Irvin, K. Zhu, B. Yang, H. Mehta, T. Duan,\\nD. Ding, A. Bagul, C. Langlotz, K. Shpanskaya, M. Lungren, and A. Ng,\\n“Chexnet: Radiologist-level pneumonia detection on chest x-rays with\\ndeep learning,” ArXiv, vol. abs/1711.05225, 2017.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='“Chexnet: Radiologist-level pneumonia detection on chest x-rays with\\ndeep learning,” ArXiv, vol. abs/1711.05225, 2017.\\n[3] A. Borghesi and R. Maroldi, “Covid-19 outbreak in italy: experimental\\nchest x-ray scoring system for quantifying and monitoring disease\\nprogression,” La radiologia medica, vol. 125, pp. 509–513, 05 2020.\\n[4] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\\nTransformers\\nfor\\nimage\\nrecognition\\nat\\nscale,”\\nin\\nInternational\\n12\\nConference on Learning Representations, 2021. [Online]. Available:\\nhttps://openreview.net/forum?id=YicbFdNTTy\\n[5] R. Yamashita, M. Nishio, R. Do, and K. Togashi, “Convolutional neural\\nnetworks: an overview and application in radiology,” Insights into\\nImaging, vol. 9, 06 2018.\\n[6] O. Semih Kayhan and J. C. van Gemert, “On translation invariance in', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='Imaging, vol. 9, 06 2018.\\n[6] O. Semih Kayhan and J. C. van Gemert, “On translation invariance in\\ncnns: Convolutional layers can exploit absolute spatial location,” in 2020\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\\n(CVPR), 2020, pp. 14 262–14 273.\\n[7] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classiﬁcation with\\ndeep convolutional neural networks,” Neural Information Processing\\nSystems, vol. 25, 01 2012.\\n[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\\nA large-scale hierarchical image database,” in 2009 IEEE Conference on\\nComputer Vision and Pattern Recognition, 2009, pp. 248–255.\\n[9] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers,\\n“Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on\\nweakly-supervised classiﬁcation and localization of common thorax\\ndiseases,” in Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), July 2017.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='weakly-supervised classiﬁcation and localization of common thorax\\ndiseases,” in Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), July 2017.\\n[10] B. Sahiner, A. Pezeshk, L. M. Hadjiiski, X. Wang, K. Drukker, K. H.\\nCha, R. M. Summers, and M. L. Giger, “Deep learning in medical\\nimaging and radiation therapy,” Medical Physics, vol. 46, no. 1, pp.\\ne1–e36, 2019. [Online]. Available: https://aapm.onlinelibrary.wiley.com/\\ndoi/abs/10.1002/mp.13264\\n[11] O. Albahri, A. Zaidan, A. Albahri, B. Zaidan, K. H. Abdulkareem,\\nZ.\\nAl-qaysi,\\nA.\\nAlamoodi,\\nA.\\nAleesa,\\nM.\\nChyad,\\nR.\\nAlesa,\\nL. Kem, M. M. Lakulu, A. Ibrahim, and N. A. Rashid, “Systematic\\nreview of artiﬁcial intelligence techniques in the detection and\\nclassiﬁcation of covid-19 medical images in terms of evaluation\\nand benchmarking: Taxonomy analysis, challenges, future solutions\\nand\\nmethodological\\naspects,”\\nJournal\\nof\\nInfection\\nand', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='and benchmarking: Taxonomy analysis, challenges, future solutions\\nand\\nmethodological\\naspects,”\\nJournal\\nof\\nInfection\\nand\\nPublic\\nHealth, vol. 13, no. 10, pp. 1381–1396, 2020. [Online]. Available:\\nhttps://www.sciencedirect.com/science/article/pii/S187603412030558X\\n[12] H. Hosseini, B. Xiao, M. Jaiswal, and R. Poovendran, “On the limitation\\nof convolutional neural networks in recognizing negative images,” in\\n2017 16th IEEE International Conference on Machine Learning and\\nApplications (ICMLA), 2017, pp. 352–358.\\n[13] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between\\ncapsules,” in Advances in Neural Information Processing Systems,\\nI. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\\nS. Vishwanathan, and R. Garnett, Eds., vol. 30.\\nCurran Associates,\\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/\\n2017/ﬁle/2cad8fa47bbef282badbb8de5374b894-Paper.pdf\\n[14] E. Hjelm˚\\nas and B. K. Low, “Face detection: A survey,” Computer', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='[14] E. Hjelm˚\\nas and B. K. Low, “Face detection: A survey,” Computer\\nVision and Image Understanding, vol. 83, no. 3, pp. 236–274, 2001.\\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\\nS107731420190921X\\n[15] M. Rahaman, C. Li, Y. Yao, F. Kulwa, M. Rahman, Q. Wang, S. Qi,\\nF. Kong, X. Zhu, and Z. X, “Identiﬁcation of covid-19 samples from\\nchest x-ray images using deep learning: A comparison of transfer\\nlearning approaches,” Journal of X-ray science and technology, vol. 28,\\nno. 5, 2020.\\n[16] S. Liu and W. Deng, “Very deep convolutional neural network based\\nimage classiﬁcation using small training sample size,” in 2015 3rd IAPR\\nAsian Conference on Pattern Recognition (ACPR), 2015, pp. 730–734.\\n[17] A. Khan, J. Shah, and M. Bhat, “Coronet: A deep neural network for\\ndetection and diagnosis of covid-19 from chest x-ray images.” Comput\\nMethods Programs Biomed., vol. 196, no. 105581, 2020.\\n[18] F. Chollet, “Xception: Deep learning with depthwise separable convolu-', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='Methods Programs Biomed., vol. 196, no. 105581, 2020.\\n[18] F. Chollet, “Xception: Deep learning with depthwise separable convolu-\\ntions,” in Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), July 2017.\\n[19] J. P. Cohen, P. Morrison, and L. Dao, “Covid-19 image data\\ncollection,”\\narXiv\\n2003.11597,\\n2020.\\n[Online].\\nAvailable:\\nhttps:\\n//github.com/ieee8023/covid-chestxray-dataset\\n[20] M. Tamal, M. Alshammari, M. Alabdullah, R. Hourani, H. A.\\nAlola, and T. M. Hegazi, “An integrated framework with machine\\nlearning and radiomics for accurate and rapid early diagnosis of\\ncovid-19 from chest x-ray,” Expert Systems with Applications, vol. 180,\\np. 115152, 2021. [Online]. Available: https://www.sciencedirect.com/\\nscience/article/pii/S0957417421005935\\n[21] G. Kim, J. Kim, C. Kim, and S.-M. Kim, “Evaluation of deep learning\\nfor covid-19 diagnosis: Impact of image dataset organization,” Journal\\nof Applied Clinical Medical Physics, 06 2021.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='for covid-19 diagnosis: Impact of image dataset organization,” Journal\\nof Applied Clinical Medical Physics, 06 2021.\\n[22] M. Heidari, S. Mirniaharikandehei, A. Zargari, G. Danala, Y. Qiu, and\\nB. Zheng, “Improving the performance of cnn to predict the likelihood\\nof covid-19 using chest x-ray images with preprocessing algorithms,”\\nInternational Journal of Medical Informatics, vol. 144, p. 104284, 09\\n2020.\\n[23] A. Z. Khuzani, M. Heidari, and S. A. Shariati, “Covid-classiﬁer:\\nAn automated machine learning model to assist in the diagnosis\\nof\\ncovid-19\\ninfection\\nin\\nchest\\nx-ray\\nimages,”\\nmedRxiv,\\n2020.\\n[Online]. Available: https://www.medrxiv.org/content/early/2020/05/18/\\n2020.05.09.20096560\\n[24] M. Alruwaili, A. Shehab, and S. Abd ElGhany, “Covid-19 diagnosis us-\\ning an enhanced inception-resnetv2 deep learning model in cxr images,”\\nJournal of Healthcare Engineering, vol. 2021, pp. 1–16, 06 2021.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='ing an enhanced inception-resnetv2 deep learning model in cxr images,”\\nJournal of Healthcare Engineering, vol. 2021, pp. 1–16, 06 2021.\\n[25] K. Purohit, A. Kesarwani, D. R. Kisku, and M. Dalui, “Covid-19\\ndetection on chest x-ray and ct scan images using multi-image\\naugmented deep learning model,” bioRxiv, 2020. [Online]. Available:\\nhttps://www.biorxiv.org/content/early/2020/10/19/2020.07.15.205567\\n[26] N. Tsiknakis, E. Trivizakis, E. Vassalou, Evangelia, Z. Papadakis,\\nGeorgios, A. Spandidos, Demetrios, A. Tsatsakis, J. S´\\nanchez-Garc´\\nıa,\\nR. L´\\nopez-Gonz´\\nalez, H. Karantanas, Apostolos, and K. Marias, “Inter-\\npretable artiﬁcial intelligence framework for covid-19 screening on chest\\nx-rays,” Experimental and therapeutic medicine, vol. 20, no. 2, pp. 727–\\n735, 2020.\\n[27] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='735, 2020.\\n[27] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking\\nthe inception architecture for computer vision,” in 2016 IEEE Confer-\\nence on Computer Vision and Pattern Recognition (CVPR), 2016, pp.\\n2818–2826.\\n[28] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\\nD. Batra, “Grad-cam: Visual explanations from deep networks via\\ngradient-based localization,” in 2017 IEEE International Conference on\\nComputer Vision (ICCV), 2017, pp. 618–626.\\n[29] Z. Wang, Y. Xiao, Y. Li, Z. Jie, F. Lu, M. Hou, and X. Liu, “Automat-\\nically discriminating and localizing covid-19 from community-acquired\\npneumonia on chest x-rays,” Pattern Recognition, vol. 110, p. 107613,\\n08 2020.\\n[30] E. Verenich, A. Velasquez, N. Khan, and F. Hussain, “Improving\\nExplainability of Image Classiﬁcation in Scenarios with Class Overlap:\\nApplication to COVID-19 and Pneumonia,” in Proceedings of the 19th\\nIEEE International Conference on Machine Learning and Applications,\\n2020.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='Application to COVID-19 and Pneumonia,” in Proceedings of the 19th\\nIEEE International Conference on Machine Learning and Applications,\\n2020.\\n[31] A. Gupta, A. Anjum, S. Gupta, and R. Katarya, “Instacovnet-19: A deep\\nlearning classiﬁcation model for the detection of covid-19 patients using\\nchest x-ray,” Applied Soft Computing, vol. 99, p. 106859, 10 2020.\\n[32] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning\\ndeep features for discriminative localization,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition, 2016, pp. 2921–\\n2929.\\n[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you\\nneed,”\\nin\\nAdvances\\nin\\nNeural\\nInformation\\nProcessing\\nSystems,\\nI. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\\nS. Vishwanathan, and R. Garnett, Eds., vol. 30.\\nCurran Associates,\\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='S. Vishwanathan, and R. Garnett, Eds., vol. 30.\\nCurran Associates,\\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/\\n2017/ﬁle/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\\n[34] Y. Cao, J. Xu, S. Lin, F. Wei, and H. Hu, “Gcnet: Non-local networks\\nmeet squeeze-excitation networks and beyond,” in 2019 IEEE/CVF\\nInternational Conference on Computer Vision Workshop (ICCVW), 2019,\\npp. 1971–1980.\\n[35] M. Yin, Z. Yao, Y. Cao, X. Li, Z. Zhang, S. Lin, and H. Hu,\\n“Disentangled non-local neural networks,” in Computer Vision – ECCV\\n2020, A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds.\\nCham:\\nSpringer International Publishing, 2020, pp. 191–207.\\n[36] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\\nworks,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 2018, pp. 7794–7803.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='works,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 2018, pp. 7794–7803.\\n[37] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and\\nJ. Shlens, “Stand-alone self-attention in vision models,” in NeurIPS,\\n2019.\\n[38] Q. Liu and S. Mukhopadhyay, “Unsupervised learning using pretrained\\nCNN and associative memory bank,” CoRR, vol. abs/1805.01033, 2018.\\n[Online]. Available: http://arxiv.org/abs/1805.01033\\n[39] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in 2016 IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2016, pp. 770–778.\\n[40] A. Krishevsky, “Learning multiple layers of features from tiny\\nimages,” University of Toronto, Tech. Rep., 2009. [Online]. Available:\\nhttps://www.cs.toronto.edu/∼kriz/learning-features-2009-TR.pdf\\n[41] T. Rahman, M. Chowdhury, and A. Khandakar, “Covid-19 radiology\\ndatabase,” Online, 2020, accessed Jan 2021. [Online]. Available:', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='[41] T. Rahman, M. Chowdhury, and A. Khandakar, “Covid-19 radiology\\ndatabase,” Online, 2020, accessed Jan 2021. [Online]. Available:\\nhttps://www.kaggle.com/tawsifurrahman/covid19-radiography-database\\n13\\n[42] C. Butt, J. Gill, D. Chun, and B. A. Babu, “Deep learning system to\\nscreen coronavirus disease 2019 pneumonia,” Applied Intelligence, p. 1,\\n2020.\\n[43] S. Wang, B. Kang, J. Ma, X. Zeng, M. Xiao, J. Guo, M. Cai, J. Yang,\\nY. Li, X. Meng et al., “A deep learning algorithm using ct images to\\nscreen for corona virus disease (covid-19),” MedRxiv, 2020.\\n[44] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++:\\nA nested u-net architecture for medical image segmentation,” in Deep\\nLearning in Medical Image Analysis and Multimodal Learning for\\nClinical Decision Support.\\nSpringer, 2018, pp. 3–11.\\n[45] S. Roy, W. Menapace, S. Oei, B. Luijten, E. Fini, C. Saltori, I. Huijben,', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='[45] S. Roy, W. Menapace, S. Oei, B. Luijten, E. Fini, C. Saltori, I. Huijben,\\nN. Chennakeshava, F. Mento, A. Sentelli et al., “Deep learning for\\nclassiﬁcation and localization of covid-19 markers in point-of-care lung\\nultrasound,” IEEE Transactions on Medical Imaging, 2020.\\n[46] B. Hurt, S. Kligerman, and A. Hsiao, “Deep learning localization of\\npneumonia: 2019 coronavirus (covid-19) outbreak,” Journal of Thoracic\\nImaging, vol. 35, no. 3, pp. W87–W89, 2020.\\n[47] R. Johnson and J. Freund, Miller and Freund’s Probability and Statistics\\nfor Engineers.\\nPrantice Hall International, 2011.\\n[48] E. Verenich, “Gcml artifacts repository,” Online, 2021, accessed Jul\\n2021. [Online]. Available: https://gitlab.com/verenich/gcmlpub', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='TAME: Attention Mechanism Based Feature Fusion\\nfor Generating Explanation Maps of Convolutional\\nNeural Networks\\nMariano Ntrougkas\\nCERTH-ITI\\nThessaloniki, Greece, 57001\\nntrougkas@iti.gr\\nNikolaos Gkalelis\\nCERTH-ITI\\nThessaloniki, Greece, 57001\\ngkalelis@iti.gr\\nVasileios Mezaris\\nCERTH-ITI\\nThessaloniki, Greece, 57001\\nbmezaris@iti.gr\\nAbstract—The apparent “black box” nature of neural networks\\nis a barrier to adoption in applications where explainability is\\nessential. This paper presents TAME (Trainable Attention Mech-\\nanism for Explanations)1, a method for generating explanation\\nmaps with a multi-branch hierarchical attention mechanism.\\nTAME combines a target model’s feature maps from multiple\\nlayers using an attention mechanism, transforming them into an\\nexplanation map. TAME can easily be applied to any convolu-\\ntional neural network (CNN) by streamlining the optimization\\nof the attention mechanism’s training method and the selection\\nof target model’s feature maps. After training, explanation\\nmaps can be computed in a single forward pass. We apply\\nTAME to two widely used models, i.e. VGG-16 and ResNet-50,\\ntrained on ImageNet and show improvements over previous top-\\nperforming methods. We also provide a comprehensive ablation\\nstudy comparing the performance of different variations of', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='trained on ImageNet and show improvements over previous top-\\nperforming methods. We also provide a comprehensive ablation\\nstudy comparing the performance of different variations of\\nTAME’s architecture.2\\nIndex Terms—CNNs, Deep Learning, Explainable AI, Inter-\\npretable ML, Attention.\\nI. INTRODUCTION\\nConvolutional neural networks (CNNs) [17] have achieved\\nexceptional performance in many important visual tasks such\\nas breast tumor detection [6], video summarization [3] and\\nevent recognition [10]. The trade-off between model perfor-\\nmance and explainability, and the end-to-end learning strategy,\\nleads to the development of CNNs that many times act as\\n“black box” models that lack transparency [12]. This fact\\nmakes it difﬁcult to convince users in critical ﬁelds, such as\\nhealthcare, law, and governance to trust and employ such sys-\\ntems, thus limiting the adoption of AI [2], [12]. Therefore, it\\nis necessary to develop solutions that address these challenges.\\nExplainable artiﬁcial intelligence (XAI) is an active research\\narea in machine learning. XAI focuses on developing explain-\\nable techniques that help users of AI systems to comprehend,\\nThis work was supported by the EU Horizon 2020 programme under grant\\nagreement H2020-101021866 CRiTERIA.\\n1Source code is made publicly available at: https://github.com/bmezaris/\\nTAME', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='agreement H2020-101021866 CRiTERIA.\\n1Source code is made publicly available at: https://github.com/bmezaris/\\nTAME\\n2©2022 IEEE. Personal use of this material is permitted. Permission from\\nIEEE must be obtained for all other uses, in any current or future media,\\nincluding reprinting/republishing this material for advertising or promotional\\npurposes, creating new collective works, for resale or redistribution to servers\\nor lists, or reuse of any copyrighted component of this work in other works.\\nFig. 1: An explanation produced by TAME. The input image\\nbelongs to the class ”velvet”, which cannot be localized. The\\nproduced explanation highlights the salient features of the\\nimage explaining the decision of the classiﬁer.\\ntrust and more efﬁciently manage them [4], [20]. For the\\nimage classiﬁcation task, a diverse range of post-hoc expla-\\nnation approaches exist that in a second step take the trained\\nmodel and try to uncover its decision strategy [20]. These\\nmethods produce an explanation map, highlighting salient\\ninput features. We should note that these methods should\\nnot be confused with approaches targeting weakly supervised\\nlearning tasks such as weakly supervised object localization\\nor segmentation [16], which also generate heatmaps as an\\nintermediate step, and their goal is to locate the region of\\nthe target object rather than to explain the classiﬁer’s decision\\n(e.g. see the example depicted in Fig. 1).', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='intermediate step, and their goal is to locate the region of\\nthe target object rather than to explain the classiﬁer’s decision\\n(e.g. see the example depicted in Fig. 1).\\nGradient-based methods [5], [22] were probably among the\\nﬁrst to appear in the XAI domain. These methods use gradient\\ninformation to produce explanations, but they are strongly\\naffected by noisy gradients, and the explanations contain high-\\nfrequency variations [1]. Perturbation-based methods [19],\\n[28], perturb the input and observe changes in the output,\\nthus do not suffer from gradient-based problems as above.\\nSimilarly, response-based methods [8], [21], [27] combine a\\nmodel’s intermediate representations, or features, to generate\\nexplanations. However, most methods of the two latter cate-\\ngories described above are computationally expensive because\\narXiv:2301.07407v1  [cs.CV]  18 Jan 2023\\neach input requires many forward passes for an accurate\\nexplanation map to be produced.\\nTo address the above limitation, L-CAM [11] trains an\\nattention mechanism to combine feature maps from the last\\nconvolutional layer of a frozen CNN model and produce\\nhigh quality explanations in one forward pass. However, L-\\nCAM, by design, uses the feature maps of only the last\\nconvolutional layer, and thus, may not be able to adequately\\ncapture all the information contained in the CNN model. To', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='CAM, by design, uses the feature maps of only the last\\nconvolutional layer, and thus, may not be able to adequately\\ncapture all the information contained in the CNN model. To\\nthis end, we propose TAME (Trainable Attention Mechanism\\nfor Explanations), which exploits intermediate feature maps\\nextracted from multiple layers of any CNN model. These\\nfeatures are then used to train a multi-branch hierarchical\\nattention architecture for generating class-speciﬁc explanation\\nmaps in a single forward pass. We provide a comprehensive\\nevaluation study of the proposed method on ImageNet [7] us-\\ning two popular CNN models (VGG-16 [23], ResNet-50 [13])\\nand popular XAI measures [5], demonstrating that TAME\\nachieves improved explainability performance over other top-\\nperforming methods in this domain.\\nII. RELATED WORK\\nIn this section, we brieﬂy survey the state-of-the-art XAI\\napproaches that are mostly related to ours. For a more com-\\nprehensive review the interested reader is referred to [4], [20].\\nMost XAI approaches can be roughly categorized into\\nresponse-, gradient- and perturbation-based. Gradient-based\\nmethods [5], [22] compute the gradient of a given input with\\nbackpropagation and modify it in various ways to produce\\nan explanation map. Grad-CAM [22], one of the ﬁrst in\\nthis category, uses global average pooling in the gradients of', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='backpropagation and modify it in various ways to produce\\nan explanation map. Grad-CAM [22], one of the ﬁrst in\\nthis category, uses global average pooling in the gradients of\\nthe target network’s logits with respect to the feature maps\\nto compute weights. The explanation maps are obtained as\\nthe weighted combination of feature maps and the computed\\nweights. Grad-CAM++ [5] similarly uses gradients to generate\\nexplanation maps. These methods suffer the same issues as the\\ngradients they use: neural network gradients can be noisy and\\nsuffer from saturation problems for typical activation functions\\nsuch as ReLU and Sigmoid [1].\\nPerturbation-based methods [19], [28] alter the input and\\nproduce explanations based on the change in the conﬁdence\\nof the original prediction; thus, avoid problems related with\\nnoise gradients. For instance, RISE [19] utilizes Monte Carlo\\nsampling to generate random masks, which are then used\\nto perturb the input image and generate a respective CNN\\nscore. Using the generated scores as weights, the explanation\\nis derived as the weighted combination of the various random\\nmasks. Thus, RISE, as most methods in this category, requires\\nmany forward passes through the network to generate an\\nexplanation, increasing the inference time considerably.\\nFinally, response-based methods [8], [11], [21], [27] use\\nfeature maps or activations of layers in the inference stage\\nto interpret the decision-making process of a neural network.', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='Finally, response-based methods [8], [11], [21], [27] use\\nfeature maps or activations of layers in the inference stage\\nto interpret the decision-making process of a neural network.\\nOne of the earliest methods in this category, CAM [29], uses\\nthe output of the global average pooling layer as weights,\\nand computes the weighted average of the features maps at\\nthe ﬁnal convolutional layer. CAM requires the existence of\\nsuch a global average pooling layer, restricting its applicability\\nto only this type of architectures. SISE [21], and later Ada-\\nSISE [27], aggregate feature maps in a cascading manner to\\nproduce explanation maps of any DCNN model. Similarly,\\nPoly-CAM [8] upscales feature maps to the dimension of the\\nlargest spatial dimension feature map and combines them in a\\ncascading manner. The above methods require many forward\\npasses to produce an explanation. L-CAM [11] mitigates\\nthe above limitation using a learned attention mechanism\\nto compute class-speciﬁc explanations in one forward pass.\\nHowever, it can only harness the salient information of one\\nset of feature maps. TAME also falls into the response-based\\ncategory and operates in one forward pass, but contrarily to\\n[11], it uses a trainable hierarchical attention module to exploit\\nfeature maps from multiple layers and generate explanations\\nof higher quality.\\nWe should also note that the methods of [9], [15] take a\\nsomewhat similar approach to ours in that they produce expla-', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='feature maps from multiple layers and generate explanations\\nof higher quality.\\nWe should also note that the methods of [9], [15] take a\\nsomewhat similar approach to ours in that they produce expla-\\nnations using an attention module and multiple sets of feature\\nmaps. However, these methods jointly train the attention model\\nwith the CNN to improve the image classiﬁcation task. In\\ncontrast, TAME does not modify the target model, which\\nhas been pretrained (and remains frozen); instead, TAME\\nfunctions as a post-hoc method, exclusively optimizing the\\nattention module in a supervised learning manner to generate\\nvisual explanations. Thus, no direct comparisons can be drawn\\nwith [9], [15] as they provide explanations for a different (i.e.\\nnot the initial pretrained one), concurrently trained classiﬁer.\\nIII. TAME\\nA. Problem formulation\\nLet f be a trained CNN for which we want to generate\\nexplanation maps,\\nf : I →RClasses,\\n(1)\\nwhere, I is the set of all possible input tensors I\\n=\\n{I | I : C × W × H →R}, C\\n=\\n{1, . . . , C}, W\\n=\\n{1, . . . , W}, H = {1, . . . , H}, C, W, H ∈R are the input\\ntensor dimensions [19], [21], and Classes is the number of\\nclasses that f has been trained to recognize. E.g., for RGB', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='tensor dimensions [19], [21], and Classes is the number of\\nclasses that f has been trained to recognize. E.g., for RGB\\nimages, C = 3, and the elements of a tensor instance are the\\nimage pixel values. Moreover, let Li : Ci × Wi × Hi →R\\nbe the feature map set corresponding to the ith layer of the\\nCNN, where, Ci, Wi, Hi are the respective channel, width and\\nheight dimensions. We deﬁne a feature map set {L}s, where s\\nis the set of layers for which we want to extract feature maps,\\ni.e., {L}s = {Li| i ∈s}.\\nAssume an attention module deﬁned as in the following,\\nAM : {L}s →E,\\n(2)\\nwhere, the tensor E at the output of the attention module is\\nthe generated explanation map, E : Classes × We × He →\\n{x | x ∈R ∩0 < x < 1}, We\\n=\\nmax {W}s and He\\n=\\nmax {H}s. Thus, explanation maps are class discriminative,\\nFig. 2: TAME’s attention module: Feature branches process feature maps to provide attention maps, which are concatenated\\nand processed by the fusion branch (shown at the bottom of the attention module) to derive explanation maps.\\nFig. 3: TAME’s training method. Var: Variation loss, Area:\\nArea loss, CE: Cross-entropy loss. The explanation of an', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='Fig. 3: TAME’s training method. Var: Variation loss, Area:\\nArea loss, CE: Cross-entropy loss. The explanation of an\\ninput image is ﬁrst derived; it is then upscaled and piece-wise\\nmultiplied with the corresponding input image. Subsequently,\\nthe masked image does a second forward pass through the\\nCNN to generate logits, which are used by the loss function to\\ncompute gradients and update the attention module’s weights.\\ni.e., each slice of E along its ﬁrst dimension corresponds to\\none of the classes that f has learned; moreover, the size of\\nthe spatial dimensions of these “class-speciﬁc” slices equal to\\nthe largest spatial dimensions in the set of feature maps.\\nGiven the above formulation, the goal is to ﬁnd an attention\\nmodule architecture that can combine all the salient informa-\\ntion contained in {L}s, and effectively train it.\\nB. Architecture\\nWe propose the attention module architecture depicted in\\nFig. 2. In this architecture, there exists a separate feature\\nbranch for each feature map set that is included in {L}s and\\none fusion branch. Each feature branch takes as input a single\\nfeature map set Li and outputs an attention map set Ai,\\nFB : Li →Ai,\\n(3)\\nwhere, Ai has the same channel and spatial dimensions as\\nLi and the ﬁnal explanation map, respectively, i.e., Ai : Ci ×', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='FB : Li →Ai,\\n(3)\\nwhere, Ai has the same channel and spatial dimensions as\\nLi and the ﬁnal explanation map, respectively, i.e., Ai : Ci ×\\nWe×He →R. The resulting attention maps are concatenated\\ninto an single attention map set {A}s, and forwarded into the\\nfusion branch to generate the explanation map,\\nFS : {A}s →E.\\n(4)\\nThe two branch types consist of different network components,\\nas described in the following:\\nFeature branch: Each feature branch is a neural network\\nthat prepares the feature maps for the fusion branch. It consists\\nof a 1 × 1 convolution layer with the same number of input\\nand output channels, a batch normalization layer, a skip\\nconnection, a ReLU activation, and a bilinear interpolation\\nthat upscales the feature map to match the ﬁnal explanation\\nmap’s dimensions (the ablation study presented in Section IV\\nassesses the importance of each part of the feature branch).\\nFusion branch: It consists of a 1 × 1 convolutional layer\\nthat brings the number of the inputted channels to the number\\nof classes that the CNN has been trained to recognize. Subse-\\nquently, a sigmoid activation function, S(x) =\\n1\\n1−e−x , is used\\nto scale the attention map values to the range (0, 1).\\nC. Training\\nThe training procedure is shown in Fig. 3. An image is\\ninputted to the CNN model, and the derived feature maps are', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='to scale the attention map values to the range (0, 1).\\nC. Training\\nThe training procedure is shown in Fig. 3. An image is\\ninputted to the CNN model, and the derived feature maps are\\nforwarded to the attention module for generating the respective\\nexplanation maps and the model truth label. A model truth\\nlabel is the CNN model’s prediction of the input image’s\\nclass, which may be different from the ground truth label. A\\nsingle channel containing a class discriminative explanation\\nis selected from the explanation map using the model truth\\nlabel; this is used as the explanation of the input image\\nwith respect to the model truth class. The explanation is\\nthen upscaled to the dimensions of the input image using\\nbilinear interpolation, and is piece-wise multiplied with the\\ninput image. The resulting masked image is then fed back into\\nthe CNN to generate logits. The logits, the original explanation\\nmaps, and the model truth labels are then used to compute the\\nloss and through backpropagation update the weights of the\\nattention module, effectively training it. As already mentioned,\\nthe weights of the original CNN remain ﬁxed to their original\\nvalues for the whole training procedure.\\nThe loss function used for training the proposed attention\\nmodule is the weighted sum of three individual loss functions,\\nL(Ψ, logits, labels)\\n=\\nλ1CE(logits, labels)\\n+ λ2Area(Ψ) + λ3Var(Ψ), (5)\\nwhere, Ψ is the slice of the explanation map E corresponding', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='=\\nλ1CE(logits, labels)\\n+ λ2Area(Ψ) + λ3Var(Ψ), (5)\\nwhere, Ψ is the slice of the explanation map E corresponding\\nto the model truth class of the input image, CE(), Area(), Var()\\nare the cross-entropy, area and variation loss, respectively, and\\nλ1, λ2, λ3, are the corresponding regularization parameters.\\nThe cross-entropy loss uses the logits generated from the CNN\\nwith the masked input image and the model truth label to\\ncompute a loss value. This term trains the attention module to\\nfocus on salient parts of the image. The variation loss is the\\nsum of the squares of the partial derivatives of the explanation\\nΨ in the x and y direction. This term penalizes fragmentation\\nin the generated heatmaps. For the partial derivatives, we use\\nthe forward difference approximation. To this end, in the x\\ndirection we have ∂Ψ[xm,ym]\\n∂x\\n≈Ψ[xm + 1, ym] −Ψ[xm, ym].\\nThus, using the forward difference approximation the variation\\nloss is deﬁned as,\\nVar(Ψ) =\\nX\\nx,y\\n\"\\x12∂Ψ[x, y]\\n∂x\\n\\x132\\n+\\n\\x12∂Ψ[x, y]\\n∂y\\n\\x132#\\n.\\n(6)\\nFinally, the area loss is the mean of the explanation map E', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='∂x\\n\\x132\\n+\\n\\x12∂Ψ[x, y]\\n∂y\\n\\x132#\\n.\\n(6)\\nFinally, the area loss is the mean of the explanation map E\\nto the Hadamard power of λ4, i.e.:\\nArea(Ψ) =\\nX\\nx,y\\nΨ[x, y]λ4.\\n(7)\\nThis term forces the attention module to output heatmaps that\\nemphasize small focused regions in the input image instead of\\narbitrarily large areas.\\nD. Inference\\nDuring inference, the ﬁnal sigmoid activation function in\\nthe attention module (Fig. 2) is replaced with a min-max\\nnormalization operator, m(x) =\\nx−min(Ψ)\\nmax(Ψ)−min(Ψ); the min()\\nand max() operators return the smallest and largest element\\nof the input tensor, respectively. This is done for consistency\\nwith other literature works, such as [5], [22], on how the ﬁnal\\nexplanation maps are scaled in order to be evaluated. The test\\nimage is then forward-passed through the CNN, producing\\nexplanation maps, which are then upscaled to the input image\\nsize. The derived model truth label can then be used to provide\\nan explanation concerning the decision of the classiﬁer.\\nIV. EXPERIMENTS\\nA. Datasets and CNNs\\nWe evaluate TAME on two popular CNNs pretrained on', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='an explanation concerning the decision of the classiﬁer.\\nIV. EXPERIMENTS\\nA. Datasets and CNNs\\nWe evaluate TAME on two popular CNNs pretrained on\\nImageNet: VGG-16 [23] and ResNet-50 [13]. We choose these\\ntwo models to test the generality of our method because\\nthere are signiﬁcant differences in the VGG and ResNet\\narchitectures. We obtain these pretrained networks using the\\ntorchvision.models library.\\nWe train the attention module of our method with the\\nImageNet ILSVRC 2012 dataset [7]. This dataset contains\\n1000 classes, 1.3 million and 50k images for training and\\nevaluation, respectively. Due to the prohibitively high cost of\\nexecuting the literature’s perturbation-based approaches that\\nwe use in the experimental comparisons, we use only 2000\\nrandomly-selected testing images for testing (the same as in\\n[11] to allow a fair comparison) and a different 2000 randomly\\nselected images as a validation set.\\nB. Evaluation measures\\nIn the experimental evaluation, two frequently used evalua-\\ntion measures, Increase in Conﬁdence (IC) and Average Drop\\n(AD) [5], are utilized,\\nAD(v) =\\nΥ\\nX\\ni=1\\nmax(0, f(Ii) −f(Ii ⊙φv(Ψi)))\\nΥf(Ii)\\n100,\\n(8)\\nIC(v) =\\nΥ\\nX', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='Υf(Ii)\\n100,\\n(8)\\nIC(v) =\\nΥ\\nX\\ni=1\\nsign(f(Ii ⊙φv(Ψi)) > f(Ii))\\nΥ\\n100,\\n(9)\\nwhere, φv() is a threshold function to select the v% higher-\\nvalued pixels of the explanation map, sign() returns 1 when\\nthe input condition is satisﬁed and 0 otherwise, Υ is the\\nnumber of test images, Ii is the ith test image and Ψi is the\\ncorresponding explanation produced by TAME or any other\\nmethod under evaluation. Intuitively, AD measures how much,\\non average, the produced explanation maps, when used to\\nmask the input images, reduce the conﬁdence of the model.\\nIn contrast, IC measures how often the explanation masks,\\nwhen used to mask the input images, increase the conﬁdence\\nof the model. We threshold the explanation maps to test how\\nwell the pixels of the explanation map are ordered based on\\nimportance. Thus, using a smaller threshold results in a much\\nmore challenging evaluation setup.\\nC. Experimental setup\\nTAME is applied to VGG-16 using feature maps from\\nthree different layers. The VGG-16 consists of ﬁve blocks\\nof convolutions separated by 2 × 2 max-pooling operations,\\nas shown in Fig. 4. We choose one layer from each of the\\nlast three blocks, namely the feature maps output by the max-', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='of convolutions separated by 2 × 2 max-pooling operations,\\nas shown in Fig. 4. We choose one layer from each of the\\nlast three blocks, namely the feature maps output by the max-\\npooling layers of each block. We have also experimented on\\nthe feature maps output by the last convolution layer of each\\nblock. On the other hand, ResNet-50 consists of ﬁve stages.\\nIn the experimental evaluation, we use the feature maps from\\nthe ﬁnal three stages.\\nTAME is trained using the loss function deﬁned in (5) with\\nthe SGD (Stochastic Gradient Descent) algorithm. The biggest\\nbatch size that can ﬁt in the graphics card’s memory is used,\\nas recommended in [25]. The learning rate is varied using the\\nOneCycleLR policy described in [26]. The maximum learning\\nrate used by the OneCycleLR policy is chosen using the LR\\nﬁnder test deﬁned in [24]. The hyperparameters of the loss\\nFig. 4: The layers from which feature map sets are extracted on VGG-16. We denote by “Convolutional Layers” the three\\nlayers before the last three max-pooling layers. In the case of VGG-16, the layer before a max-pooling layer is the ReLU\\nactivation function. We use the same layer naming as the library torchvision.models.feature_extraction.\\nfunction ((5), (7)) are empirically chosen using the validation', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='activation function. We use the same layer naming as the library torchvision.models.feature_extraction.\\nfunction ((5), (7)) are empirically chosen using the validation\\ndataset, as: λ1 = 1.5, λ2 = 2, λ3 = 0.01, λ4 = 0.3.\\nWe train the attention module for eight epochs in total and\\nselect the epoch for which the attention module achieved the\\nbest IC(15%) and AD(15%) in the validation set. That is, in\\nthis model selection procedure we opt for the measures at the\\n15% threshold because they are the most challenging measures\\nto improve upon and provide more focused explanation masks.\\nDuring training, each image is transformed in the same way\\nas with the original CNN, i.e., its smaller spatial dimension\\nis resized to 256 pixels, random-cropped to dimensions W =\\nH = 224, and normalized to zero mean and unit variance.\\nThe same is done during testing, except that center-cropping\\nis used. The feature maps are extracted from the CNN us-\\ning torchvision.models.feature_extraction li-\\nbrary.\\nD. Quantitative Evaluation\\nThe\\nproposed\\nmethod\\nis\\ncompared\\nagainst\\nthe\\ntop-\\nperforming approaches in the visual explanation domain for\\nwhich source code is publicly available i.e. Grad-CAM [22],\\nGrad-CAM++ [5], Score-CAM [28], RISE [19] and L-CAM', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='which source code is publicly available i.e. Grad-CAM [22],\\nGrad-CAM++ [5], Score-CAM [28], RISE [19] and L-CAM\\n[11]. The performance is measured using AD(v) and IC(v)\\non three different thresholds v of increasing difﬁculty, i.e.,\\nv = 100%, 50% and 15%, similarly to the evaluation protocol\\nof [11]. An ablation study is also conducted, to assess the\\nimportance of the different architecture components for VGG-\\n16 and ResNet-50, as well as to showcase the effect of different\\nlayer selections in the VGG-16 model.\\n1) Comparison with the State-Of-The-Art: In Table I we\\nhighlight with bold letters the best result and underline the\\nsecond best result for each measure, separately for each base\\nmodel. We can see that TAME outperforms the gradient-\\nbased methods, and is competitive to the perturbation-based\\nmethods, obtaining the best results for the more demanding\\n15% measures while requiring only one forward pass.\\n2) Ablation Study: In Table II we highlight with bold letters\\nthe best results and underline the second best result for each\\nmeasure in each model and layer selection. For the VGG-16\\nmodel, inspired from similar works in the literature suggesting\\nthat the last layers of the network provide more salient features\\n[15], we report two sets of experiments, one that uses features\\nextracted from the three last max-pooling layers and one where', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='that the last layers of the network provide more salient features\\n[15], we report two sets of experiments, one that uses features\\nextracted from the three last max-pooling layers and one where\\nfeatures are extracted from the layers exactly before the last\\nthree max-pooling layers (Fig. 4). There is a difference in the\\nspatial dimensions of the explanation maps generated using the\\nformer or the latter layers for feature extraction, i.e., 28 × 28\\nversus 56 × 56, since the dimension of the explanation maps\\nobtained by TAME is dictated by the largest feature map set (as\\nexplained in Section III). For the ResNet-50 model, we extract\\nfeatures from the outputs of the ﬁnal three stages, resulting to\\nan explanation map of 28×28 spatial dimensions. We examine\\nthe following variants of the proposed architecture:\\nNo skip connection: It has been shown that the skip\\nconnection promotes a smoother loss landscape [18], thus\\ncontributing to training very deep neural networks. Even for\\nshallower neural networks, such as the proposed attention\\nmodule, we can beneﬁt from using a skip connection. We see\\nthat by omitting the skip connection, we get worse results\\nin ResNet-50. Similarly, for both baseline models we report\\nworse performance for the harder 50% and 15% measures.\\nNo skip + No batch norm: Batch normalization is used\\nin CNNs for speeding up training and combating internal\\ncovariate shift [14]. Compared to the proposed architecture,\\nwe see that this variant generally performs better in the 100%', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='in CNNs for speeding up training and combating internal\\ncovariate shift [14]. Compared to the proposed architecture,\\nwe see that this variant generally performs better in the 100%\\nmeasures, but this does not hold for the other measures. We\\ncompare the masks produced by this variant in Fig. 5.\\nSigmoid in feature branch: In this variant we replace the\\nReLU function with the sigmoid function, which squeezes the\\ninput from (−∞, ∞) to the output (0, 1). It is well known\\nthat the sigmoid function in deeper neural networks causes the\\nvanishing gradient problem, making it more difﬁcult to train\\nthe early layers of the CNN. We see again that the proposed\\narchitecture prevails for the more challenging 15% measures.\\nTwo layers and One layer: In this case, the proposed\\nattention module architecture is employed with fewer feature\\nmaps. The results when using just one layer, i.e., omitting the\\ntwo earlier layers in the CNN pipeline (Fig. 4), are very similar\\nto the L-CAM-Img method (as shown in Table I), which also\\nTABLE I: Comparison of TAME with other methods\\nModel\\nMeasure\\nGrad-CAM [22]\\nGrad-CAM++ [5]\\nScore-CAM [28]\\nRISE [19]\\nL-CAM-Img [11]\\nTAME\\nVGG-16\\nAD(100%)\\n32.12\\n30.75\\n27.75\\n8.74\\n12.15\\n9.33', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='L-CAM-Img [11]\\nTAME\\nVGG-16\\nAD(100%)\\n32.12\\n30.75\\n27.75\\n8.74\\n12.15\\n9.33\\nIC(100%)\\n22.1\\n22.05\\n22.8\\n51.3\\n40.95\\n50\\nAD(50%)\\n58.65\\n54.11\\n45.6\\n42.42\\n37.37\\n36.5\\nIC(50%)\\n9.5\\n11.15\\n14.1\\n17.55\\n20.25\\n22.45\\nAD(15%)\\n84.15\\n82.72\\n75.7\\n78.7\\n74.23\\n73.29\\nIC(15%)\\n2.2\\n3.15\\n4.3\\n4.45\\n4.45\\n5.6\\nForward Passes (Inference)\\n1\\n1\\n512\\n4000\\n1\\n1\\nResNet-50\\nAD(100%)\\n13.61\\n13.63\\n11.01\\n11.12\\n11.09\\n7.81\\nIC(100%)\\n38.1\\n37.95\\n39.55\\n46.15\\n43.75\\n54\\nAD(50%)\\n29.28\\n30.37\\n26.8\\n36.31\\n29.12\\n27.88\\nIC(50%)\\n23.05\\n23.45\\n24.75\\n21.55\\n24.1\\n27.5\\nAD(15%)\\n78.61\\n79.58\\n78.72', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='27.88\\nIC(50%)\\n23.05\\n23.45\\n24.75\\n21.55\\n24.1\\n27.5\\nAD(15%)\\n78.61\\n79.58\\n78.72\\n82.05\\n79.41\\n78.58\\nIC(15%)\\n3.4\\n3.4\\n3.6\\n3.2\\n3.9\\n4.9\\nForward Passes (Inference)\\n1\\n1\\n2048\\n8000\\n1\\n1\\nTABLE II: Ablation study of TAME\\nModel\\nFeature Extraction\\nArchitecture Variant\\nAD(100%)\\nIC(100%)\\nAD(50%)\\nIC(50%)\\nAD(15%)\\nIC(15%)\\nVGG-16\\nMax-pooling layers\\nProposed Architecture\\n9.33\\n50\\n36.5\\n22.45\\n73.29\\n5.6\\nNo skip connection\\n10.09\\n45.25\\n36.44\\n20.65\\n74.85\\n5.15\\nNo skip + No batch norm\\n5.92\\n57.9\\n34.49\\n24.2\\n74.58\\n5.15\\nSigmoid in feature branch\\n7.22\\n55.65\\n38.4\\n21.6\\n79\\n4.85\\nTwo layers\\n10.72\\n45.45\\n34.48\\n23.05\\n71.94\\n5.75\\nOne layer\\n12.1\\n42.1\\n35.81\\n20.8\\n74.19\\n4.85\\nConvolutional layers', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='34.48\\n23.05\\n71.94\\n5.75\\nOne layer\\n12.1\\n42.1\\n35.81\\n20.8\\n74.19\\n4.85\\nConvolutional layers\\nProposed Architecture\\n9.07\\n51.1\\n40.72\\n20.9\\n77.05\\n4.8\\nNo skip connection\\n6.22\\n58.85\\n41.47\\n20.9\\n79.12\\n3.8\\nNo skip + No batch norm\\n6.62\\n56.6\\n40.48\\n20.75\\n77.84\\n4.95\\nSigmoid in feature branch\\n6.8\\n60\\n42.17\\n19.75\\n80.73\\n4.1\\nTwo layers\\n10.99\\n45.85\\n40.89\\n19.55\\n76.66\\n4.8\\nOne layer\\n13.09\\n39.65\\n42.3\\n17.7\\n78.02\\n3.8\\nResNet-50\\nStage Outputs\\nProposed Architecture\\n7.81\\n54\\n27.88\\n27.5\\n78.58\\n4.9\\nNo skip connection\\n5.7\\n62.65\\n46.58\\n18.25\\n89.32\\n2.3\\nNo skip + No batch norm\\n9.29\\n50.25\\n29.43\\n25.95\\n79.81\\n3.95\\nSigmoid in feature branch\\n9.11\\n53.3\\n45.68\\n18.1\\n86.95\\n3.15\\nTwo layers', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='29.43\\n25.95\\n79.81\\n3.95\\nSigmoid in feature branch\\n9.11\\n53.3\\n45.68\\n18.1\\n86.95\\n3.15\\nTwo layers\\n9.48\\n47.05\\n27.83\\n25\\n77.95\\n4.25\\nOne layer\\n11.32\\n43.45\\n29.85\\n24.25\\n79.59\\n3.55\\nuses just one feature map set. All measures are improved when\\nutilizing a second feature map set, i.e., excluding only the\\nearliest layer in the CNN pipeline; however, the case is not\\nthe same clear when going from the two to three feature map\\nsets, which are used in the proposed architecture. These mixed\\nresults could be attributed to the extra noise of feature maps\\ntaken earlier in a CNN pipeline.\\nWe note that by omitting both the skip connection and\\nthe batch normalization in the feature branch architecture,\\nwe obtain generally better results in the case of the VGG-\\n16 model, but this is not the case for the same architecture\\napplied to the ResNet-50 model. In addition, all architectures\\nstruggle with the more difﬁcult 15% measures compared to\\nthe proposed architecture. Although every architecture varies\\nbetween models, the proposed architecture generalizes best\\nacross different models. Thus, our goal of ﬁnding an effec-\\ntive architecture across radically different models is achieved\\nthrough the proposed architecture.\\nFig. 5: Qualitative comparison between the proposed attention', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='across different models. Thus, our goal of ﬁnding an effec-\\ntive architecture across radically different models is achieved\\nthrough the proposed architecture.\\nFig. 5: Qualitative comparison between the proposed attention\\nmodule and the ‘no skip + no batch norm’ variant, applied to\\nVGG-16. We observe that for the ‘no skip + no batch norm’\\narchitecture, the produced explanation map is more spread out,\\nshowing that even if it performs well on the 100% measures,\\nit fails to precisely identify the salient regions in the image.\\nE. Qualitative Analysis\\nAn extensive qualitative analysis is also performed using the\\nILSVRC 2012 ImageNet dataset in order to gain insight of the\\nFig. 6: TAME applied to ResNet-50 and VGG-16 for the\\nground truth class (Top image: “analog clock” (406), Bottom\\nimage: “padlock” (695). The explanation masks produced\\nusing the VGG-16 are more focused in comparison to the ones\\nof ResNet-50.\\nproposed approach and appreciate its usefulness in real-world\\napplications, e.g., understanding why an image was correctly\\nclassiﬁed or misclassiﬁed. The examples used in this study are\\ndepicted in Figs. 5, 6 and 7.\\nFig. 5 compares TAME generated explanation maps with\\nexplanations generated by the “No skip + No batch norm”\\narchitecture examined in Section IV-D2. The improved ability', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='Fig. 5 compares TAME generated explanation maps with\\nexplanations generated by the “No skip + No batch norm”\\narchitecture examined in Section IV-D2. The improved ability\\nof TAME to identify the salient image regions highlights the\\nimportance of evaluating the method using the AD and IC\\nmeasures on multiple thresholds (Table II), and particularly\\nthe signiﬁcance of the 15% measures over the 100% and 50%\\nones in determining the quality of generated explanations.\\nThe differences between explanations produced using\\nTAME on ResNet-50 and VGG-16 are examined in Fig. 6. We\\nobserve that explanations produced for the ResNet-50 model\\nare generally more activated, and, in general, explanations\\nproduced for the two different CNN types attend different\\nareas of the image. This suggests that ResNet-50 and VGG-16\\nclassify images in fundamentally different ways, focusing on\\ndifferent features of an input image to make their predictions.\\nIn Fig. 7, we provide class-speciﬁc explanation masks\\nreferring to the ground truth class but also to an erroneous\\nbut closely related class, for both ResNet-50 and VGG-16\\nmodels. The ﬁrst image of Fig. 7a, depicts a spoonbill, a bird\\nsimilar to the ﬂamingo. Two signiﬁcant differences between\\nthe spoonbill and the ﬂamingo are the characteristic bill and\\nthe darker pink stripe on the wing of the spoonbill. We can see', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='the spoonbill and the ﬂamingo are the characteristic bill and\\nthe darker pink stripe on the wing of the spoonbill. We can see\\nin the explanation maps of both models, that when choosing\\nthe class ﬂamingo, there is no signiﬁcance attributed to the\\nbill, but, on the other hand, when the spoonbill class is chosen,\\nthe bill area is gaining signiﬁcant attention. By comparing the\\nexplanation maps for adversarial classes, we can gain insight\\ninto important features which characterize a speciﬁc object\\nagainst similar ones, and possibly gain new insight from the\\nclassiﬁer. The second image in Fig. 7a is a similar case.\\nThe examples of Fig. 7b demonstrate the potential of the\\nexplanation maps to be used for explaining multiple different\\nclasses contained in a single image, i.e., the “english fox-\\nhound” and “soccer ball” image, and the “head cabbage” and\\n“butternut squash” image.\\nFinally, in Fig. 7c we provide two cases of images that have\\nbeen miscategorized, and use the explanations to understand\\nwhat has gone wrong. The ﬁrst image of Fig. 7c belongs to the\\n“dingo” class (273) but is evidently misclassiﬁed as “timber\\nwolf” from both CNN models. Using the explanations, we can\\nidentify important features on the image for each class and', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='wolf” from both CNN models. Using the explanations, we can\\nidentify important features on the image for each class and\\nCNN model. The second image depicts a lighthouse. VGG-16\\nmisclassiﬁes this image as a “sundial”. Again, using the expla-\\nnations generated by TAME we can understand which features\\nled the model to produce a wrong decision. For instance, in this\\ncase, we see that for both models the “sundial” explanations\\nfocus on the lighthouse roof, which might resemble a sundial,\\nexplaining the erroneous classiﬁcation decision of VGG-16.\\nV. CONCLUSIONS\\nWe proposed TAME, a novel method for generating visual\\nexplanations for various CNNs. This is accomplished by\\ntraining a hierarchical attention module to extract information\\nfrom feature map sets of multiple layers. Experimental results\\nveriﬁed that TAME outperforms gradient-based methods and\\ncompetes with perturbation-based, while, in contrast to them,\\nrequires only a single forward pass to generate explanations.\\nFurther research is needed to discover the limits of the pro-\\nposed approach, e.g., generalizing it to non-CNN architectures.\\nREFERENCES\\n[1] J. Adebayo, J. Gilmer, et al. Sanity checks for saliency maps. In Proc.\\nNIPS, page 9525–9536, Montr´\\neal, Canada, 2018.', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='NIPS, page 9525–9536, Montr´\\neal, Canada, 2018.\\n[2] J. Amann, A. Blasimme, et al. Explainability for artiﬁcial intelligence\\nin healthcare: a multidisciplinary perspective. BMC Medical Informatics\\nand Decision Making, 20(1):1–9, 2020.\\n[3] E. Apostolidis, G. Balaouras, V. Mezaris, and I. Patras. Summarizing\\nvideos using concentrated attention and considering the uniqueness and\\ndiversity of the video frames.\\nIn Proc. ICMR, pages 407–415, New\\nYork, NY, USA, June 2022.\\n[4] A. B. Arrieta, N. D´\\nıaz-Rodr´\\nıguez, et al. Explainable artiﬁcial intelligence\\n(XAI): Concepts, taxonomies, opportunities and challenges toward re-\\nsponsible ai. Information fusion, 58:82–115, 2020.\\n[5] A. Chattopadhay, A. Sarkar, et al. Grad-CAM++: Generalized gradient-\\nbased visual explanations for deep convolutional networks.\\nIn Proc.\\nIEEE WACV, pages 839–847, 2018.\\n[6] J.-Y. Chiao, K.-Y. Chen, et al. Detection and classiﬁcation the breast\\ntumors using mask R-CNN on sonograms. Medicine, 98(19), 2019.', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='tumors using mask R-CNN on sonograms. Medicine, 98(19), 2019.\\n[7] J. Deng, W. Dong, et al. ImageNet: A large-scale hierarchical image\\ndatabase.\\nIn Proc. IEEE CVPR, pages 248–255, Miami, USA, June\\n2009.\\n[8] A. Englebert, O. Cornu, and C. de Vleeschouwer. Backward recursive\\nclass activation map reﬁnement for high resolution saliency map. In\\nProc. ICPR, 2022.\\n[9] H. Fukui, T. Hirakawa, et al. Attention branch network: Learning of\\nattention mechanism for visual explanation. In Proc. IEEE CVPR, pages\\n10705–10714, 2019.\\n[10] N. Gkalelis, A. Goulas, D. Galanopoulos, and V. Mezaris. ObjectGraphs:\\nUsing objects and a graph convolutional network for the bottom-up\\nrecognition and explanation of events in video. In Proc. IEEE CVPRW,\\npages 3375–3383, June 2021.\\n(a)\\n(b)\\n(c)\\nFig. 7: Explanation for six input images. In each case we display four class-speciﬁc explanations, i.e., of the true (top) and\\nan erroneous (bottom) class prediction of the input image, for both ResNet-50 and VGG-16. Figs. 7a, 7b depict examples of', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='an erroneous (bottom) class prediction of the input image, for both ResNet-50 and VGG-16. Figs. 7a, 7b depict examples of\\nimages with similar classes and with images containing multiple classes, respectively. In Fig. 7c two cases of misclassiﬁcation\\nare provided: dataset misclassiﬁcation (left side example) and model misclasssiﬁcation (right side example).\\n[11] I. Gkartzonika, N. Gkalelis, and V. Mezaris. Learning visual explana-\\ntions for DCNN-based image classiﬁers using an attention mechanism.\\nIn Proc. ECCV, Workshop on Vision with Biased or Scarce Data (VBSD),\\nOct. 2022.\\n[12] R. Hamon, H. Junklewitz, I. Sanchez, et al. Bridging the gap between\\nAI and explainability in the GDPR: Towards trustworthiness-by-design\\nin automated decision-making. IEEE Computational Intelligence Mag-\\nazine, 17(1):72–85, 2022.\\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image\\nrecognition. In Proc. IEEE CVPR, pages 770–778, 2016.\\n[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network\\ntraining by reducing internal covariate shift. In Proc. ICML, pages 448–\\n456, 2015.', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network\\ntraining by reducing internal covariate shift. In Proc. ICML, pages 448–\\n456, 2015.\\n[15] S. Jetley, N. A. Lord, et al. Learn to pay attention. In Proc. ICLR,\\nVancouver, BC, Canada, May 2018.\\n[16] P.-T. Jiang, C.-B. Zhang, et al.\\nLayerCAM: Exploring hierarchical\\nclass activation maps for localization.\\nIEEE Transactions on Image\\nProcessing, 30:5875–5888, 2021.\\n[17] A. Krizhevsky, I. Sutskever, et al. ImageNet classiﬁcation with deep\\nconvolutional neural networks. In F. Pereira, C. J. Burges, L. Bottou,\\nand K. Q. Weinberger, editors, Proc. NIPS, volume 25, 2012.\\n[18] H. Li, Z. Xu, et al. Visualizing the loss landscape of neural nets. Proc.\\nNIPS, 31, 2018.\\n[19] V. Petsiuk, A. Das, and K. Saenko. RISE: randomized input sampling\\nfor explanation of black-box models. In Proc. BMVC, Newcastle, UK,\\nSeptember 2018.\\n[20] W. Samek, G. Montavon, et al. Explaining deep neural networks and\\nbeyond: A review of methods and applications. Proc. IEEE, 109(3):247–', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='September 2018.\\n[20] W. Samek, G. Montavon, et al. Explaining deep neural networks and\\nbeyond: A review of methods and applications. Proc. IEEE, 109(3):247–\\n278, 2021.\\n[21] S. Sattarzadeh, M. Sudhakar, et al. Explaining convolutional neural net-\\nworks through attribution-based input sampling and block-wise feature\\naggregation. In Proc. AAAI, volume 35, pages 11639–11647, 2021.\\n[22] R. R. Selvaraju, M. Cogswell, et al. Grad-CAM: Visual explanations\\nfrom deep networks via gradient-based localization.\\nIn Proc. IEEE\\nICCV, pages 618–626, 2017.\\n[23] K. Simonyan and A. Zisserman. Very deep convolutional networks for\\nlarge-scale image recognition. In Proc. ICLR, San Diego, CA, USA,\\nMay 2015.\\n[24] L. N. Smith. Cyclical learning rates for training neural networks. In\\nProc. IEEE WACV, pages 464–472, 2017.\\n[25] L. N. Smith. A disciplined approach to neural network hyper-parameters:\\nPart 1–learning rate, batch size, momentum, and weight decay. arXiv\\npreprint arXiv:1803.09820, 2018.\\n[26] L. N. Smith and N. Topin. Super-convergence: Very fast training of', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='preprint arXiv:1803.09820, 2018.\\n[26] L. N. Smith and N. Topin. Super-convergence: Very fast training of\\nneural networks using large learning rates. In Proc. Artiﬁcial intelligence\\nand machine learning for multi-domain operations applications, volume\\n11006, pages 369–386, 2019.\\n[27] M. Sudhakar, S. Sattarzadeh, et al. Ada-SISE: adaptive semantic input\\nsampling for efﬁcient explanation of convolutional neural networks. In\\nProc. IEEE ICASSP, pages 1715–1719, 2021.\\n[28] H. Wang, Z. Wang, et al. Score-CAM: Score-weighted visual explana-\\ntions for convolutional neural networks. In Proc. IEEE CVPRW, pages\\n24–25, 2020.\\n[29] B. Zhou, A. Khosla, et al. Learning deep features for discriminative\\nlocalization. In Proc. IEEE CVPR, pages 2921–2929, 2016.', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='DMFORMER: CLOSING THE GAP BETWEEN CNN AND VISION TRANSFORMERS\\nZimian Wei1, Hengyue Pan1, Lujun Li2, Menglong Lu1, Xin Niu1, Peijie Dong1, Dongsheng Li1\\n1 College of Computer, National University of Defense Technology\\n2 Chinese Academy of Sciences, Beijing, China\\nABSTRACT\\nVision transformers have shown excellent performance\\nin computer vision tasks. As the computation cost of their\\nself-attention mechanism is expensive, recent works tried\\nto replace the self-attention mechanism in vision transform-\\ners with convolutional operations, which is more efﬁcient\\nwith built-in inductive bias.\\nHowever, these efforts either\\nignore multi-level features or lack dynamic prosperity, lead-\\ning to sub-optimal performance. In this paper, we propose\\na Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel\\nsizes and enables input-adaptive weights with a gating mech-\\nanism.\\nBased on DMA, we present an efﬁcient backbone\\nnetwork named DMFormer.\\nDMFormer adopts the over-\\nall architecture of vision transformers, while replacing the\\nself-attention mechanism with our proposed DMA. Extensive\\nexperimental results on ImageNet-1K and ADE20K datasets\\ndemonstrated that DMFormer achieves state-of-the-art per-\\nformance, which outperforms similar-sized vision transform-\\ners(ViTs) and convolutional neural networks (CNNs).', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='demonstrated that DMFormer achieves state-of-the-art per-\\nformance, which outperforms similar-sized vision transform-\\ners(ViTs) and convolutional neural networks (CNNs).\\nIndex Terms—\\nvision transformer;\\nCNN; attention\\nmechanism; multi-level feature\\n1. INTRODUCTION\\nRecently, vision transformers (ViT) [1, 2] have drawn grow-\\ning attention in computer vision research. Due to the capac-\\nity of modeling long-range dependencies, ViTs are special-\\nized in extracting global features. However, the self-attention\\nmechanism in transformers brings about heavy computation\\ncosts, making them unaffordable for high-resolution down-\\nstream tasks(e.g., semantic segmentation). Although recent\\nlocal vision transformer methods [3, 4] alleviate this problem\\nto some extent, the implementation of cross-window strate-\\ngies in local self-attention is still sophisticated.\\nAs a complementary, convolution neural network(CNN)\\nfocus on capturing local relations with high efﬁciency. With\\nbuilt-in inductive biases, CNNs are easy to train with quick\\nconvergence. To this end, there is a trend to take the mer-\\nits of both CNNs and ViTs by migrating desired properties\\nof ViTs to CNNs, including the overall architecture design,\\nlarge receptive ﬁeld, and data speciﬁcity provided by the at-\\n\\x04\\n\\x02\\n\\x05\\n\\x02\\n\\x06\\n\\x02\\n\\x07\\n\\x02\\n\\x08\\n\\x02\\n\\t\\n\\x02', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='\\x02\\n\\x0b\\n\\x02\\n\\t\\n\\x0b\\n\\x01\\n\\x07\\n\\n\\x02\\n\\x01\\n\\x02\\n\\n\\x02\\n\\x01\\n\\x07\\n\\n\\x03\\n\\x01\\n\\x02\\n\\n\\x03\\n\\x01\\n\\x07\\n\\n\\x04\\n\\x01\\n\\x02\\n\\n\\x04\\n\\x01\\n\\x07\\n\\n\\x05\\n\\x01\\n\\x02\\n\\n\\x05\\n\\x01\\n\\x07\\n\\n\\x06\\n\\x01\\n\\x02\\n\\x08\\n\\x0b\\n\\x0c\\n\\x05\\n\\x06\\n\\x01\\n\\x07\\n\\n\\n\\x0e\\n\\r\\n\\t\\n\\n\\x0f\\n\\x03\\n\\x02\\n\\x04\\n\\n\\x11\\n\\x18\\n\\x11\\n\\x15\\n\\x12\\n\\x1a\\n\\x12\\n\\x18\\n\\x19\\n\\x02\\n\\x07\\n\\x03\\n\\x01\\n\\x0b\\n\\x12\\n\\x13\\n\\x08\\n\\x12\\n\\x1a\\n\\x10\\n\\x01\\n\\x0c\\n\\x1d\\n\\x14\\n\\x16\\n\\x01\\n\\x05\\n\\x17\\n\\x16\\n\\x1c\\n\\x08\\n\\x12\\n\\x0f\\n\\x1a\\n\\x01\\n\\x06\\n\\x12\\n\\x14\\n\\r\\n\\x01\\n\\x0b\\n\\x12\\n\\x19\\n\\x08\\n\\x12\\n\\x1a\\n\\x01', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='\\x0e\\n\\r\\n\\x1c\\n\\x04\\n\\x01\\n\\t\\n\\x1b\\n\\x18\\n\\x19\\nFig. 1.\\nResults of different models on ImageNet-1K valida-\\ntion set. We compare accuracy-parameters trade-off of recent\\nmodels RegNet [9], Swin Transformer [3], ConvNeXt [5],\\nDeiT [2], ResNet [6], PVTv2 [10] and our DMFormer.\\nTable 1. Favorable properties correspond to different mod-\\nules, including self-attention, multi-scale convolution, dilated\\nconvolution, and DMA.\\nProperties\\nSelf-\\nattention\\nMulti-scale\\nConvolution\\nDilated\\nConvolution\\nDMA\\nLocal inductive bias\\n\\x17\\n\\x13\\n\\x13\\n\\x13\\nLarge receptive ﬁeld\\n\\x13\\n\\x17\\n\\x13\\n\\x13\\nMulti-scale Interaction\\n\\x17\\n\\x13\\n\\x13\\n\\x13\\nInput Adaptive\\n\\x13\\n\\x17\\n\\x17\\n\\x13\\ntention mechanism. For example, ConvNext [5] built a pure\\nCNN family based on ResNet [6], which performs on par or\\nslightly better than ViT by learning their training procedure\\nand macro/micro-level architecture designs. RepLKNet [7]\\nadopts as large as 31 × 31 kernel size to enlarge effective\\nreceptive ﬁelds following the design in ViT. Although en-\\ncouraging performance has been achieved by the above meth-\\nods, their computation costs are relatively large. [8] com-', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='receptive ﬁelds following the design in ViT. Although en-\\ncouraging performance has been achieved by the above meth-\\nods, their computation costs are relatively large. [8] com-\\npetes favorably with Swin transformer [3] by replacing the\\nlocal self-attention layer with the dynamic depth-wise convo-\\nlution layer, while keeping the overall structure unchanged.\\nHowever, the lack of multi-level features limits its capacity to\\nachieve better performance.\\nIn this paper, we propose a new dynamic multi-level at-\\narXiv:2209.07738v3  [cs.CV]  29 Nov 2022\\n1x1 Conv\\n1x1 Conv\\n1x1 Conv\\nSoftmax\\n1x1 Conv\\n(a) Self-Attention\\n1x1 Conv\\n1x1 Conv\\nGAP\\nFC\\nFC\\n(d) DMA\\n1x1 Conv\\n1x1 Conv\\n1x1 Conv\\n(b) Multi-scale Convolution\\n3x3 DwConv 5x5 DwConv 7x7 DwConv\\n1x1 Conv\\n1x1 Conv\\n3x3 DwConv', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='5x5 DwConv\\nDilate rate=2\\n\\n7x7 DwConv\\nDilate rate=3\\nGating Mechnism\\n3x3 DwConv\\n\\n5x5 DwConv\\nDilate rate=2', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='7x7 DwConv\\nDilate rate=3\\n(c) Dilated Convolution\\nFig. 2. The structure comparison between related modules: self-attention (a), multi-scale convolution (b), dilated convolution\\n(c), and DMA (d). DwConv, GAP, σ, and FC refer to depth-wise convolution, global average pool, sigmoid function, and fully\\nconnected layer, respectively. C refers to the number of channels, while r is the expansion ratio in the DMA.\\ntention (DMA) mechanism (see Table 1 and Fig. 2 for more\\ndetails). Firstly, DMA is characterized by applying multi-\\nple kernel sizes for different resolution patterns. As men-\\ntioned in [11], multiple kernel sizes can effectively improve\\nthe model’s performance. Secondly, DMA involves dilated\\nconvolutions to efﬁciently enlarge the receptive ﬁeld. Thirdly,\\nwe design a lightweight gating mechanism in DMA to provide\\ninput adaptability, with which the channel-wise relationships\\nare captured and the representational power of the network\\nis enhanced. Based on DMA, we extend the architecture of\\nSwin Transformer [3] and propose a new framework named\\nDMFormer. The experimental results show that DMFormer\\nachieves state-of-the-art performance on ImageNet classiﬁca-\\ntion (see Fig. 1) and semantic segmentation tasks.\\nIn summary, the contributions of this paper have two\\naspects: 1) We propose a new attention mechanism named', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='tion (see Fig. 1) and semantic segmentation tasks.\\nIn summary, the contributions of this paper have two\\naspects: 1) We propose a new attention mechanism named\\nDMA, which combines the advantages of convolution and\\nself-attention.\\n2) Based on DMA, we design a backbone\\nmodel named DMFormer.\\nExtensive experiments on Ima-\\ngeNet classiﬁcation and semantic segmentation tasks show\\nthe superior of DMFormer over other existing models.\\n2. APPROACHES\\nIn this section, we ﬁrst present details of DMA. Then we show\\nthe overall structure and present different architecture designs\\nin DMFormer family.\\n2.1. DMA\\nWe illustrate the structure of DMA in Fig. 2 (d). The two\\nkey components in DMA are the multi-scale dilated convolu-\\ntion and gating mechanism. Multi-scale dilated convolution\\ncaptures different patterns with various resolutions of input\\nimages, while the gating mechanism learns to selectively em-\\nphasize informative features by re-calibration.\\nAssuming that the input of DMA is X. As depicted in\\nFig. 2 (d), a 1 × 1 convolution layer (Convexp r1 × 1) is\\napplied to expand the number of channels by r times. Then, a\\nparallel design of 3 × 3, 5 × 5, 7 × 7 depth-wise convolution\\nis introduced to learn multi-scale features. BatchNorm and\\nReLU are followed to prevent over-ﬁt when training. Next, in', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='is introduced to learn multi-scale features. BatchNorm and\\nReLU are followed to prevent over-ﬁt when training. Next, in\\norder to apply a residual connection for better optimization,\\nwe apply a 1 × 1 convolution layer to reduce the number of\\nchannels as the original input X. The above operators can be\\nexpressed as follows:\\nXE = Convexp r1 × 1 (X) ,\\nX1, X2,X3 = Parallel3×3,5×5,7×7 (XE) ,\\nXP = ReLU (BN (X1 + X2 + X3)) ,\\nX′ = X + Conv1 × 1 (XP ) ,\\n(1)\\nwhere Parallel3×3,5×5,7×7 contains multi-branch of 3 ×\\n3, 5 × 5, 7 × 7 convolution layers. Speciﬁcally, for branches\\nwith kernel size 5 × 5 and 7 × 7, we set the corresponding\\ndilated rates as 2, 3 to obtain larger receptive ﬁeld.\\nFor the gating mechanism, we apply a global average\\npooling (GAP) layer to obtain global information, followed\\nby two successive fully connected layers. At last, a sigmoid\\nfunction is applied to compute the attention vector. The oper-\\nations in the gating mechanism can be formulated as follows:\\nV = ReLU (FC (GAP (X))) ,\\nAttn = Sigmoid (FC (V )) ,\\n(2)', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='ations in the gating mechanism can be formulated as follows:\\nV = ReLU (FC (GAP (X))) ,\\nAttn = Sigmoid (FC (V )) ,\\n(2)\\nFinally, the output of DMA is obtained by re-calibrating\\nthe fused feature X′ with the gating mechanism as follows:\\nOutput = Attn ⊗Conv1 × 1\\n\\x00X′\\x01\\n,\\n(3)\\nwhere ⊗means the element-wise matrix multiplication.\\nIn the following section, we will introduce the basic\\nblock, overall framework, and detailed architectural design in\\nDMFormer family.\\n2.2. DMFormer\\nWe build DMFormer with a hierarchical design similar to tra-\\nditional CNNs [6] and recent local vision transformers [3].\\nTable 2. The detailed setting for different versions of DM-\\nFormer. ER, r represent the expansion ratio in the MLP mod-\\nule and DMA module, respectively.\\nstage\\noutput size\\nER\\nr\\nDMFormer-S\\nDMFormer-L\\n1\\nH\\n4 × W\\n4 × C1\\n8\\n4\\nC1 = 64\\nN1 = 2\\nC1 = 64\\nN1 = 3\\n2\\nH\\n8 × W\\n8 × C2\\n8\\n4\\nC2 = 128\\nN2 = 2\\nC2 = 128\\nN2 = 3\\n3\\nH\\n16 × W\\n16 × C3\\n4\\n4\\nC3 = 320\\nN3 = 6\\nC3 = 320\\nN3 = 12\\n4\\nH', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='N2 = 3\\n3\\nH\\n16 × W\\n16 × C3\\n4\\n4\\nC3 = 320\\nN3 = 6\\nC3 = 320\\nN3 = 12\\n4\\nH\\n32 × W\\n32 × C4\\n4\\n4\\nC4 = 512\\nN4 = 2\\nC4 = 512\\nN4 = 3\\nParameters (M)\\n26.7\\n45.0\\nFLOPs (G)\\n5.0\\n8.7\\nConv Stem\\nBlock\\nInput\\nStage 1\\nConv 3x3, Stride=2\\nConv 3x3, Stride=2\\nConv 3x3, Stride=2\\nStage 2\\nStage 3\\nStage 4\\nLinear\\n(b) Block\\nBlock\\nBlock\\nBlock\\n(a) Overall Framework\\nMLP\\nBatch Norm\\nDMA\\nBatch Norm\\nFig. 3. (a) The overall framework of DMFormer. Follow-\\ning [3], DMFormer adopts the hierarchical architecture with\\n4 stages, and each stage consists of multiple blocks. Ci, Ni\\nrefer to the feature dimension and block number in stage i,\\nrespectively. (b) The basic block in DMFormer. We apply the\\nmodular design in vision transformers, while replacing the\\nself-attention layer with DMA.\\nFig. 3 (a) presents the overall framework of DMFormer and\\nFig. 3 (b) shows the basic block in DMFormer.\\nThe input image I is ﬁrst processed by the convolution', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='Fig. 3 (a) presents the overall framework of DMFormer and\\nFig. 3 (b) shows the basic block in DMFormer.\\nThe input image I is ﬁrst processed by the convolution\\nstem module, which consists of a 7×7 convolution layer with\\na stride of 2, a 3 × 3 convolution layer with a stride of 1, and\\na non-overlapping 2 × 2 convolution layer with a stride of 2.\\nThen the spatial size of output features after the convolution\\nstem module is H\\n4 × W\\n4 .\\nX = ConvStem(I),\\n(4)\\nwhere X ∈RN×C1× H\\n4 × W\\n4 is the output feature of the con-\\nvolution stem module. N, C1 is the batch size and number\\nof channels. Then X is fed to repeated DMFormer blocks,\\neach of which consists of two sub-blocks. Speciﬁcally, the\\nmain components of the ﬁrst sub-block include DMA and the\\nBatchNorm module, which we present as\\nY = DMA(BN(X)) + X,\\n(5)\\nwhere BN(·) denotes a Batch Normalization.\\nDetails of\\nDMA(·) are depicted in Section 2.1. The second sub-block\\nconsists of two fully-connected layers and a non-linear activa-\\ntion GELU [12]. The output of the MLP module is formulated\\nas follows:\\nZ = GELU(W1(BN(Y ))W2 + Y,', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='tion GELU [12]. The output of the MLP module is formulated\\nas follows:\\nZ = GELU(W1(BN(Y ))W2 + Y,\\n(6)\\nTable 3. Compare with the state-of-the-art methods on the\\nImageNet validation set. Params means parameter. GFLOPs\\ndonates ﬂoating point operations.\\nMethod\\nParams. (M)\\nGFLOPs\\nTop-1 Acc (%)\\nPVT-Small [18]\\n24.5\\n3.8\\n79.8\\nSwin-T [3]\\n28.3\\n4.5\\n81.3\\nPoolFormer-S36 [19]\\n31.0\\n5.2\\n81.4\\nTwins-SVT-S [20]\\n24.0\\n2.8\\n81.7\\nFocal-T [14]\\n29.1\\n4.9\\n82.2\\nConvNeXt-T [5]\\n28.6\\n4.5\\n82.1\\nDMFormer-S\\n26.7\\n5.0\\n82.8\\nPVT-Medium [18]\\n44.2\\n6.7\\n81.2\\nFocal-S [14]\\n51.1\\n9.1\\n83.5\\nSwin-S [3]\\n49.6\\n8.7\\n83.0\\nConvNeXt-S [5]\\n50.0\\n8.7\\n83.1\\nDMFormer-L\\n45.0\\n8.7', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='49.6\\n8.7\\n83.0\\nConvNeXt-S [5]\\n50.0\\n8.7\\n83.1\\nDMFormer-L\\n45.0\\n8.7\\n83.6\\nwhere W1 ∈RCi×eCi and W2 ∈ReCi×Ci are learnable\\nparameters in fully connected layers. e is the MLP expansion\\nratio for the number of channels; Ci is the number of channels\\nin the corresponding stage.\\nBased on the above DMFormer block, we formulate\\nDMFormer-S and DMFormer-L with different model sizes.\\nThe numbers of channels corresponding to the four stages are\\nidentical for both DMFormer-S and DMFormer-L, which are\\nset as 64, 128, 320, and 512. Differently, DMFormer-L is\\nlarger with more block numbers. Speciﬁcally, stages 1, 2, 3,\\nand 4 of DMFormer-S contain 2, 2, 6, 2 blocks, while that\\nfor DMFormer-L are 3, 3, 12, 3, respectively. Their detailed\\narchitecture designs are shown in Table 2.\\n3. EXPERIMENTS\\n3.1. Image classiﬁcation\\nWe conduct image classiﬁcation experiments on ImageNet-\\n1K [13]. Each model is trained for 300 epochs with AdamW\\noptimizer, and a total batch size of 1024 on 8 GPUs. The\\ninitial learning rate is set as 1e −3. We adopt the cosine', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='1K [13]. Each model is trained for 300 epochs with AdamW\\noptimizer, and a total batch size of 1024 on 8 GPUs. The\\ninitial learning rate is set as 1e −3. We adopt the cosine\\ndecay schedule to adjust the learning rate during training.\\nTable 3 summarizes the experimental results on ImageNet-\\n1K classiﬁcation. Comparing with recently well-established\\nViTs like Swin-S [3] and Focal-S [14], DMFormer consis-\\ntently shows better performance. Speciﬁcally, DMFormer-L\\nsurpasses Swin-S, Focal-S by 0.6%, 0.1% top-1 accuracy with\\n9%, 12% fewer parameters. Comparatively, ConvNeXt [5] is\\nan excellent CNN that learns architecture designs and training\\nschedules from ViTs for better performance. DMFormer-L\\noutperforms ConvNeXt-S by 0.5%, while reducing the model\\nsize by 10%. Moreover, in Fig. 4 (a), we utilize Grad-CAM\\n[15] to localize discriminative regions generated by Swin-\\nT [16], ResNet50 [6] and DMFormer-S. It can be observed\\nthat the highlighted class activation area of DMFormer-S is\\nmore accurate.\\nThe experimental results demonstrate the\\nsuperiority of DMFormer.\\nResNet 50\\nSwin-T\\nOurs\\nInput\\n（a）\\n（b）', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='more accurate.\\nThe experimental results demonstrate the\\nsuperiority of DMFormer.\\nResNet 50\\nSwin-T\\nOurs\\nInput\\n（a）\\n（b）\\nFig. 4. (a) Grad-CAM [15] results generated by Swin-T [3],\\nResNet50 [6] and ours. The images are randomly selected\\nfrom the ImageNet validation dataset. Lighter colors in Grad-\\nCAM results refer to stronger attention regions. (b) Visualiza-\\ntion results of semantic segmentation on ADE20K [17] with\\nImageNet pre-trained DMFormer-S as the backbone.\\n3.2. Semantic segmentation\\nWe evaluate models for the semantic segmentation task on\\nADE20K [17]. mIoU (mean Intersection Over Union) is ap-\\nplied to measure the model performance. Semantic FPN [21]\\nand UperNet [22] are used as main frameworks to evaluate the\\nDMFormer-S backbone. Pre-trained weights on ImageNet-\\n1K are utilized to initialize our backbone.\\nWe train each\\nmodel with AdamW optimizer, a total batch size of 16 on\\n8 GPUs. When equipped with Semantic FPN [21], we use the\\n40K-iteration training scheme in [18, 19]. The learning rate\\nis set as 2 × 10−4 and decays by polynomial schedule with\\na power of 0.9. Comparatively, we adopt the 160K-iteration', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='is set as 2 × 10−4 and decays by polynomial schedule with\\na power of 0.9. Comparatively, we adopt the 160K-iteration\\ntraining scheme in [3] for UperNet. Speciﬁcally, the learning\\nrate is set as 6 × 10−5 with 1500 iteration warmup and linear\\nlearning rate decay.\\nTable 4 lists the performance of different backbones us-\\ning FPN and UperNet. Generally, DMFormer consistently\\noutperforms other state-of-the-art backbones.\\nWhen using\\nSemantic FPN for semantic segmentation, DMFormer-S sur-\\npasses VAN-B2 [24] by 0.5 mIoU with similar computation\\ncost. Moreover, although the parameter of DMFormer-S is\\n37% smaller than that of PVT-Medium [18], the mIoU is still\\n5.6 points higher (47.2 vs 41.6). When applying UperNet as\\nthe framework, our model outperforms ConvNeXt-T [5] by\\n0.9 mIOU, with about 6% model size reduction. Addition-\\nally, DMFormer-S is 1.8 mIOU higher than Focal-T [14] with\\n9% decreased parameters. In Fig. 4 (b), we present visualiza-\\ntion of semantic segmentation results applying UperNet [22].\\nThe results indicate the powerful capacity of DMFormer for\\nthe semantic segmentation task.\\nTable 4. Results of semantic segmentation on ADE20K [17]', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='tion of semantic segmentation results applying UperNet [22].\\nThe results indicate the powerful capacity of DMFormer for\\nthe semantic segmentation task.\\nTable 4. Results of semantic segmentation on ADE20K [17]\\nvalidation set.\\nThe pre-trained DMFormer-S is applied as\\nthe backbone and plugged in Semantic FPN [21] and Uper-\\nNet [22] frameworks. Note that the difference in the number\\nof parameters between two DMFormer-S is due to the usage\\nof semantic FPN and UperNet.\\nMethod\\nBackbone\\n#Param (M)\\nmIoU (%)\\nSemantic FPN [21]\\nResNet101 [6]\\n47.5\\n38.8\\nResNeXt101-32x4d [23]\\n47.1\\n39.7\\nPoolFormer-S36 [19]\\n34.6\\n42.0\\nPVT-Medium [18]\\n48.0\\n41.6\\nTwinP-S [20]\\n28.4\\n44.3\\nVAN-B2 [24]\\n30.0\\n46.7\\nDMFormer-S\\n30.4\\n47.2\\nUperNet [22]\\nConvNeXt-T [5]\\n60.0\\n46.7\\nSwin-T [3]\\n60.0\\n46.1\\nTwinP-S [20]\\n54.6\\n46.2\\nFocal-T [14]\\n62.0\\n45.8\\nDMFormer-S\\n56.6', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='60.0\\n46.1\\nTwinP-S [20]\\n54.6\\n46.2\\nFocal-T [14]\\n62.0\\n45.8\\nDMFormer-S\\n56.6\\n47.6\\nTable 5. Ablation study of different modules in DMA. Block\\nnumbers are modiﬁed to achieve comparable computation\\ncomplexity. We adopt Semantic FPN [21] as main frame-\\nwork, and use ImageNet pre-trained DMFormer-S variants as\\nbackbone for semantic segmentation. Parameters and FLOPs\\nare calculated under the ImageNet classiﬁcation setting.\\nVariants\\nBlocks\\nParams.\\nFLOPs\\nImageNet\\nADE20k\\n(M)\\n(G)\\nTop-1 Acc\\nmIoU\\nw/o Expansion rate r\\n2, 2, 12, 2\\n26.9\\n5.0\\n82.8\\n46.3\\nw/o Multi-scale Conv\\n2, 2, 6, 2\\n26.2\\n4.9\\n82.5\\n46.0\\nw/o Dilation\\n2, 2, 6, 2\\n26.7\\n5.0\\n82.4\\n46.4\\nw/o Gating Mechanism\\n2, 2, 7, 2\\n26.2\\n5.4\\n82.7\\n45.7\\nDMFormer-S\\n2, 2, 6, 2\\n26.7\\n5.0\\n82.8\\n47.2\\n3.3. Ablation studies', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='5.4\\n82.7\\n45.7\\nDMFormer-S\\n2, 2, 6, 2\\n26.7\\n5.0\\n82.8\\n47.2\\n3.3. Ablation studies\\nWe conduct ablation studies on each component of DMA\\nmodule to shed light on various architecture designs. DMFormer-\\nS is adopted as the baseline model. In the w/o multi-scale\\nConv variant, we only keep a single convolution branch with\\n7 × 7 kernel size. For w/o Expansion rate variant, we set r in\\nthe DMA (see Table 2 and Fig. 2) as 1. To achieve compa-\\nrable model complexity, we modify block numbers in some\\nvariants. The experimental results in Table 5 indicate that all\\ncomponents in DMA are non-trivial to improve performance.\\n4. CONCLUSION\\nIn this paper, we have introduced DMFormer, a novel efﬁcient\\nvision backbone that outperforms most similar-sized state-of-\\nthe-art models on ImageNet classiﬁcation and semantic seg-\\nmentation tasks. We have also presented DMA, the key com-\\nponent of DMFormer which shows much efﬁciency over the\\nexisting self-attention mechanisms. Future work may include\\nadapting DMFormer to other tasks such as video processing\\nand object detection, or extending DMA to a hybrid design\\nwith existing excellent CNN or self-attention mechanisms to\\nfurther enhance performance.\\n5. REFERENCES\\n[1] Alexey', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='and object detection, or extending DMA to a hybrid design\\nwith existing excellent CNN or self-attention mechanisms to\\nfurther enhance performance.\\n5. REFERENCES\\n[1] Alexey\\nDosovitskiy,\\nLucas\\nBeyer,\\nAlexander\\nKolesnikov, and Dirk Weissenborn,\\n“An image is\\nworth 16x16 words: Transformers for image recogni-\\ntion at scale,” in International Conference on Learning\\nRepresentations, 2020.\\n[2] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\\nand Herv´\\ne J´\\negou, “Training data-efﬁcient image trans-\\nformers & distillation through attention,”\\nin Interna-\\ntional Conference on Machine Learning. PMLR, 2021,\\npp. 10347–10357.\\n[3] Ze Liu, Yutong Lin, Yue Cao, and Baining Guo,\\n“Swin transformer: Hierarchical vision transformer us-\\ning shifted windows,” in Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision (ICCV),\\nOctober 2021, pp. 10012–10022.\\n[4] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei\\nCheng, Gang Yu, and Bin Fu, “Shufﬂe transformer: Re-\\nthinking spatial shufﬂe for vision transformer,” arXiv\\npreprint arXiv:2106.03650, 2021.', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='thinking spatial shufﬂe for vision transformer,” arXiv\\npreprint arXiv:2106.03650, 2021.\\n[5] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Fe-\\nichtenhofer, Trevor Darrell, and Saining Xie, “A convnet\\nfor the 2020s,” arXiv preprint arXiv:2201.03545, 2022.\\n[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\\nSun, “Deep residual learning for image recognition,” in\\nCVPR, 2016, pp. 770–778.\\n[7] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and\\nGuiguang Ding, “Scaling up your kernels to 31x31: Re-\\nvisiting large kernel design in cnns,” in CVPR, 2022, pp.\\n11963–11975.\\n[8] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng,\\nJiaying Liu, and Jingdong Wang, “On the connection\\nbetween local attention and dynamic depth-wise convo-\\nlution,” in International Conference on Learning Rep-\\nresentations, 2021.\\n[9] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\\nKaiming He, and Piotr Doll´\\nar, “Designing network de-\\nsign spaces,” 2020, pp. 10428–10436.', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='Kaiming He, and Piotr Doll´\\nar, “Designing network de-\\nsign spaces,” 2020, pp. 10428–10436.\\n[10] Wenhai Wang, Enze Xie, Tong Lu, Ping Luo, and Ling\\nShao, “Pvtv2: Improved baselines with pyramid vision\\ntransformer,” arXiv preprint arXiv:2106.13797, 2021.\\n[11] Mingxing Tan and Quoc V Le,\\n“Mixconv: Mixed\\ndepthwise convolutional kernels,”\\narXiv preprint\\narXiv:1907.09595, 2019.\\n[12] Dan Hendrycks and Kevin Gimpel,\\n“Gaussian error\\nlinear units (gelus),” arXiv preprint arXiv:1606.08415,\\n2016.\\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei, “Imagenet: A large-scale hierarchical\\nimage database,” in 2009 IEEE conference on computer\\nvision and pattern recognition. Ieee, 2009, pp. 248–255.\\n[14] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang\\nDai, Bin Xiao, Lu Yuan, and Jianfeng Gao, “Focal self-\\nattention for local-global interactions in vision trans-\\nformers,” arXiv preprint arXiv:2107.00641, 2021.', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='attention for local-global interactions in vision trans-\\nformers,” arXiv preprint arXiv:2107.00641, 2021.\\n[15] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek\\nDas, Ramakrishna Vedantam, Devi Parikh, and Dhruv\\nBatra, “Grad-cam: Visual explanations from deep net-\\nworks via gradient-based localization,” 2017, pp. 618–\\n626.\\n[16] Ze Liu, Yutong Lin, Yue Cao, Stephen Lin, and Bain-\\ning Guo, “Swin transformer: Hierarchical vision trans-\\nformer using shifted windows,” 2021.\\n[17] Bolei Zhou, Hang Zhao, and Antonio Torralba, “Scene\\nparsing through ade20k dataset,” in CVPR, 2017, pp.\\n633–641.\\n[18] Wenhai Wang, Enze Xie, Ping Song, and Ling Shao,\\n“Pyramid vision transformer: A versatile backbone for\\ndense prediction without convolutions,” in ICCV, Octo-\\nber 2021, pp. 568–578.\\n[19] Weihao Yu, Mi Luo, and Shuicheng Yan, “Metaformer\\nis actually what you need for vision,” in CVPR, 2022,\\npp. 10819–10829.\\n[20] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang,', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='is actually what you need for vision,” in CVPR, 2022,\\npp. 10819–10829.\\n[20] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang,\\nHaibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua\\nShen, “Twins: Revisiting the design of spatial attention\\nin vision transformers,” vol. 34, 2021.\\n[21] Alexander Kirillov, Ross Girshick, Kaiming He, and Pi-\\notr Doll´\\nar, “Panoptic feature pyramid networks,” 2019,\\npp. 6399–6408.\\n[22] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang,\\nand Jian Sun, “Uniﬁed perceptual parsing for scene un-\\nderstanding,” 2018, pp. 418–434.\\n[23] Saining Xie, Ross Girshick, Piotr Doll´\\nar, Zhuowen Tu,\\nand Kaiming He, “Aggregated residual transformations\\nfor deep neural networks,” in CVPR, 2017, pp. 1492–\\n1500.\\n[24] Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-\\nMing Cheng, and Shi-Min Hu, “Visual attention net-\\nwork,” arXiv preprint arXiv:2202.09741, 2022.', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='JOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n1\\nSpeech Emotion Recognition Via CNN-Transforemr\\nand Multidimensional Attention Mechanism\\nXiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng\\nAbstract—Speech Emotion Recognition (SER) is crucial in\\nhuman-machine interactions. Mainstream approaches utilize\\nConvolutional Neural Networks or Recurrent Neural Networks\\nto learn local energy feature representations of speech segments\\nfrom speech information, but struggle with capturing global\\ninformation such as the duration of energy in speech. Some use\\nTransformers to capture global information, but there is room\\nfor improvement in terms of parameter count and performance.\\nFurthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal\\ninformation in speech. In this paper, to model local and global\\ninformation at different levels of granularity in speech and cap-\\nture temporal, spatial and channel dependencies in speech signals,\\nwe propose a Speech Emotion Recognition network based on\\nCNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing\\nlocal information in speech from a time-frequency perspective.\\nIn addition, a time-channel-space attention mechanism is used to\\nenhance features across three dimensions. Moreover, we model\\nlocal and global dependencies of feature sequences using large\\nconvolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='enhance features across three dimensions. Moreover, we model\\nlocal and global dependencies of feature sequences using large\\nconvolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed\\nmethod on IEMOCAP and Emo-DB datasets and show our\\napproach significantly improves the performance over the state-\\nof-the-art methods1.\\nIndex Terms—Speech emotion recognition, temporal-channel-\\nspatial attention, lightweight convolution transformer, local global\\nfeature fusion.\\nI. INTRODUCTION\\nE\\nMOTION recognition has significant importance in var-\\nious fields, especially in increasingly common human-\\ncomputer interaction systems [1], and speech emotion recogni-\\ntion (SER) has promising applications in areas such as mental\\nhealth monitoring [2], educational assistance, personalized\\ncontent recommendation, and customer service quality mon-\\nitoring. Speech contains rich emotional information, and as\\nCorresponding author: Xiaoyu Tang. E-mail address: tangxy@scnu.edu.cn.\\nXiaoyu Tang is with the School of Electronic and Information Engineering,\\nFaculty of Engineering, South China Normal University, Foshan, Guangdong\\n528225, China, and also with the School of Physics and Telecommunica-\\ntions Engineering, South China Normal University, Guangzhou, Guangdong\\n510000, China.\\nYixin Lin is with the School of Electronic and Information Engineering,\\nFaculty of Engineering, South China Normal University, Foshan, Guangdong\\n528225, China.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='510000, China.\\nYixin Lin is with the School of Electronic and Information Engineering,\\nFaculty of Engineering, South China Normal University, Foshan, Guangdong\\n528225, China.\\nTing Dang is with the Nokia Bell Labs, Cambridge, UK.\\nYuanfang Zhang is with the Autocity (Shenzhen) Autonomous Driving\\nCo.,ltd.\\nJintao Cheng is with the School of Physics and Telecommunications En-\\ngineering, South China Normal University, Guangzhou, Guangdong 510000,\\nChina.\\n1Our\\ncode\\nis\\navailable\\non\\nhttps://github.com/SCNU-RISLAB/CNN-\\nTransforemr-and-Multidimensional-Attention-Mechanism\\none of the most basic human communication methods, speech\\nemotion recognition is particularly important for computers to\\nanalyze and respond to the emotional state of human users and\\nrespond to them accordingly. With the rapid development of\\nartificial intelligence, speech emotion recognition has received\\nextensive research attention. Human speech contains a wealth\\nof information, including not only the language content but\\nalso attributes such as gender and emotions of the speaker.\\nIt is of great significance to accurately identify emotional\\ninformation from speech signals.\\nFeature extraction of speech is a rather important and\\nchallenging task in speech emotion recognition, and the ex-\\ntraction of features directly affects the effectiveness of subse-\\nquent model training and the accuracy of the final algorithm\\nfor emotion recognition. Speech features can be categorized\\nas acoustic-based features and deep learning-based features', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='traction of features directly affects the effectiveness of subse-\\nquent model training and the accuracy of the final algorithm\\nfor emotion recognition. Speech features can be categorized\\nas acoustic-based features and deep learning-based features\\nwhere acoustic-based features can be broadly classified into\\nrhythmic features [3], spectral-based correlation features [4],\\nand phonetic features [5]. Among them, spectral-based corre-\\nlation features reflect the characteristics of the signal in the\\nfrequency domain, where there are differences in the perfor-\\nmance of different emotions in the frequency domain. Based\\non the spectral correlation features include linear spectrum\\n[6] and inverse spectrum [7], Linear Prediction Cofficients\\n(LPC), Log Frequency Power Coefficients (LFPC), etc.; In-\\nverse spectrum includes Mel-Frequency Cepstrum Coefficients\\n(MFCC), Linear Prediction Cepstrum Cofficients (LPCC), etc.\\nAmong them, MFCC is regarded as a low-level feature based\\non human knowledge, which is widely used in the field of\\nspeech.\\nEarly SER algorithms mainly used acoustic-based features\\nand combined with traditional machine learning algorithms,\\nincluding hidden Markov models [8], Gaussian mixture mod-\\nels [9], and support vector machines [10]. In recent years,\\ndeep learning-based neural networks have gradually become\\nactive in the field of speech emotion recognition [11], [12], and\\ncompared with traditional models, deep learning-based models\\nhave shown better performance in speech emotion recognition.\\nDeep learning-based features use neural networks to learn', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='active in the field of speech emotion recognition [11], [12], and\\ncompared with traditional models, deep learning-based models\\nhave shown better performance in speech emotion recognition.\\nDeep learning-based features use neural networks to learn\\nmore advanced features from the original signal of speech or\\nsome low-dimensional acoustic features. Convolutional neural\\nnetworks (CNNs) are effective in capturing local acoustic\\ndetails in speech, while long short-term memory networks\\n(LSTMs) are widely used in speech emotion recognition for\\nmodeling dynamic information and temporal dependencies in\\nspeech. Additionally, attention mechanisms are also a key\\n0000–0000/00$00.00 © 2021 IEEE\\narXiv:2403.04743v1  [eess.AS]  7 Mar 2024\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n2\\nfactor in improving model performance, as they can adaptively\\nfocus on the importance of different features to obtain better\\nspeech features at the discourse level. For example, Qi et\\nal. [13] proposed a hierarchical network based on static and\\ndynamic features, which uses LSTM to encode dynamic and\\nstatic features of speech and designs a gating model to fuse\\nthe features, an attention mechanism is used to acquire the\\ndiscourse-level speech features. Liu et al. [14] proposed a\\nlocal-global perceptual depth representation learning system.\\nOne module contains a multiscale CNN and a time-frequency\\nCNN (TFCNN) to learn the local representation, and in the', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='local-global perceptual depth representation learning system.\\nOne module contains a multiscale CNN and a time-frequency\\nCNN (TFCNN) to learn the local representation, and in the\\nother module, a Capsule network with an improved routing\\nalgorithm is utilized to design a multi-block dense connection\\nstructure, which can learn both shallow and deep global\\ninformation.\\nAlthough speech emotion recognition (SER) models com-\\nposed of CNNs exhibit better performance than traditional\\nmodels, these networks can only extract local information in\\nspeech, such as the energy and rhythm of a particular segment\\nof speech, while struggling to learn global information in\\nspeech features, such as the overall volume and speaking rate\\nof the speaker, and the duration of energy, thus neglecting the\\nglobal correlation of features [15]. In recent years, the trans-\\nformer [16] based on the self-attentive mechanism has been\\nwidely used in major deep learning tasks. Tarantino et al. [17]\\nused transformer in combination with global windowing for\\nspeech emotion recognition and achieved better performance,\\nbut transformer is weak for local feature extraction. Some\\nrecent work has attempted to combine CNN and transformer\\nto alleviate the limitations of using CNN and transformer\\nalone. Wang et al. [18] stacked the transformer blocks after\\nthe CNN model to improve the global features of aggregation.\\nA model combining the transformer and CNN is proposed in\\n[19], enabling it to learn local information while capturing\\nglobal dependencies. The original transformer tends to have a\\nhigh number of parameters for computing multi-headed self-\\nattention, which requires a lot of resources and poses some', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='[19], enabling it to learn local information while capturing\\nglobal dependencies. The original transformer tends to have a\\nhigh number of parameters for computing multi-headed self-\\nattention, which requires a lot of resources and poses some\\ndifficulties in the training of the network. In addition, this\\nkind of work tends to stack transformers in the last part of\\nthe model or a simple combination of transformer and CNN,\\nwhich makes it hard to obtain better local information of the\\nspeech.\\nIn addition, attention mechanisms improve the effectiveness\\nof task processing by selectively attending to features that are\\nmost relevant to the current task, and have received widespread\\nattention in major fields. In recent years, several researchers\\nhave utilized deep learning methods for feature extraction\\nand used attention mechanisms to improve performance. A\\nlightweight self-attention module is proposed in [20], which\\nuses MLP to extract channel information and a large perceptual\\nfield extended CNN to extract spatial contextual information.\\nGuo et al. [21] proposed an attention mechanism based on\\ntime, frequency, and CNN channels to improve representa-\\ntion learning ability. However, temporal information is often\\nembedded in speech, which reflects the dynamic changes of\\nspeech, such as pitch and energy variations over time. Tem-\\nporal features can reflect the temporal context and evolution\\nof emotion expression in speech. However, it falls short in\\ncapturing the temporal information present in speech, which\\nrepresents the dynamics of speech and plays a crucial role in\\nemotion recognition. This limitation is a common issue in both\\nMLPs and CNNs.\\nIn this paper, we investigate how to effectively combine', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='represents the dynamics of speech and plays a crucial role in\\nemotion recognition. This limitation is a common issue in both\\nMLPs and CNNs.\\nIn this paper, we investigate how to effectively combine\\ntransformer and CNN and apply them to SER to characterize\\nlocal features and global features in speech signals, and\\npropose the temporal-channel-space attention mechanism in\\nthe model for multiple dimensions of feature enhancement.\\nSpecifically, we first use a set of stacked CNNs to capture local\\ninformation in speech and learn shallow features of speech for\\nthe transformer module for better training of the transformer\\nmodule. In the stacked CNN module, two sets of convolutional\\nfilters of different shapes are used to capture both temporal\\nand frequency domain contextual information. Specifically,\\nafter stacking the CNN modules, we introduced a temporal-\\nchannel-space attention mechanism that models the contextual\\nemotional expression of features over time, and efficiently\\nfuses the attention of the spatial and channel dimensions of\\nthe speech feature map through the Shuffle unit. Furthermore,\\na combination of transformer and CNN is used to model\\nthe local and global dependencies of feature sequences by a\\ndeep separable convolution with residuals and a lightweight\\ntransformer module. The main contributions of this work are\\nsummarized as follows:\\n• A framework based on CNN and transformer is proposed\\nfor speech emotion recognition. Our framework uses\\ntime-frequency domain convolution and stacked convolu-\\ntion blocks to extract initial local features of speech and\\nstacked CNN and transformer blocks are used to enhance\\nlocal and global features.\\n• To enhance the finiteness of the feature map and model', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='tion blocks to extract initial local features of speech and\\nstacked CNN and transformer blocks are used to enhance\\nlocal and global features.\\n• To enhance the finiteness of the feature map and model\\nthe temporal information of speech, a temporal-channel-\\nspace attention mechanism called Time-Shuffle Attention\\n(T-Sa) is used in our model. T-Sa enhances the feature\\nmap in multi-dimensions.\\n• We propose a module based on deeply separable convo-\\nlution and a lightweight transformer called Lightweight\\nconvolution\\ntransformer(LCT).\\nThis\\nmodel\\nemploys\\nlightweight convolutional blocks to efficiently extract\\nlocal information from features, and incorporates Coor-\\ndinate Attention (CA) into the multi-head self-attention\\nmechanism to capture long-range dependencies among\\nfeatures while enhancing their temporal and spectral\\ninformation.\\n• Extensive experiments of our proposed model on IEMO-\\nCAP and EMO-DB datasets demonstrate the effectiveness\\nof the model in SER tasks.\\nThe rest of the paper is organized as follows. Section\\nII briefly reviews related work. Details of the system are\\npresented in Section II. In Section IV,we present experimental\\nresults to showcase the effectiveness of our model on two\\nwidely-used benchmark datasets. Section V concludes this\\nwork.\\nII. RELATED WORK\\nIn this section, we will briefly review the algorithms related\\nto speech emotion recognition, namely convolutional and\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n3', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='to speech emotion recognition, namely convolutional and\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n3\\nrecurrent neural networks, attention mechanism, and trans-\\nformer.\\nA. Convolutional neural networks and recurrent neural net-\\nworks\\nSpeech is a continuous time-series signal, and CNN and\\nRNN have been two main network structures for SER. Moti-\\nvated by the studies of CNN in computer vision, AlexNet [22]\\nand ResNet [23] show promising results in image classification\\ntasks and therefore have been studies in SER. Zhu et al.\\n[24] has proposed a new Global Aware Multi-scale (GLAM)\\nneural network that utilizes a global perception fusion module\\nto learn multi-scale feature representations, with a focus on\\nemotional information. The multi-time-scale (MTS) method\\nwas introduced in [25], which extends the CNNs by scaling\\nand resampling the convolutional kernel along the time axis\\nto increase temporal flexibility. Liu et al. [14] proposed a\\nlocal global-aware deep representation learning system that\\nuses CNNs and Capsule Networks to learn local and deep\\nglobal information.\\nRNN can model the temporal information in speech and\\ncapture long-term dependencies in the speech signal more\\neffectively. A new layered network HNSD was proposed [13]\\nthat can efficiently integrate static and dynamic features of\\nSER, which uses LSTM to encode static and dynamic features\\nand gated multi-features unit (GMU) for frame level feature', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='that can efficiently integrate static and dynamic features of\\nSER, which uses LSTM to encode static and dynamic features\\nand gated multi-features unit (GMU) for frame level feature\\nfusion for the emotional intermediate representation. Xu et\\nal. [26] proposed a hierarchical grained and feature model\\n(HGFM) that uses recurrent neural networks to process both\\ndiscourse-level and frame-level information of the speech.\\nSince convolutional neural networks can capture local in-\\nformation of features, while recurrent neural networks take\\nadvantage of modeling temporal information, many works\\nhave combined these two approaches and achieved outstanding\\nresults. Li [27] extracted location information from MFCC\\nfeatures and VGGish features by bi-direction long short time\\nmemory (BiLSTM) neural network, and then fused these two\\nfeatures to predict emotions. Liu et al. [28] combined triplet\\nloss and CNN-LSTM models to obtain more discriminative\\nsentiment information, and the proposed framework yielded\\nexcellent results in experiments. Zou [29] et al. used CNN,\\nBiLSTM, and wav2vec2 to extract different levels of speech\\ninformation, including MFCC, spectrogram, and acoustic in-\\nformation, and fused these three features by an attention\\nmechanism. Zhang [30] et al. used CNNs to learn segment-\\nlevel features in spectrograms, using a deep LSTM model to\\ncapture temporal dependencies between speech segments.\\nB. Attention mechanism\\nIn recent years, attention mechanisms have received a lot of', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='level features in spectrograms, using a deep LSTM model to\\ncapture temporal dependencies between speech segments.\\nB. Attention mechanism\\nIn recent years, attention mechanisms have received a lot of\\nattention in major fields to improve the effectiveness of task\\nprocessing by focusing on information that is more critical to\\nthe current task among the many inputs. A channel attention\\nmechanism called Squeeze-and-Excitation (SE) was proposed\\nin [31], which assigns weights to individual channels and adap-\\ntively recalibrates the feature responses of the channels. Woo\\net al. [32] proposed a convolutional block attention module\\nthat combines both spatial and channel dimensions to obtain\\nattention with better results. In addition, some researchers have\\nused deep learning methods for feature extraction of speech\\nand enhancement of feature maps using attention mechanisms.\\nAn attention pooling-based approach was proposed in [33],\\ncompared to existing average and maximum pooling, it can\\ncombine both class-agnostic bottom-up attention maps and\\nclass-specific top-down attention maps in an effective manner.\\nMustaqeem et al. [20] proposed a self-attentive module (SAM)\\nfor SER systems,which uses a multilayer perceptron (MLP)\\nto recognize global information of the channels and identifies\\nspatial information using a special dilated CNN to generate\\nan attentional map for both channels. SAM significantly re-\\nduces the computational and parametric overhead. A spectro-\\ntemporal-channel (STC) attention mechanism was proposed in', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='an attentional map for both channels. SAM significantly re-\\nduces the computational and parametric overhead. A spectro-\\ntemporal-channel (STC) attention mechanism was proposed in\\n[21], which acquires attention feature maps along three dimen-\\nsions: time, frequency, and channel. Xi et al. [34] employed an\\nattention mechanism based on the time and frequency domain\\nto introduce long-distance contextual information.\\nThe current attention mechanisms typically focus more\\non spatial or channel information in feature maps, often\\nneglecting the temporal characteristics in speech. However,\\ntemporal features in speech are equally important for emotion\\nrecognition. Therefore, it is necessary to pay more attention to\\ntemporal information in attention mechanisms to better explore\\nand utilize the temporal characteristics in speech signals.\\nC. Transformer\\nTransformer has been rapidly developing in the field of\\nnatural language processing (NLP) in recent years and has\\nachieved great success. Due to its powerful ability to obtain\\nglobal information, the transformer has been gradually ex-\\ntended to the fields of speech. An end-to-end speech emotion\\nrecognition model [18] was proposed to enhance the global\\nfeature representation of the model by using stacked trans-\\nformer blocks at the end of the model. Hu et al. [19] took\\nadvantage of multiple models, improved the learning ability\\nof the model by residual BLSTM, and proposed a convolu-\\ntional neural network and E-transformer module to learn both\\nlocal and global information. Recently, transformer-based self-\\nsupervised methods have also been applied to speech, and', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='tional neural network and E-transformer module to learn both\\nlocal and global information. Recently, transformer-based self-\\nsupervised methods have also been applied to speech, and\\nsome transformer-based models have achieved great success in\\nautomatic speech recognition (ASR), including wav2vec [35],\\nVQ-wav2vec [36], and wav2vec2.0 [37]. There is also some\\nwork in speech emotion recognition that employs these models\\nfor migration learning. A pre-trained wav2vec2.0 model [38]\\nis used as the input to the network and the outputs of multiple\\nnetwork layers of the pre-trained model are combined to\\nproduce a richer representation of speech features. Cai et\\nal. [39] proposed a multi-task learning (MTL) framework\\nthat uses the wav2vec2.0 model for feature extraction and\\nsimultaneously training for speech emotion classification and\\ntext recognition. Among computer vision tasks, ViT [40] first\\napplied transformer directly to image patch sequences which is\\ngroundbreaking in applying transformer structure to computer\\nvision. ViT has a superior structure and reduced computa-\\ntional resource consumption compared to convolutional neural\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n4\\nFig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='EX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n4\\nFig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in\\nspeech; ii) T-Sa attention module enhances speech information in three dimensions: time-space-channel; iii) LCT Block combines local and global information\\nin speech.\\nnetworks. There are many similar approaches in the field of\\nspeech. ViT was introduced to speech and improved based\\non the properties of spectrograms in [41], which proposed\\na separable transformer (SepTr) that uses the transformer to\\nprocess tokens at the same time and the same frequency\\ninterval, respectively. In [42], a method to improve ViT was\\napplied to infant cry recognition by combining the original\\nlog-Mel spectrogram, first-order time-frame and frequency-\\nbin differential log-Mels 3D features into ViT for infant cry\\nrecognition.\\nIII. METHOD\\nIn this section, we describe the proposed model in detail.\\nOur proposed model is shown in Fig. 1, which consists of three\\nparts, namely CNN Block, T-Sa attention mechanism module,\\nand LCT Block, these three modules will be introduced in\\ndetail next.\\nA. Overview of the model\\nTo take full advantage of convolutional neural networks and\\ntransformer to model the speech sequences and use attention\\nmechanism to enhance the features in time, space and channel,\\nbased on which our model is designed. As shown in Fig. 1, for', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='transformer to model the speech sequences and use attention\\nmechanism to enhance the features in time, space and channel,\\nbased on which our model is designed. As shown in Fig. 1, for\\na given input speech sequence, a series of preprocessing steps\\nare performed. Specifically, we uniformly process different\\nlengths of speech sequences into 1.8s, the longer sequences\\nwill be cropped into subsegments, and for shorter sequences,\\nwe process them using loop filling, after which MFCC features\\nare extracted of speech as the input to the model. The local\\nfeatures of the speech first are obtained by a CNN block, where\\nthe irregular-sized time-frequency domain convolution is used\\nto obtain the features in the time and frequency domains of the\\nspeech. The features are then enhanceed using a T-Sa attention\\nmechanism block, which contains a bilstm attention module\\nto model the features in the temporal order, followed by a\\nspatial-channel attention mechanism to focus on the spatial and\\nchannel information. Finally, the global and local information\\nof speech is learned interactively by an LCT Block, which\\nenables the model to learn information at different scales. The\\nthree parts of the model in this paper are described in detail\\nin the following sections.\\nB. CNN Block\\nFor a large model such as transformer, if MFCC features\\nare directly input into transformer, it will bring a large number\\nof parameters. And since the dataset of speech emotion\\nrecognition is generally small, using transformer directly for\\nfeature learning will make the model difficult to converge.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='are directly input into transformer, it will bring a large number\\nof parameters. And since the dataset of speech emotion\\nrecognition is generally small, using transformer directly for\\nfeature learning will make the model difficult to converge.\\nTherefore, we introduce a CNN Block to pre-learn the local\\nfeatures in speech. As shown in Figure 1, the CNN Block\\nconsists of a series of convolutional and pooling layers.\\nFor the input feature MFCC, the two dimensions of MFCC\\ncorrespond to two dimensions in the temporal and frequency\\ndomains, respectively, for which we first use a pair of irregular\\nconvolutions to obtain the perceptual field in a specific range.\\nFor a convolution of size 3 × 1, we set the perceptual field\\nin the time domain to 1, thus minimizing the effect in the\\ntime domain to learn information in the frequency domain,\\nand for a convolution of size 1×3 which is a similar process,\\nwhich in turn allows capturing a multi-scale representation in\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n5\\nFig. 2. An illustration of our proposed T-Sa attention module consisting of two modules: Timing attention which enhances the current features temporally\\nthrough BiLSTM, and Space-channel attention which enhances the features from the spatial and channel dimensions.\\nthe temporal-frequency domain. The results are then fed to\\nsuccessive convolutional layers and maxpooling layers, which\\nare used to further capture the local representation in speech,\\nthe batch normalization (BN), and relu activation function are', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='successive convolutional layers and maxpooling layers, which\\nare used to further capture the local representation in speech,\\nthe batch normalization (BN), and relu activation function are\\napplied after each convolutional layer.\\nC. T-Sa attention module\\nIn this paper, inspired by [43], we adopt a novel attention\\nmodule to combine temporal attention with spatial-channel\\nattention, focusing on the temporal dynamics of speech fea-\\ntures and spatial-channel information in the feature map, as\\nshown in Fig. 2, which is divided into two parts before and\\nafter. In the former part, the attention model enhances the\\ntemporal information in the features by Bilstm to model the\\ncurrent features temporally, based on the fact that speech\\ninformation is temporal information and the order of features\\nin time is of importance. The latter part follows the spatial\\nand channel attention, which is a common concern in existing\\nattention, and efficiently combines the attention of the spatial\\nand channel dimensions of speech feature maps through the\\nShuffle unit. T-Sa attention module enhances the features in\\nthe model through three dimensions and with a small number\\nof parameters and achieves better results in SER.\\n1) Timing attention: After Pre-processing and CNN Block\\nfor feature extraction of speech, giving the feature map size\\nof X ∈RC×H×W for the input T-Sa attention module, where\\nC, H and W denote the number of channels, spatial height\\nand width, respectively. To model the temporal attention in\\nspeech features, a recurrent neural network is used to model', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='C, H and W denote the number of channels, spatial height\\nand width, respectively. To model the temporal attention in\\nspeech features, a recurrent neural network is used to model\\nthe speech information which is bilstm. The input of bilstm is\\na two-dimensional sequence while our speech feature map is\\nthree-dimensional, so what we need is to process the feature\\nmap X. If we directly reshape the feature map, the input bilstm\\nwill bring a great number of parameters number. Therefore,\\naverage pooling is used to reduce the dimensionality of the\\nchannels, and the specific calculation is:\\nXCAvgP ool = 1\\nC\\nC\\nX\\ni=1\\nX(i)\\n(1)\\nAfter the feature passes through average pooling, the size\\nof the feature map is XCAvgP ool ∈RH×W . Feature map\\nis adjusted to XCAvgP ool ∈RW ×H by reshaping operation\\nand then feeding it to the bilstm layer. By encoding long\\ndistances from front to back and from back to front, bilstm can\\nbetter capture bidirectional feature dependencies. XCAvgP ool\\nis encoded by bilstm as follows:\\n−\\n→\\nh bilstm = −\\n−\\n−\\n−\\n−\\n−\\n−\\n→\\nBILSTM(XCAvgP ool)\\n(2)\\n←\\n−\\nh bilstm = ←\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\nBILSTM(XCAvgP ool)', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='(2)\\n←\\n−\\nh bilstm = ←\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\nBILSTM(XCAvgP ool)\\n(3)\\nTwo LSTMs in bilstm process the sequence forward and\\nbackward, respectively, and then concate the outputs of the\\ntwo LSTMs together:\\nHbilstm = Concatenate\\n\\x10−\\n→\\nh bilstm; ←\\n−\\nh bilstm\\x11\\n(4)\\nHbilstm is then applied sigmoid activation and multiplied with\\nthe input feature map using the residual scheme to output\\ntemporal attention:\\nXtime = σ\\n\\x00Hbilstm\\x01\\n· X\\n(5)\\n2) Space-channel attention: Spatial attention and channel\\nattention are widely used in computer vision. Most methods\\ntransform and aggregate features in these two directions, such\\nas SE [31], CBAM [32], BAM [44], GCNet [45], but these\\nmethods do not make full use of the correlation between space\\nand channel which are not efficient. Therefore, we adopted\\nSA-Net [43] as our spatial-channel attention module.\\nGiven the output Xtime of the temporal attention module,\\nthe input is first divided into G groups, and the size of\\neach sub-feature map is Xtime′ ∈RC/G×H×W . Then, each\\ngroup is split into two sub-branches in the channel dimension.,\\nXspatial and Xchannel, one of which obtains spatial attention', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='group is split into two sub-branches in the channel dimension.,\\nXspatial and Xchannel, one of which obtains spatial attention\\nand the other obtains channel attention.\\nFor the channel attention branch, firstly, the global average\\npooling (GAP) operation is performed on the input of the\\nbranch to embed the global information. Then, a simple\\ngating mechanism and sigmoid activation are used to perform\\nadaptive learning of spatial features. A residual scheme is used\\nto multiply the input channel branch feature map. The specific\\noperation of the spatial attention module is as follows:\\nXchannel′ = σ\\n\\x00W1 · GAP(Xchannel) + b1\\n\\x01\\n· Xchannel (6)\\nW1 and b1 are learnable parameters.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n6\\nFig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the\\nfeatures through a lightweight convolution module; ii) CA-LMAM, which captures the long-range dependencies in the features and enhance the time-frequency\\ndomain features of speech through a CA module; iii) SE-IBFFN, which introduces a nonlinear part through a feedforward network with inverse residuals to\\nenhance the model the expressiveness of the model.\\nFor the spatial attention branch, GN [46] operation is\\nfirst performed on the input of the branch to embed spatial\\nstatistical information, and then a simple gating mechanism', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='enhance the model the expressiveness of the model.\\nFor the spatial attention branch, GN [46] operation is\\nfirst performed on the input of the branch to embed spatial\\nstatistical information, and then a simple gating mechanism\\nand sigmoid activation are used to perform adaptive learning\\nof features on the channel, and the residual scheme is used to\\nmultiply the input channel branch feature map. The specific\\noperation of the channel attention module is as follows:\\nXspatial′ = σ\\n\\x00W2 · GN(Xspatial) + b2\\n\\x01\\n· Xspatial\\n(7)\\nBoth W2 and b2 are learnable parameters, and then the outputs\\nof the two branches are merged by concatenating them along\\nthe channel dimension:\\nXattention = Concatenate\\n\\x10\\nXchannel′; Xspatial′\\x11\\n(8)\\nThe attention weights on space and channel are learned sep-\\narately through two branches, and the corresponding residual\\nscheme is multiplied with the respective input feature map to\\nenhance the representations of space and channel. Finally, the\\nchannel shuffle [47] is employed to facilitate communication\\nof information between different groups along the channel\\ndimension. The information of the features interacts in the\\nchannel dimension, and the size of the output feature map is\\nthe same as the initial input.\\nD. LCT Block\\nInspired by [48], we proposed an LCT Block shown in Fig.\\n3, which consists of three modules: lightweight local con-\\nvolution (LLC), coordinate attention-lightweight multi-head', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Inspired by [48], we proposed an LCT Block shown in Fig.\\n3, which consists of three modules: lightweight local con-\\nvolution (LLC), coordinate attention-lightweight multi-head\\nattention mechanism (CA-LMAM) and SE Inverted Bottleneck\\nfeed forward network (SE-IBFFN). Through the combination\\nof convolution and transformer, LCT Block obtains the in-\\nformation of features from the local and the whole, which is\\nmore efficient and has fewer parameters than the traditional\\ntransformer. LLC is a lightweight convolution module, which\\nefficiently obtains local information of features. The CA-\\nLMAM module can capture the long-distance dependencies\\nof features and enhance the information in the time-frequency\\ndomain through the Coordinate Attention (CA) [49] module.\\nSE-IBFFN introduces a nonlinear part through a feedforward\\nnetwork with inverted residuals, which enhances the perfor-\\nmance of the model and can further capture the local infor-\\nmation of features. These three modules will be introduced in\\ndetail below\\n1) LLC: In order to make up for the lack of local infor-\\nmation in trasnformer, a convolution module is used to obtain\\nlocal information in speech which called LLC. Some work\\nsuch as CMT [48] also adopted a similar architecture, but the\\nconvolution module in CMT is relatively simple, only using\\na Depthwise convolution with residuals, which can not obtain\\neffective local information. Inspired by [50], we used a large', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='convolution module in CMT is relatively simple, only using\\na Depthwise convolution with residuals, which can not obtain\\neffective local information. Inspired by [50], we used a large\\nconvolution kernel Depthwise convolution with residuals and\\na Pointwise convolution in the local feature extraction module,\\ngiven an input feature X ∈RC×H×W as follows:\\nLLC (X) = PWConv (DWConv (X) + X)\\n(9)\\nThe activation layer and batch normalization are omitted, and\\nthe same omission will be in subsequent formulas. PWConv\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n7\\nrepresents Pointwise convolution and DWConv represents\\nDepthwise. In PWConv we used a large 7 × 7 convolution\\nkernel is used to get a larger receptive field than a 3 × 3\\nconvolution kernel. In addition, PWConv has smaller param-\\neters than DWConv and ordinary convolution. Additionally,\\na residual structure is incorporated to address the issue of\\ngradient dispersion.\\n2) CA-LMAM: In transformer, multi-head attention is usu-\\nally used to make the model pay more attention to the more\\nnoteworthy part of itself, which can obtain long-distance de-\\npendence in speech features. Given the output X ∈RC×H×W\\nin the LLC module, the input of multi-head attention is query\\nQ, key K, and value V , respectively. If the original features', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='pendence in speech features. Given the output X ∈RC×H×W\\nin the LLC module, the input of multi-head attention is query\\nQ, key K, and value V , respectively. If the original features\\nare directly input into multi-head attention, it will often bring\\na large amount of calculation. The amount of calculation is\\nrelated to the size of the input features. Therefore, 2 × 2\\nDWConv is used to downsample the parts of K and V , as\\nshown below:\\nK = DWConv (X)\\n(10)\\nV = DWConv (X)\\n(11)\\nWhere K ∈RC× H\\n2 × W\\n2 , V ∈RC× H\\n2 × W\\n2 , then the H and W\\ndimensions are merged and input a linear layer respectively.\\nFinally get K′ ∈RN ′×C, V ′ ∈RN ′×C, where N ′ = H\\n2 × W\\n2 ,\\nC is the dimension of the linear layer output.\\nFor Q, a CA module is used to enhance the time-frequency\\ndomain representation of speech features. This attention mech-\\nanism can obtain long-distance feature dependence along one\\ndirection and spatial dependence information in another direc-\\ntion. However, in speech, speech features have more special\\nsignificance in the spatial dimension. The abscissa of speech\\nfeatures is the time axis, which represents the time domain\\ninformation of speech while the ordinate of the speech feature\\nrepresents the frequency information of the speech, so CA', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='features is the time axis, which represents the time domain\\ninformation of speech while the ordinate of the speech feature\\nrepresents the frequency information of the speech, so CA\\ncan better aggregate the dependent information in the time\\ndomain and frequency domain for speech features, which is\\nmore suitable for SER tasks.\\nThe specific operation of CA is shown in Fig. 3. Given\\nthe input X ∈RC×H×W , pooling is performed in the time\\ndomain and frequency domain respectively:\\nXT AvgP ool = 1\\nW\\nW\\nX\\ni=1\\nX(i)\\n(12)\\nXF AvgP ool = 1\\nH\\nH\\nX\\nj=1\\nX(j)\\n(13)\\nXT AvgP ool ∈RC×H×1 and XF AvgP ool ∈RC×1×W are\\nthe feature maps after aggregation in two directions of time-\\nfrequency domain. Then, they concatenate and perform con-\\nvolution operations to encode the information:\\nXtf = Conv\\n\\x00concatenate\\n\\x00XT AvgP ool; XF AvgP ool\\x01\\x01\\n(14)\\nWhere Xtf ∈RC/r×1×(W +H), r is the reduction rate of the\\nchannel, and then separated into two separate features Xt, Xf\\nalong the spatial direction, where Xt ∈RC/r×H×1, Xf ∈\\nRC/r×1×W . The extraction of features is then performed by\\ntwo separate convolutions, followed by activation using the σ', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='RC/r×1×W . The extraction of features is then performed by\\ntwo separate convolutions, followed by activation using the σ\\nactivation function, and then multiplied by the initial output\\nto learn the more critical information in the feature:\\nst = σ\\n\\x00Conv\\n\\x00Xt\\x01\\x01\\n(15)\\nsf = σ\\n\\x00Conv\\n\\x00Xf\\x01\\x01\\n(16)\\nQ = X · st · sf\\n(17)\\nWe adjust the dimension of Q, eventually Q′ ∈RN×C, where\\nN = H × W.\\nEventually learning information about the model itself\\nthrough multi-headed self-attention:\\nLMAM(Q, K, V ) = Softmax\\n\\x12Q′K′T\\n√\\nC\\n+ B\\n\\x13\\nV ′\\n(18)\\nWhere B is a learnable parameter, representing the relative\\nposition coding of multi-head self-attention, which is used to\\ncharacterize the relative position relationship between tokens.\\nIt is more flexible than the traditional absolute position coding,\\nmaking transformer better model the relative position informa-\\ntion of speech features.\\n3) SE-IBFFN: In the original transformer, FFN is generally\\ncomposed of two linear layers. In this paper, inspired by [51],\\na series of Pointwise convolution and Depthwise convolution\\nmake up our SE-IBFFN. Compared with the Transformer\\nmodel traditionally composed of linear layers, this module can\\ncapture local information while learning channel information,\\nand has a smaller number of convolution parameters than\\nordinary ones.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='model traditionally composed of linear layers, this module can\\ncapture local information while learning channel information,\\nand has a smaller number of convolution parameters than\\nordinary ones.\\nThe structure of SE-IBFFN is shown in Fig. 3. Given the\\ninput X ∈RC×H×W , the specific calculation process is as\\nfollows:\\nXconv = PWConv (DWConv (PWConv (X)))\\n(19)\\nSE-IBFFN (X) = SE (Xconv) + X\\n(20)\\nFirstly, the Pointwise convolution operation is performed on\\nthe input, and the number of channels is expanded to 4 times of\\nthe original to increase the feature size of the channels. Then\\na 7 × 7 large convolution kernel DWConv is used to obtain\\nthe local information in the feature, and the large convolution\\nkernel can provide a larger receptive field without increasing\\ntoo many parameters. The feature map is then restored to its\\noriginal size using PWConv.\\nThen, the SE module is to obtain the attention of the channel\\ndimension of the feature map. Given the input X ∈RC×H×W\\nof SE, the specific calculation process is as follows:\\nSE (X) = σ (GAP (FC (FC (X)))) · X\\n(21)\\nCompared with [51], we put the SE module after PWConv,\\nwhich makes the parameters of the model smaller. A residual\\nstructure is used to solve the problem of gradient dispersion.\\nJOURNAL OF L\\nAT', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='which makes the parameters of the model smaller. A residual\\nstructure is used to solve the problem of gradient dispersion.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n8\\nIV. EXPERIMENTS SETUP\\nIn this section, we will introduce the dataset and experimen-\\ntal details used in our study, as well as the evaluation metrics\\nused to assess the performance of our algorithm.\\nA. Corpus Description\\nTo verify the performance of our proposed algorithm, per-\\nformance tests on two benchmark databases are conducted, on\\nwhich we will evaluate our algorithm.\\nActually, Interactive Emotional Dyadic Motion Capture\\n(IEMOCAP) [52] is an action, multimodal, and multimodal\\ndatabase that contains data from 10 actors and actresses\\nduring an emotional binary interaction, with two speakers\\n(one male and one female) speaking in each session. The\\nIEMOCAP database has been annotated by several annotators\\nwith categorical labels and dimensional labels. The database\\ncombines discrete and dimensional sentiment models. In our\\nwork, the method used improvisational and scripted data,\\nchoose anger, happiness, neutral, sadness and excitement as\\nbasic emotions, and merge happy and excited into happy. We\\npartitioned the IEMOCAP dataset into training and testing sets\\nby randomly selecting 80% and 20% of the data, respectively.\\nTo valid our method robustness, we tested our method\\nin other datasets. The Berlin Emotional Database (Emo-DB)\\n[53] is a German emotional speech database recorded by', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='To valid our method robustness, we tested our method\\nin other datasets. The Berlin Emotional Database (Emo-DB)\\n[53] is a German emotional speech database recorded by\\nthe Technical University of Berlin. The database includes\\nrecordings of ten actors, comprising of five male and five\\nfemale, who simulate seven emotions, including neutral, anger,\\nfear, joy, sadness, disgust, and boredom, on ten sentences\\n(five short and five long), resulting in a total of 535 speech\\nrecordings (233 male and 302 female). It has high emotional\\nfreedom, adopts 16 kHz sampling, and 16-bit quantization, and\\nsaves files in WAV format. It is a discrete emotional language\\ndatabase, and the excitation method is performance type.\\nB. Implementation Details\\nIn the feature generation phase, the method uses the mel-\\nfrequency cepstrum coefficients (MFCCs) extracted by 26 Mel\\nfilters as the feature input. Meanwhile it divided each input\\nspeech into 1.8 seconds of speech segments, and the overlap\\nbetween segments is 1.6 seconds, which can generate a large\\nnumber of speech samples to solve the problem of scarcity of\\ndata set samples in SER. To obtain the prediction result for a\\nsentence in the test set, we take the average of the prediction\\nresults of all speech segments within that sentence. Our model\\ntrained a total of 150 epochs, used the cross entropy criterion\\nas the objective function, and used the Adam optimizer. The\\nweight decay rate is 10−6, the learning rate and the mini-', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='trained a total of 150 epochs, used the cross entropy criterion\\nas the objective function, and used the Adam optimizer. The\\nweight decay rate is 10−6, the learning rate and the mini-\\nbatch size are set to 0.001 and 128, respectively, and the\\nmultiplication factor 0.95 is exponentially decayed until the\\nvalue reaches 10−6. Our experiment is carried out on Ubuntu\\n18.04 with a GeForce RTX 2080ti GPU, and we utilized\\nPytorch 1.7 as the training framework.\\nIn addition, we use the method of mixup [54] to train, so\\nas to improve the generalization ability of the system. This\\nmethod constructs new training samples and labels by linear\\ninterpolation, and effectively smoothes the discrete data space\\ninto continuous space. In our proposed model, we set α to 0.2\\nfor best performance.\\nC. Evaluation Metrics\\nIn this section, we’ll describe in detail the criteria we use\\nto evaluate the performance of our algorithms. For various\\ncategories of performance in the dataset, Precision, Recall, and\\nF1-score are the general metrics to measure their performance.\\nFirst of all, four concepts will be introduced: True Positive\\n(TP), False Positive (FP), True Negative (TN), and False\\nNegative (FN), where TP means actual positive and predicted\\npositive, FP means actual positive and predicted negative, TN\\nmeans actual negative and predicted positive, and FN means\\nactual negative and predicted negative. The Precision, Recall,\\nand F1-score can be expressed as:\\nPrecision =\\nTP', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='means actual negative and predicted positive, and FN means\\nactual negative and predicted negative. The Precision, Recall,\\nand F1-score can be expressed as:\\nPrecision =\\nTP\\nTP + FP\\n(22)\\nRecall =\\nTP\\nTP + FN\\n(23)\\nF1 −score = precisio × recall × 2\\nprecision + recall\\n(24)\\nTo evaluate the overall performance of our model, weighted\\naverage accuracy (WA) and unweighted average accuracy\\n(UA) will be used as evaluation metrics, where WA is the\\nweighted average accuracy of different sentiment categories,\\nand its weight is related to the number of sentiment categories,\\nand UA is the average accuracy of different sentiment cate-\\ngories. The validity of the model is better evaluated in the\\ncontext of unbalanced SER dataset samples. The calculation\\nmethods for these two metrics are as follows:\\nAcci =\\nTPi\\nTPi + FPi\\n(25)\\nWA =\\nPC\\ni=1 Ni × Acci\\nPC\\ni=1 Ni\\n(26)\\nUA = 1\\nC\\nC\\nX\\ni=1\\nAcci\\n(27)\\nAmong them, C represents the number of emotional cate-\\ngories, and Ni represents the number of samples of class i.\\nV. RESULTS\\nIn this section, we conduct extensive experiments to evaluate\\nthe performance of our method on two datasets. The section\\nmainly compares our proposed model with state-of-the-art\\nbaselines, and then verify the effectiveness of our proposed\\nmodules through ablation experiments.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='the performance of our method on two datasets. The section\\nmainly compares our proposed model with state-of-the-art\\nbaselines, and then verify the effectiveness of our proposed\\nmodules through ablation experiments.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n9\\nA. Comparison with State-of-the-Art\\nTo compare with our proposed model, we evaluate the\\nperformance of the algorithm from the WA and UA perspective\\nwith a series of existing methods in IEMOCAP and Emo-DB\\ndatasets, as shown in Table I and Table II. The proposed model\\nis compared with several commonly used speech emotion\\nrecognition models, including algorithms based on the combi-\\nnation of CNN and transformer [19], CNN-based algorithms\\n[14] [55], LSTM-based algorithms [56] [57], attention-based\\nmethods [21] [58], and some other algorithms.\\nTABLE I\\nCOMPARISON ON IEMOCAP\\nComparative Methods\\nWA\\nUA\\nLatif et al. [59]\\n-\\n68.8\\nHu et al. [19]\\n69.73\\n70.11\\nGuo et al. [21]\\n61.32\\n60.43\\nGao et al. [60]\\n70.34\\n70.82\\nWang et al. [57]\\n69.40\\n69.50\\nLatif et al. [56]\\n-\\n64.10\\nLiu et al. [14]\\n70.34\\n70.78', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Wang et al. [57]\\n69.40\\n69.50\\nLatif et al. [56]\\n-\\n64.10\\nLiu et al. [14]\\n70.34\\n70.78\\nDai et al. [61]\\n65.40\\n66.90\\nProposed\\n71.64\\n72.72\\nTABLE II\\nCOMPARISON ON EMO-DB\\nComparative Methods\\nWA\\nUA\\nTuncer et al. [62]\\n90.09\\n89.47\\nLi et al. [58]\\n83.30\\n82.10\\nZhong et al. [63]\\n85.76\\n86.12\\nKerkeni et al. [64]\\n-\\n86.22\\nSuganya et al. [55]\\n-\\n85.62\\nProposed\\n90.65\\n89.51\\nTo verify the performance of our proposed method, we\\ncompare it with other speech emotion recognition algorithms\\nin IEMOCAP and Emo-DB datasets. The results are shown in\\nTable I and Table II.\\nFor IEMOCAP dataset, as shown in Table I, our proposed\\nmethod outperforms other methods. In addition, compared\\nwith the simple splicing of transformer and CNN [19], our\\nmethod achieves the best results through the ingenious design\\nof local and global feature extraction. Compared to tradi-\\ntional spatial and channel attention [21], temporal attention\\ninformation makes it more competitive. In addition, for the\\nCNN or LSTM-based method [14] [57] [56] [61], our method', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='tional spatial and channel attention [21], temporal attention\\ninformation makes it more competitive. In addition, for the\\nCNN or LSTM-based method [14] [57] [56] [61], our method\\nintroduces global information through a lightweight trans-\\nformer module to bring more comprehensive features to the\\nsystem, and also shows that our transformer module has a\\nstronger ability to obtain global information than some capsule\\nnetworks. Finally, our method is as competitive as the method\\nusing semi-supervised methods [59] and pre-trained models\\n[60].\\nFor Emo-DB dataset, as shown in Table II, our proposed\\nmethod outperforms several methods. Similar to the perfor-\\nmance in IEMOCAP dataset, our attention mechanism and\\nlocal-global model have significant advantages over traditional\\nattention and CNN-based models [58] [63] [55]. In addition,\\nEmo-DB dataset is a smaller dataset. Compared with non-deep\\nlearning feature extraction and feature learning methods [62]\\n[64], our overall end-to-end network based on deep learning\\nalso has a relatively better performance on this small dataset,\\nindicating that our method still has excellent robustness on\\nsmall datasets.\\nIn summary, combining the performance of these two\\ndatasets proves the superiority of our proposed method.\\nB. Results and Analysis\\nIn this section, Table III and Table IV list the Precision,\\nRecall, F1-score, and overall WA and UA for each sentiment\\ncategory in IEMOCAP dataset and Emo-DB dataset, respec-', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='In this section, Table III and Table IV list the Precision,\\nRecall, F1-score, and overall WA and UA for each sentiment\\ncategory in IEMOCAP dataset and Emo-DB dataset, respec-\\ntively. In addition, the confusion matrices are visualized of the\\ntwo datasets in Fig. 4 and Fig. 5, where the diagonal indicates\\nthat the sentiment is correctly classified, and other locations\\nindicate that the sentiment is misclassified as other sentiments.\\nThe darker the color in the grid while higher the accuracy.\\nTABLE III\\nCONFUSION MATRIX OF THE PROPOSED MODEL ON IEMOCAP\\nEmotion\\nPrecision\\nRecall\\nF1-score\\nNeural\\n72.88\\n62.32\\n67.19\\nSad\\n70.51\\n75.00\\n72.68\\nAngry\\n74.43\\n79.13\\n76.71\\nHappy\\n69.68\\n74.43\\n71.98\\nWA\\n71.64\\nUA\\n72.72\\nTABLE IV\\nCONFUSION MATRIX OF THE PROPOSED MODEL ON EMO-DB\\nEmotion\\nPrecision\\nRecall\\nF1-score\\nNeural\\n86.67\\n100.00\\n92.86\\nSad\\n94.44\\n100.00\\n97.14\\nAngry\\n88.00\\n95.65\\n91.67\\nHappy\\n83.33\\n83.33\\n83.33\\nBoredom\\n100.00\\n93.75\\n96.77\\nDisgust\\n100.00\\n76.92', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='95.65\\n91.67\\nHappy\\n83.33\\n83.33\\n83.33\\nBoredom\\n100.00\\n93.75\\n96.77\\nDisgust\\n100.00\\n76.92\\n86.96\\nFear\\n83.33\\n76.92\\n80.00\\nWA\\n90.65\\nUA\\n89.51\\nFor IEMOCAP dataset, as shown in Table IV and Fig. 4,\\nour proposed model achieves good results on this dataset and\\nhas good accuracy for all four emotions, especially sadness,\\nanger and happiness. Among these four emotions, anger has\\nthe highest recognition accuracy, while neutral has the lowest\\nrecognition accuracy. Sadness, anger and happiness will be\\nmisjudged as neutral in some cases. The result is similar\\nto that in [65], which also verifies that neutral emotion is\\na defect of expression. This emotion is easily expressed as\\nother emotions, which makes the model misjudged. Therefore,\\nneutral emotions are easily confused with other emotions.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n10\\n(a)\\n(b)\\nFig. 4. Visualization the confusion matrices of the proposed method: (a) Confusion matrix on IEMOCAP; (b) Confusion matrix on Emo-DB.\\nFor Emo-DB dataset, as shown in Table IV and Fig.\\n5, our proposed model also achieved good results on this\\ndataset, and has good accuracy for seven emotions. The three', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='For Emo-DB dataset, as shown in Table IV and Fig.\\n5, our proposed model also achieved good results on this\\ndataset, and has good accuracy for seven emotions. The three\\nemotions achieved 100% accuracy, and Angry also achieved\\n96% accuracy. In addition, the wrong judgment in happiness is\\nanger. We believe that this is because these two emotions have\\nsimilar arousal, while the wrong judgment in disgust is fear.\\nFinally, the accuracy of fear is lower than that of other emotion\\ncategories, but it also achieves relatively good results. In some\\ncases, fear is easily misjudged as sad, angry and happy, this\\nis because these four emotions have a high degree of arousal.\\nTABLE V\\nPERFORMANCE WITH DIFFERENT EXPERIMENTAL SETTINGS\\nModels\\nWA\\nUA\\nW/o T-Sa\\n69.92\\n71.18\\nW/o lstm attention\\n67.84\\n69.62\\nW/o LCT\\n67.12\\n68.93\\nW/ Conv-LCT\\n68.29\\n69.98\\nW/o CA\\n69.29\\n70.12\\nW/o SE\\n69.92\\n71.31\\nProposed\\n71.64\\n72.72\\nC. Ablation Study\\nTo explore the role of each part of our proposed model,\\nTable V shows the results of a series of ablation experiments.\\nThe following are the modules for comparison.\\n• W/o T-Sa: This module removes the T-Sa module from', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Table V shows the results of a series of ablation experiments.\\nThe following are the modules for comparison.\\n• W/o T-Sa: This module removes the T-Sa module from\\nour model, using only the CNN Block and LCT Block\\nsections.\\n• W/o lstm attention: This module removes the temporal\\nattention lstm attention part in our T-Sa module, and the\\nother parts are retained.\\n• W/o LCT: This module removes the LCT module in our\\nmodel and only uses the CNN Block and T-Sa parts.\\n• W/ Conv-LCT: This module replaces the LLC module in\\nour LCT module with a 3 × 3 convolution, and the rest\\nis preserved.\\n• W/o CA: This module removes the CA portion of our\\nLCT module and preserves the rest.\\n• W/o SE: This module removes the SE Attention part of\\nour LCT module, and the other parts are retained.\\nIt can be seen from the table that when using the T-Sa\\nmodule, our method has an absolute improvement of 1.2%\\nand 1.54% in WA and UA, indicating that our attention\\nmechanism module has a very significant effect and can\\naggregate the noteworthy parts of the features. In addition,\\nwhen the temporal attention is removed, the model effect is\\nalso greatly reduced, indicating that temporal attention plays\\na non-important role in our model to enhance the temporal\\ninformation in speech. We also directly removed the LCT\\nmodule, which caused the performance of the model to be', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='also greatly reduced, indicating that temporal attention plays\\na non-important role in our model to enhance the temporal\\ninformation in speech. We also directly removed the LCT\\nmodule, which caused the performance of the model to be\\nreduced by 4.52% and 3.79% on WA and UA, it clearly shows\\nthe importance of introducing global information into our LCT\\nmodule.\\nFor our LCT part, our experiments also verified the role\\nof different modules within the LCT. Firstly, the LLC part is\\nreplaced with a 3×3 ordinary convolution, which reduces the\\nperformance of the model by 3.35% and 2.74% on WA and\\nUA, and the number of parameters has also been improved.\\nThis shows that the wider receptive field brought by our large\\nconvolution kernel LLC module is very important, and it does\\nnot bring a larger number of parameters. In order to verify\\nthe role of our CA module, we removed the CA module,\\nwhich caused the performance of the model to decrease\\nby 2.35% and 2.6% on WA and UA, indicating that the\\ntime-frequency domain representation of the speech features\\nenhanced by the CA module is of vital importance. Finally,\\nthe SE Attention part in our SE-IBFFN is removed, which\\nreduces the performance of the model by 1.72% and 1.41%\\non WA and UA. It also proves that using the SE module to\\nobtain the attention of the feature map channel dimension can\\nimprove the performance of the model to recognize emotions.\\nD. Model Efficiency Analysis', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='on WA and UA. It also proves that using the SE module to\\nobtain the attention of the feature map channel dimension can\\nimprove the performance of the model to recognize emotions.\\nD. Model Efficiency Analysis\\nIn order to explore the size and efficiency of the proposed\\nmodel, Table VI presents a comparison of the parameter\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n11\\nTABLE VI\\nCOMPARISON OF MODEL PARAMETERS AND ACCURACY\\nModels\\nParams\\nWA\\nUA\\nW/ Conv-LCT\\n1,404,922\\n68.29\\n69.98\\nW/ConvFFN-LCT-L\\n10,390,522\\n70.46\\n72.14\\nW/ConvFFN-LCT-S\\n2,526,202\\n68.83\\n69.71\\nW/Transformer\\n1,357,810\\n69.38\\n70.43\\nW/MobileVitv1 block-depth2 [66]\\n2,036,458\\n70.01\\n70.98\\nW/MobileVitv1 block-depth3 [66]\\n2,726,506\\n66.58\\n67.30\\nW/MobileVitv2 block-depth2 [67]\\n937,898\\n68.29\\n69.68\\nW/MobileVitv2 block-depth3 [67]\\n1,086,634\\n67.12\\n69.79', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='937,898\\n68.29\\n69.68\\nW/MobileVitv2 block-depth3 [67]\\n1,086,634\\n67.12\\n69.79\\nW/MobileVitv3 block-depth2 [68]\\n661,162\\n69.20\\n70.14\\nW/MobileVitv3 block-depth3 [68]\\n884,010\\n68.47\\n69.73\\nProposed\\n1,031,674\\n71.64\\n72.72\\ncounts and accuracy between our model and other models.\\nThe following are the modules we designed for comparison.\\n• W/ Conv-LCT: This module replaces the LLC module in\\nour LCT module with a 3 × 3 convolution and the other\\nparts are retained.\\n• w/ ConvFFN-LCT-L: This module replaces the first PW\\nconvolution and DW convolution in the SE-IBFFN mod-\\nule of our LCT module with a 7 × 7 convolution and the\\nother parts are retained.\\n• W/ ConvFFN-LCT-S: This module replaces the first PW\\nconvolution and DW convolution in the SE-IBFFN mod-\\nule of our LCT module with a 3 × 3 convolution and the\\nother parts are retained.\\n• W/Transformer: This module replaces CA-LMAM and\\nSE-IBFFN with traditional transformer, which is the same\\nas the setting of LCT.\\n• W/ MobileVit block: This module replaces our LCT', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='SE-IBFFN with traditional transformer, which is the same\\nas the setting of LCT.\\n• W/ MobileVit block: This module replaces our LCT\\nmodule with the MobileVit block of various series of\\nMobile Vit, with an input channel size of 128 and a depth\\nof 2 or 3. The intermediate layer size of the MLP is set\\nto 4 times of the input channel size for all versions. For\\nv1 and v2, the number of intermediate layers is 192, and\\nfor v3, the number of intermediate layers is 128.\\nFrom the Table VI, it can be seen that when the LLC\\nmodule in LCT is replaced with a regular convolution, the\\nnumber of parameters increases and the accuracy is lower\\nthan that of the LLC module. This indicates that LLC has\\nbetter accuracy with fewer parameters. Additionally, when the\\nfirst PW and DW convolutions in SE-IBFFN are replaced\\nwith regular convolutions, the number of parameters increases\\nsignificantly, and the accuracy is still lower than the original\\nmodel. We also reduced the size of the convolution kernel to\\n3×3, which improved the number of parameters, but it is still\\nlarger than the original SE-IBFFN, and the accuracy decreased.\\nThis suggests that the SE-IBFFN module did not bring a huge\\nnumber of parameters while using a larger convolution kernel\\nto achieve a larger receptive field, and the accuracy is still\\nexcellent.\\nAdditionally, we attempted to replace CA-LMAM and SE-\\nIBFFN in LCT with traditional transformers, and experimental', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='to achieve a larger receptive field, and the accuracy is still\\nexcellent.\\nAdditionally, we attempted to replace CA-LMAM and SE-\\nIBFFN in LCT with traditional transformers, and experimental\\nresults showed that the traditional transformer has a larger\\nnumber of parameters and a decrease in accuracy compared\\nto LCT, with a decrease of 2.23% and 2.29%, respectively. The\\nentire LCT module was also replaced with other transformer\\nmodels, including the MobileVit series which combines CNN\\nand Vit and is a lightweight transformer model. The MobileVit\\nblock in each series was used to replace LCT, and the results\\nshowed that the parameter size of the MobileVitv3 block is\\nsmaller than LCT, but there is still a gap in accuracy. The\\nperformance of the two-layer MobileVitv1 block was the best,\\nbut it still did not reach the accuracy of LCT. Additionally,\\nwe found that models with a depth of 2 performed better than\\nthose with a depth of 3, due to the difficulty of fitting to small\\nSER datasets as the number of parameters increases.\\nVI. CONCLUSION\\nIn this study, to better model local and global features\\nof speech signals at different levels of granularity in SER\\nand capture temporal, spatial and channel dependencies in\\nspeech signals, we propose a Speech Emotion Recognition\\nnetwork based on CNN-Transformer and multi-dimensional\\nattention mechanisms. The network consists of three modules.\\nFirst, a CNN block is used to model time-frequency domain\\ninformation in speech, capturing preliminary local information', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='network based on CNN-Transformer and multi-dimensional\\nattention mechanisms. The network consists of three modules.\\nFirst, a CNN block is used to model time-frequency domain\\ninformation in speech, capturing preliminary local information\\nin speech. Second, we propose a T-Sa network to model\\nthe emotional expression context of features over time and\\nefficiently fuse the spatial and channel dimensions of speech\\nfeature maps through Shuffle units. Finally, to efficiently fuse\\nlocal information and long-distance dependencies in speech,\\nwe propose an LCT module that uses lightweight convolu-\\ntional modules and introduces Coordinate Attention into multi-\\nhead self-attention. This allows for the fusion of features at\\ndifferent levels of granularity while enhancing information in\\nthe time-frequency domain of features without introducing a\\nhigh number of parameters.\\nIn future work, in addition to MFCC features, we will\\ntry more hierarchical speech features and combine current\\nCNN and transformer structures to improve the performance\\nof Speech Emotion Recognition from multiple feature dimen-\\nsions.\\nACKNOWLEDGMENTS\\nThis work was supported by the National Natural Science\\nFoundation of China (Grant No. 62001173), the Project of\\nSpecial Funds for the Cultivation of Guangdong College\\nStudents’ Scientific and Technological Innovation (”Climb-\\ning Program” Special Funds) (Grant No. pdjh2022a0131,\\npdjh2023b0141).\\nREFERENCES\\n[1] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech emotion', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='pdjh2023b0141).\\nREFERENCES\\n[1] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech emotion\\nrecognition: Features, classification schemes, and databases,” Pattern\\nrecognition, vol. 44, no. 3, pp. 572–587, 2011.\\n[2] L.-S. A. Low, N. C. Maddage, M. Lech, L. B. Sheeber, and N. B. Allen,\\n“Detection of clinical depression in adolescents’ speech during family\\ninteractions,” IEEE Transactions on Biomedical Engineering, vol. 58,\\nno. 3, pp. 574–586, 2010.\\n[3] A. Mahdhaoui, M. Chetouani, and C. Zong, “Motherese detection based\\non segmental and supra-segmental features,” in 2008 19th International\\nConference on Pattern Recognition.\\nIEEE, 2008, pp. 1–4.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n12\\n[4] S. E. Bou-Ghazale and J. H. Hansen, “A comparative study of traditional\\nand newly proposed features for recognition of speech under stress,”\\nIEEE Transactions on speech and audio processing, vol. 8, no. 4, pp.\\n429–442, 2000.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='and newly proposed features for recognition of speech under stress,”\\nIEEE Transactions on speech and audio processing, vol. 8, no. 4, pp.\\n429–442, 2000.\\n[5] C. Gobl and A. N. Chasaide, “The role of voice quality in communi-\\ncating emotion, mood and attitude,” Speech communication, vol. 40, no.\\n1-2, pp. 189–212, 2003.\\n[6] J. Hernando and C. Nadeu, “Linear prediction of the one-sided autocor-\\nrelation sequence for noisy speech recognition,” IEEE Transactions on\\nSpeech and Audio Processing, vol. 5, no. 1, pp. 80–84, 1997.\\n[7] S. S. Barpanda, B. Majhi, P. K. Sa, A. K. Sangaiah, and S. Bakshi, “Iris\\nfeature extraction through wavelet mel-frequency cepstrum coefficients,”\\nOptics & Laser Technology, vol. 110, pp. 13–23, 2019.\\n[8] Y. Q. Qin and X. Y. Zhang, “Hmm-based speaker emotional recognition\\ntechnology for speech signal,” in Advanced Materials Research, vol.\\n230.\\nTrans Tech Publ, 2011, pp. 261–265.\\n[9] J. Pribil, A. Pribilova, and J. Matousek, “Artefact determination by gmm-\\nbased continuous detection of emotional changes in synthetic speech,” in', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='[9] J. Pribil, A. Pribilova, and J. Matousek, “Artefact determination by gmm-\\nbased continuous detection of emotional changes in synthetic speech,” in\\n2019 42nd International Conference on Telecommunications and Signal\\nProcessing (TSP).\\nIEEE, 2019, pp. 45–48.\\n[10] B. Schuller, B. Vlasenko, F. Eyben, M. W¨\\nollmer, A. Stuhlsatz, A. Wen-\\ndemuth, and G. Rigoll, “Cross-corpus acoustic emotion recognition:\\nVariances and strategies,” IEEE Transactions on Affective Computing,\\nvol. 1, no. 2, pp. 119–131, 2010.\\n[11] B. Gupta and S. Dhawan, “Deep learning research: Scientometric\\nassessment of global publications output during 2004-17,” Emerging\\nScience Journal, vol. 3, no. 1, pp. 23–32, 2019.\\n[12] S. Kumar, T. Roshni, and D. Himayoun, “A comparison of emotional\\nneural network (enn) and artificial neural network (ann) approach for\\nrainfall-runoff modelling,” Civil Engineering Journal, vol. 5, no. 10, pp.\\n2120–2130, 2019.\\n[13] Q. Cao, M. Hou, B. Chen, Z. Zhang, and G. Lu, “Hierarchical network\\nbased on the fusion of static and dynamic features for speech emotion', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='[13] Q. Cao, M. Hou, B. Chen, Z. Zhang, and G. Lu, “Hierarchical network\\nbased on the fusion of static and dynamic features for speech emotion\\nrecognition,” in ICASSP 2021-2021 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP).\\nIEEE, 2021, pp.\\n6334–6338.\\n[14] J. Liu, Z. Liu, L. Wang, L. Guo, and J. Dang, “Speech emotion\\nrecognition with local-global aware deep representation learning,” in\\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP).\\nIEEE, 2020, pp. 7174–7178.\\n[15] R. A. Khalil, E. Jones, M. I. Babar, T. Jan, M. H. Zafar, and T. Alhussain,\\n“Speech emotion recognition using deep learning techniques: A review,”\\nIEEE Access, vol. 7, pp. 117 327–117 345, 2019.\\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\\nneural information processing systems, vol. 30, 2017.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\\nneural information processing systems, vol. 30, 2017.\\n[17] L. Tarantino, P. N. Garner, A. Lazaridis et al., “Self-attention for speech\\nemotion recognition,” in Interspeech, 2019, pp. 2578–2582.\\n[18] X. Wang, M. Wang, W. Qi, W. Su, X. Wang, and H. Zhou, “A novel\\nend-to-end speech emotion recognition network with stacked transformer\\nlayers,” in ICASSP 2021-2021 IEEE International Conference on Acous-\\ntics, Speech and Signal Processing (ICASSP).\\nIEEE, 2021, pp. 6289–\\n6293.\\n[19] D. Hu, X. Hu, and X. Xu, “Multiple enhancements to lstm for learning\\nemotion-salient features in speech emotion recognition,” Proc. Inter-\\nspeech 2022, pp. 4720–4724, 2022.\\n[20] S. Kwon et al., “Att-net: Enhanced emotion recognition system using\\nlightweight self-attention module,” Applied Soft Computing, vol. 102, p.\\n107101, 2021.\\n[21] L. Guo, L. Wang, C. Xu, J. Dang, E. S. Chng, and H. Li, “Representation\\nlearning with spectro-temporal-channel attention for speech emotion', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='learning with spectro-temporal-channel attention for speech emotion\\nrecognition,” in ICASSP 2021-2021 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP).\\nIEEE, 2021, pp.\\n6304–6308.\\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification\\nwith deep convolutional neural networks,” Communications of the ACM,\\nvol. 60, no. 6, pp. 84–90, 2017.\\n[23] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 2016, pp. 770–778.\\n[24] W. Zhu and X. Li, “Speech emotion recognition with global-aware fusion\\non multi-scale feature representation,” in ICASSP 2022-2022 IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing\\n(ICASSP).\\nIEEE, 2022, pp. 6437–6441.\\n[25] E. Guizzo, T. Weyde, and J. B. Leveson, “Multi-time-scale convolution\\nfor emotion recognition from speech audio signals,” in ICASSP 2020-\\n2020 IEEE International Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP).\\nIEEE, 2020, pp. 6489–6493.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='2020 IEEE International Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP).\\nIEEE, 2020, pp. 6489–6493.\\n[26] Y. Xu, H. Xu, and J. Zou, “Hgfm: A hierarchical grained and feature\\nmodel for acoustic emotion recognition,” in ICASSP 2020-2020 IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing\\n(ICASSP).\\nIEEE, 2020, pp. 6499–6503.\\n[27] C. Li, “Robotic emotion recognition using two-level features fusion in\\naudio signals of speech,” IEEE Sensors Journal, 2021.\\n[28] J. Liu and H. Wang, “A speech emotion recognition framework for better\\ndiscrimination of confusions,” in Interspeech, 2021, pp. 4483–4487.\\n[29] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emotion\\nrecognition with co-attention based multi-level acoustic information,” in\\nICASSP 2022-2022 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP).\\nIEEE, 2022, pp. 7367–7371.\\n[30] S. Zhang, X. Zhao, and Q. Tian, “Spontaneous speech emotion recog-\\nnition using multiscale deep convolutional lstm,” IEEE Transactions on\\nAffective Computing, 2019.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='nition using multiscale deep convolutional lstm,” IEEE Transactions on\\nAffective Computing, 2019.\\n[31] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” 2018.\\n[32] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, “Cbam: Convolutional\\nblock attention module,” in Proceedings of the European conference on\\ncomputer vision (ECCV), 2018, pp. 3–19.\\n[33] P. Li, Y. Song, I. V. McLoughlin, W. Guo, and L.-R. Dai, “An atten-\\ntion pooling based representation learning method for speech emotion\\nrecognition,” 2018.\\n[34] Y.-X. Xi, Y. Song, L.-R. Dai, I. McLoughlin, and L. Liu, “Frontend\\nattributes disentanglement for speech emotion recognition,” in ICASSP\\n2022-2022 IEEE International Conference on Acoustics, Speech and\\nSignal Processing (ICASSP).\\nIEEE, 2022, pp. 7712–7716.\\n[35] S. Schneider, A. Baevski, R. Collobert, and M. Auli, “wav2vec:\\nUnsupervised pre-training for speech recognition,” arXiv preprint\\narXiv:1904.05862, 2019.\\n[36] A.\\nBaevski,', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Unsupervised pre-training for speech recognition,” arXiv preprint\\narXiv:1904.05862, 2019.\\n[36] A.\\nBaevski,\\nS.\\nSchneider,\\nand\\nM.\\nAuli,\\n“vq-wav2vec:\\nSelf-\\nsupervised learning of discrete speech representations,” arXiv preprint\\narXiv:1910.05453, 2019.\\n[37] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A frame-\\nwork for self-supervised learning of speech representations,” Advances\\nin Neural Information Processing Systems, vol. 33, pp. 12 449–12 460,\\n2020.\\n[38] L. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from speech\\nusing wav2vec 2.0 embeddings,” arXiv preprint arXiv:2104.03502,\\n2021.\\n[39] X. Cai, J. Yuan, R. Zheng, L. Huang, and K. Church, “Speech emotion\\nrecognition with multi-task learning,” in Interspeech, vol. 2021, 2021,\\npp. 4508–4512.\\n[40] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='pp. 4508–4512.\\n[40] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\\n“An image is worth 16x16 words: Transformers for image recognition\\nat scale,” arXiv preprint arXiv:2010.11929, 2020.\\n[41] N.\\nRistea,\\nR.\\nIonescu,\\nand\\nF.\\nKhan,\\n“Septr:\\nSeparable\\ntrans-\\nformer for audio spectrogram processing. arxiv 2022,” arXiv preprint\\narXiv:2203.09581.\\n[42] H.-t. Xu, J. Zhang, and L.-r. Dai, “Differential time-frequency log-mel\\nspectrogram features for vision transformer based infant cry recogni-\\ntion,” Proc. Interspeech 2022, pp. 1963–1967, 2022.\\n[43] Q.-L. Zhang and Y.-B. Yang, “Sa-net: Shuffle attention for deep con-\\nvolutional neural networks,” in ICASSP 2021-2021 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP).\\nIEEE, 2021, pp. 2235–2239.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='volutional neural networks,” in ICASSP 2021-2021 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP).\\nIEEE, 2021, pp. 2235–2239.\\n[44] J. Park, S. Woo, J.-Y. Lee, and I. S. Kweon, “Bam: Bottleneck attention\\nmodule,” arXiv preprint arXiv:1807.06514, 2018.\\n[45] Y. Cao, J. Xu, S. Lin, F. Wei, and H. Hu, “Gcnet: Non-local networks\\nmeet squeeze-excitation networks and beyond,” in Proceedings of the\\nIEEE/CVF international conference on computer vision workshops,\\n2019, pp. 0–0.\\n[46] Y. Wu and K. He, “Group normalization,” in Proceedings of the\\nEuropean conference on computer vision (ECCV), 2018, pp. 3–19.\\n[47] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “Shufflenet v2: Practical\\nguidelines for efficient cnn architecture design,” in Proceedings of the\\nEuropean conference on computer vision (ECCV), 2018, pp. 116–131.\\n[48] J. Guo, K. Han, H. Wu, Y. Tang, X. Chen, Y. Wang, and C. Xu,\\n“Cmt: Convolutional neural networks meet vision transformers,” in', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='“Cmt: Convolutional neural networks meet vision transformers,” in\\nProceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, 2022, pp. 12 175–12 185.\\n[49] Q. Hou, D. Zhou, and J. Feng, “Coordinate attention for efficient\\nmobile network design,” in Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, 2021, pp. 13 713–13 722.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n13\\n[50] A. Trockman and J. Z. Kolter, “Patches are all you need?” arXiv preprint\\narXiv:2201.09792, 2022.\\n[51] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang,\\nY. Zhu, R. Pang, V. Vasudevan et al., “Searching for mobilenetv3,”\\nin Proceedings of the IEEE/CVF international conference on computer\\nvision, 2019, pp. 1314–1324.\\n[52] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N.\\nChang, S. Lee, and S. S. Narayanan, “Iemocap: Interactive emotional', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Chang, S. Lee, and S. S. Narayanan, “Iemocap: Interactive emotional\\ndyadic motion capture database,” Language resources and evaluation,\\nvol. 42, no. 4, pp. 335–359, 2008.\\n[53] F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, B. Weiss et al.,\\n“A database of german emotional speech.” in Interspeech, vol. 5, 2005,\\npp. 1517–1520.\\n[54] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond\\nempirical risk minimization,” arXiv preprint arXiv:1710.09412, 2017.\\n[55] S. Suganya and E. Charles, “Speech emotion recognition using deep\\nlearning on audio recordings,” in 2019 19th International Conference\\non Advances in ICT for Emerging Regions (ICTer), vol. 250.\\nIEEE,\\n2019, pp. 1–6.\\n[56] S. Latif, R. Rana, S. Khalifa, R. Jurdak, and B. W. Schuller, “Deep\\narchitecture enhancing robustness to noise, adversarial attacks, and\\ncross-corpus setting for speech emotion recognition,” arXiv preprint\\narXiv:2005.08453, 2020.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='architecture enhancing robustness to noise, adversarial attacks, and\\ncross-corpus setting for speech emotion recognition,” arXiv preprint\\narXiv:2005.08453, 2020.\\n[57] J. Wang, M. Xue, R. Culhane, E. Diao, J. Ding, and V. Tarokh, “Speech\\nemotion recognition with dual-sequence lstm architecture,” in ICASSP\\n2020-2020 IEEE International Conference on Acoustics, Speech and\\nSignal Processing (ICASSP).\\nIEEE, 2020, pp. 6474–6478.\\n[58] S. Li, X. Xing, W. Fan, B. Cai, P. Fordson, and X. Xu, “Spatiotem-\\nporal and frequential cascaded attention networks for speech emotion\\nrecognition,” Neurocomputing, vol. 448, pp. 238–248, 2021.\\n[59] S. Latif, R. Rana, S. Khalifa, R. Jurdak, J. Epps, and B. W. Schuller,\\n“Multi-task semi-supervised adversarial autoencoding for speech emo-\\ntion recognition,” IEEE Transactions on Affective Computing, vol. 13,\\nno. 2, pp. 992–1004, 2022.\\n[60] Y. Gao, J. Liu, L. Wang, and J. Dang, “Metric learning based feature\\nrepresentation with gated fusion model for speech emotion recognition.”', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='[60] Y. Gao, J. Liu, L. Wang, and J. Dang, “Metric learning based feature\\nrepresentation with gated fusion model for speech emotion recognition.”\\nin Interspeech, 2021, pp. 4503–4507.\\n[61] D. Dai, Z. Wu, R. Li, X. Wu, J. Jia, and H. Meng, “Learning discrimi-\\nnative features from spectrograms using center loss for speech emotion\\nrecognition,” in ICASSP 2019-2019 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP).\\nIEEE, 2019, pp.\\n7405–7409.\\n[62] T. Tuncer, S. Dogan, and U. R. Acharya, “Automated accurate speech\\nemotion recognition system using twine shuffle pattern and iterative\\nneighborhood component analysis techniques,” Knowledge-Based Sys-\\ntems, vol. 211, p. 106547, 2021.\\n[63] S. Zhong, B. Yu, and H. Zhang, “Exploration of an independent training\\nframework for speech emotion recognition,” IEEE Access, vol. 8, pp.\\n222 533–222 543, 2020.\\n[64] L. Kerkeni, Y. Serrestou, K. Raoof, M. Mbarki, M. A. Mahjoub, and\\nC. Cleder, “Automatic speech emotion recognition using an optimal', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='C. Cleder, “Automatic speech emotion recognition using an optimal\\ncombination of features based on emd-tkeo,” Speech Communication,\\nvol. 114, pp. 22–35, 2019.\\n[65] M. Hou, Z. Zhang, Q. Cao, D. Zhang, and G. Lu, “Multi-view speech\\nemotion recognition via collective relation construction,” IEEE/ACM\\nTransactions on Audio, Speech, and Language Processing, vol. 30, pp.\\n218–229, 2021.\\n[66] S.\\nMehta\\nand\\nM.\\nRastegari,\\n“Mobilevit:\\nlight-weight,\\ngeneral-\\npurpose,\\nand\\nmobile-friendly\\nvision\\ntransformer,”\\narXiv\\npreprint\\narXiv:2110.02178, 2021.\\n[67] Mehta, Sachin and Rastegari, Mohammad, “Separable self-attention for\\nmobile vision transformers,” arXiv preprint arXiv:2206.02680, 2022.\\n[68] S. N. Wadekar and A. Chaurasia, “Mobilevitv3: Mobile-friendly vision\\ntransformer with simple and effective fusion of local, global and input\\nfeatures,” arXiv preprint arXiv:2209.15159, 2022.\\nXiaoyu Tang (Member, IEEE) received the B.S.\\ndegree from South China Normal University in 2003', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='features,” arXiv preprint arXiv:2209.15159, 2022.\\nXiaoyu Tang (Member, IEEE) received the B.S.\\ndegree from South China Normal University in 2003\\nand the M.S. degree from Sun Yat-sen University\\nin 2011. He is currently pursuing the Ph.D. degree\\nwith South China Normal University. He is working\\nwith the School of Physics and Telecommunication\\nEngineering, South China Normal University, where\\nhe engaged in information system development. His\\nresearch interests include machine vision, intelligent\\ncontrol, and the Internet of Things. He is a member\\nof the IEEE ICICSP Technical Committee.\\nYixin Lin received the B.Eng. degree from the\\nSchool of Physics and Telecommunication Engi-\\nneering, South China Normal University, in 2021,\\nwhere he is currently pursuing the M.E. degree\\nwith the Department of Electronics and Information\\nEngineering. His research interests include artificial\\nintelligence and speech emotion recognition.\\nTing Dang is a Senior Research Scientist at Nokia\\nBell Labs, Cambridge, UK. Before joining Nokia,\\nshe worked as a Senior Research Associate at the\\nUniversity of Cambridge. Dr. Dang earned her Ph.D.\\ndegree from the University of New South Wales\\nin Sydney, Australia, and holds MEng and BEng\\ndegrees in Signal Processing from the Northwest-\\nern Polytechnical University in China. Her primary\\nresearch interests are on exploring the potential of\\naudio signals (e.g., speech) for mobile health, i.e.,\\nautomatic disease and mental state prediction and', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='ern Polytechnical University in China. Her primary\\nresearch interests are on exploring the potential of\\naudio signals (e.g., speech) for mobile health, i.e.,\\nautomatic disease and mental state prediction and\\nmonitoring (e.g., COVID-19, emotion) via mobile and wearable audio sensing.\\nHer work aims to develop generalised and interpretable machine learning\\nmodels to improve healthcare delivery. She served as the (senior) program\\ncommittee and invited reviewer for more than 30 top-tier conferences and\\njournals, such as AAAI, NeurIPS, ICASSP, IEEE TAC, IEEE TASL etc. She\\nhas won the Asian Dean’s Forum (ADF) Rising Star Women in Engineering\\nAward 2022, IEEE Early Career Writing Retreat Grant 2019 and ISCA Grant\\n2017.\\nYuanfang Zhang is currently principal researcher\\nat Autocity (Shenzhen) autonomous driving Co.,ltd.\\nHe got his dual Ph.D. degrees with the School\\nof Computer Science at Northwestern Polytechnical\\nUniversity, Shaanxi Province, China and Faculty\\nof Engineering and IT, University of Technology\\nSydney, Australia. His current research focuses on\\nunmanned vehicle sensing technology, multimodal\\nlearning methodology, comprehensive computer vi-\\nsion.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n14\\nJintao Cheng received his bachelor’s degree from\\nthe school of Physics and Telecommunications En-\\ngineering, South China Normal University in 2021.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='14\\nJintao Cheng received his bachelor’s degree from\\nthe school of Physics and Telecommunications En-\\ngineering, South China Normal University in 2021.\\nHis research is computer vision, SLAM and deep\\nlearning.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Vision Transformer: Vit and its Derivatives\\nZujun Fu\\nMay 25, 2022\\nAbstract\\nTransformer, an attention-based encoder-decoder architecture, has not only revolutionized the\\nﬁeld of natural language processing (NLP), but has also done some pioneering work in the ﬁeld\\nof computer vision (CV). Compared to convolutional neural networks (CNNs), the Vision Trans-\\nformer (ViT) relies on excellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the self-attention mecha-\\nnism in natural language processing, where word embeddings are replaced with patch embeddings.\\nThis paper reviews the derivatives in the ﬁeld of ViT and the cross-applications of ViT with\\nother ﬁelds.\\n1\\nPyramid Vision Transformer\\nTo overcome the quadratic complexity of the attention mechanism, the Pyramid Vision Trans-\\nformer (PVT) uses a variant of self-attention called Spatial-Reduced Attention (SRA). It is character-\\nized by spatial reduction of keys and values, similar to Linformer attention in the NLP ﬁeld.\\nBy applying SRA, the feature space dimension of the whole model is slowly reduced and the\\nconcept of order is enhanced by applying positional embedding in all transformer blocks.PVT has\\nbeen used as a backbone network for object detection and semantic segmentation to process high\\nresolution images.', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='concept of order is enhanced by applying positional embedding in all transformer blocks.PVT has\\nbeen used as a backbone network for object detection and semantic segmentation to process high\\nresolution images.\\nLater on, the research team further improved their PVT model named PVT-v2, with the following\\nmajor improvements.\\n• overlapping patch embedding\\n• convolutional feedforward networks\\n• linear-complexity self-attention layers.\\nFigure 1: Overall architecture of the proposed Pyramid Vision Transformer (PVT).\\n1\\narXiv:2205.11239v2  [cs.CV]  24 May 2022\\nFigure 2: PVT-v2.\\nFigure 3: Swin-transformer\\nOverlapping patches is a simple and general idea to improve ViT, especially for dense tasks\\n(e.g. semantic segmentation).By exploiting overlapping regions/patch, PVT-v2 can obtain more local\\ncontinuity of image representations.\\nConvolution between fully connected layers (FC) eliminates the need for ﬁxed size positional\\nencoding in each layer. The 3x3 deep convolution with zero padding (p=1) is designed to compensate\\nfor the removal of positional encoding from the model (they are still present, but only in the input).\\nThis process allows more ﬂexibility to handle multiple image resolutions.\\nFinally, using key and value pooling(p=7), the self-attentive layer is reduced to a complexity\\nsimilar to that of a CNN.\\n2\\nSwin Transformer: Hierarchical Vision Transformer using\\nShifted Windows', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='similar to that of a CNN.\\n2\\nSwin Transformer: Hierarchical Vision Transformer using\\nShifted Windows\\nSwin Transformer aims to build the idea of locality from the standard NLP transformer, i.e. local\\nor window attention:\\nIn the Swin Transformer, local self-attention is used for non-overlapping windows. The next layer\\nof window-to-window communication produces hierarchical representation by progressively merging\\nwindows.\\nAs shown in Figure 3, the left shows the regular window partitioning scheme in the ﬁrst layer,\\nwhere self-attention is computed within each window. The window partitioning in the second layer on\\nthe right is shifted by 2 image patches, resulting in crossing the boundary of the previous window.\\n2\\nFigure 4: local attention\\nFigure 5: scaling on jft data\\nThe local self-attention scales linearly with image size O(M ∗N) instead of O(N 2) in the window\\nsize used for sequence length N and M.\\nBy merging and adding many local layers, there is a global representation. In addition, the spatial\\ndimensions of the feature maps has been signiﬁcantly reduced. The authors claim to have achieved\\npromising results on both ImageNet-1K and ImageNet-21K.\\n3\\nScaling Vision Transformer\\nDeep learning and scale are related. In fact, scale is a key component in pushing the state-of-the-\\nart. In this study, the authors from Google Brain Research trained a slightly modiﬁed ViT model with', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='art. In this study, the authors from Google Brain Research trained a slightly modiﬁed ViT model with\\n2 billion parameters and achieved a top-1 accuracy of 90.45% on ImageNet. This over-parameterized\\ngeneralized model was tested on few-shot learning, with only 10 examples per class. A top-1 accuracy\\nof 84.86% was achieved on ImageNet.\\nFew-shot learning refers to ﬁne-tuning a model with an extremely limited number of samples. The\\ngoal of few-shot learning is to motivate generalization by slightly adapting the acquired pre-trained\\nknowledge to a speciﬁc task. If large models are successfully pre-trained, it makes sense to perform\\nwell with a very limited understanding of the downstream task (provided by only a few examples).\\nThe following are some of the core contributions and main results of this paper.\\n• Representation quality can be bottlenecked by model size, given that you have enough data to\\nfeed it;\\n• Large models beneﬁt from additional supervised data, even over 1B images.\\nFigure 5 depicts the eﬀect of switching from a 300M image dataset (JFT-300M) to 3 billion\\nimages (JFT-3B) without any further scaling.\\nBoth the medium (B/32) and large (L/16) models\\nbeneﬁt from adding data, roughly by a constant factor.\\nResults are obtained by few-shot(linear)\\nevaluation throughout the training process.\\n3\\nFigure 6: Weight decay decoupling eﬀect', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='Results are obtained by few-shot(linear)\\nevaluation throughout the training process.\\n3\\nFigure 6: Weight decay decoupling eﬀect\\n• Larger models are more sample eﬃcient, achieving the same level of error rate with fewer visible\\nimages.\\n• To save memory, they remove class tokens (cls). Instead, they evaluated global average pooling\\nand multi-head attention pooling to aggregate the representations of all patch tokens.\\n• They use diﬀerent weight decay for the head and the rest of the layers called ’body’. The authors\\ndemonstrate this well in the Figure 6. The box values are few-shot accuracy, while the horizontal\\nand vertical axes indicate the weight decay for the body and the head, respectively. Surprisingly,\\nthe stronger decay of the head produces the best results. The authors speculate that a strong\\nweight decay of the head leads to representations with a larger margin between classes.\\nThis is perhaps the most interesting ﬁnding that can be more widely applied to pre-training ViT.\\nThey used a warm-up phase at the beginning of training and a cool-down phase at the end\\nof training, where the learning rate linearly anneals to zero. In addition, they used the Adafactor\\noptimizer, which has a memory overhead of 50% compared to traditional Adam.\\n4\\nReplacing self-attention: independent token + channel mix-\\ning methods\\nIt is well known that self-attention can be used as an information routing mechanism with fast', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='4\\nReplacing self-attention: independent token + channel mix-\\ning methods\\nIt is well known that self-attention can be used as an information routing mechanism with fast\\nweights. So far, 3 papers tell the same story: replacing self-attention with 2 information mixing layers;\\none for mixing token (projected patch vector) and one for mixing channel/feature information.\\n4.1\\nMLP-Mixer\\nThe MLP-Mixer contains two MLP layers: the ﬁrst applied independently to the image patches\\n(i.e., ” mixing” the features at each location) and the other across the patches (i.e., ” mixing” the\\nspatial information).MLP Mixer architecture is shown in Figure 7.\\n4\\nFigure 7: MLP Mixer architecture\\nFigure 8: XCiT architecture\\n4.2\\nXCiT: Cross-Covariance Image Transformers\\nThe other is the recent architecture XCiT, which aims to modify the core building block of ViT:\\nself-attention applied to the token dimension.XCiT architecture is shown in Figure 8.\\nXCA: For information mixing, the authors propose a cross-covariance attention (XCA) function\\nthat operates on the feature dimension of a token rather than on its own. Importantly, this method is\\nonly applicable to the L2-normalized set of queries, keys, and values. the L2 norm is denoted by the', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='that operates on the feature dimension of a token rather than on its own. Importantly, this method is\\nonly applicable to the L2-normalized set of queries, keys, and values. the L2 norm is denoted by the\\nhat above the letters K and Q. The result of multiplication is also normalized to [-1,1] before softmax.\\nLocal Patch Interaction: To achieve explicit communication between the patches, the re-\\nsearchers added two depth-wise 3×3 convolutional layers with Batch Normalization and GELU non-\\nlinearity in between, as shown in Figure 9. Depth-wise convolution was applied to each channel (here\\nthe patch) independently.\\nFigure 9: depthwise convolutions\\n5\\nFigure 10: ConvMixer architecture\\nFigure 11: depthwise convolution with pointwise convolution\\n4.3\\nConvMixer\\nSelf-attention and MLP are theoretically more general modeling mechanisms, as they allow for\\nlarger receptive ﬁelds and content-aware behaviour. Nevertheless, the inductive bias of convolution\\nhas undeniable results in computer vision tasks.\\nMotivated by this, researchers have proposed another variant based on convolutional networks\\ncalled ConvMixer, as shown in Figure 10. the main idea is that it operates directly on the patches\\nas input, separating the mixing of spatial and channel dimensions and maintaining the same size and\\nresolution throughout the network.\\nMore speciﬁcally, depthwise convolution is responsible for mixing spatial locations, while pointwise', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='as input, separating the mixing of spatial and channel dimensions and maintaining the same size and\\nresolution throughout the network.\\nMore speciﬁcally, depthwise convolution is responsible for mixing spatial locations, while pointwise\\nconvolution (1x1x channel kernel) for mixing channel locations, as shown in the Figure 11.\\nMixing of distant spatial locations can be achieved by selecting a larger kernel size to create a\\nlarger receptive ﬁeld.\\n5\\nMultiscale Vision Transformers\\nThe CNN backbone architecture beneﬁts from the gradual increase of channels while reducing the\\nspatial dimension of the feature map. Similarly, the Multiscale Vision Transformer (MViT) exploits\\nthe idea of combining a multi-scale feature hierarchies with a Vision Transformer model. In practice,\\nthe authors start with an initial image size of 3 channels and gradually expand (hierarchically) the\\nchannel capacity while reducing the spatial resolution.\\nThus, a multi-scale feature pyramid is created, as shown in Figure 12. Intuitively, the early layers\\nwill learn high-spatial with simple low-level visual information, while the deeper layers are responsible\\nfor complex high-dimensional features.\\n6\\nFigure 12: Multi-scale Vit\\nFigure 13: Block-based and architecture-based / module-based space-time attention architectures for\\nvideo recognition\\n6\\nVideo classiﬁcation: Timesformer\\nAfter a successful image task, the Vision Transformer is applied to video recognition. Two archi-\\ntectures are presented here,as shown in Figure 13.\\n• Right: Reducing the architecture level.', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='After a successful image task, the Vision Transformer is applied to video recognition. Two archi-\\ntectures are presented here,as shown in Figure 13.\\n• Right: Reducing the architecture level.\\nThe proposed method applies a spatial Transformer\\nto the projection image patches and then has another network responsible for capturing time\\ncorrelations. This is similar to the winning strategy of CNN+LSTM based on video processing.\\n• Left: Space-time attention that can be implemented at the self-attention level, with the best\\ncombination in the red box. Attention is applied sequentially in the time domain by ﬁrst treating\\nthe image frames as tokens. Then, the combined space attention of the two spatial dimensions\\nis applied before the MLP projection. Figure 14 is the t-SNE visualization of the method.\\nIn Figure 14, each video is visualized as a point. Videos belonging to the same action category\\nhave the same color. A TimeSformer with split space-time attention learns more separable features\\nsemantically than a TimeSformer with only space attention or ViT.\\n7\\nFigure 14: Feature visualization with t-SNE of Timesformer\\nFigure 15: Segformer architecture\\n7\\nViT in semantic segmentation: SegFormer\\nNVIDIA has proposed a well-conﬁgured setup called SegFormer. SegFormer has an interesting\\ndesign component. First, it consists of a hierarchical Transformer encoder that outputs multi-scale\\nfeatures. Second, it does not require positional encoding, which can deteriorate performance when the\\ntest resolution is diﬀerent from the training.', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='features. Second, it does not require positional encoding, which can deteriorate performance when the\\ntest resolution is diﬀerent from the training.\\nSegFormer ,as shown in Figure 15, uses a very simple MLP decoder to aggregate the multi-scale\\nfeatures of the encoder. Contrary to ViT, SegFormer uses small image patches, such as 4 x 4, which are\\nknown to favor intensive prediction tasks. The proposed Transformer encoder outputs 1/4, 1/8, 1/16,\\n1/32 multi-scale features at the original image resolution. These multi-level features are provided to\\nthe MLP decoder to predict the segmentation mask.\\nMix-FFN in Figure 15: In order to mitigate the impact of positional encoding, the researchers\\nuse zero-padding 3 × 3 convolutional layers to leak location information.Mix-FFN can be expressed\\nas follows.\\nxout = MLP(GELU(Conv(MLP(xin)))) + xin\\nEﬃcient self-attention is proposed in PVT, which uses a reduction ratio to reduce the length of\\nthe sequence. The results can be measured qualitatively by visualizing the eﬀective receptive ﬁeld\\n(ERF) as shown in Figure 16.\\n8\\nFigure 16: SegFormer’s encoder naturally produces local attention, similar to the convolution of lower\\nstages, while being able to output highly non-local attention, eﬀectively capturing the context of Stage-', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='Figure 16: SegFormer’s encoder naturally produces local attention, similar to the convolution of lower\\nstages, while being able to output highly non-local attention, eﬀectively capturing the context of Stage-\\n4. As shown in the enlarged patch, the ERF in the MLP header (blue box) diﬀers from Stage-4 (red\\nbox) in that local attention is signiﬁcantly stronger in addition to non-local attention.\\nFigure 17: Unetr architecture\\n8\\nVision Transformers in Medical imaging: Unet + ViT =\\nUNETR\\nAlthough there are other attempts in medical imaging, UNETR provides the most convincing\\nresults. In this approach, ViT is applied to 3D medical image segmentation. It was shown that a\\nsimple adaptation is suﬃcient to improve the baselines for several 3D segmentation tasks.\\nEssentially, UNETR uses the Transformer as an encoder to learn the sequence representation of\\nthe input audio, as in Figure 17. Similar to the Unet model, it aims to eﬃciently capture global\\nmulti-scale information that can be passed to the decoder through long skip connections, forming skip\\nconnections at diﬀerent resolutions to compute the ﬁnal semantic segmentation output.\\n9\\nReferences\\n[1] Wang, W., Xie, E., Li, X., Fan, D. P., Song, K., Liang, D., ... & Shao, L. (2021). Pyramid', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='9\\nReferences\\n[1] Wang, W., Xie, E., Li, X., Fan, D. P., Song, K., Liang, D., ... & Shao, L. (2021). Pyramid\\nvision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint\\narXiv:2102.12122.\\n[2] Wang, S., Li, B. Z., Khabsa, M., Fang, H., & Ma, H. (2020). Linformer: Self-attention with linear\\ncomplexity. arXiv preprint arXiv:2006.04768.\\n[3] Wang, W., Xie, E., Li, X., Fan, D. P., Song, K., Liang, D., ... & Shao, L. (2021). Pvtv2: Improved\\nbaselines with pyramid vision transformer. arXiv preprint arXiv:2106.13797.\\n[4] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer:\\nHierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030.\\n[5] Zhai, X., Kolesnikov, A., Houlsby, N., & Beyer, L. (2021). Scaling vision transformers. arXiv\\npreprint arXiv:2106.04560.', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='preprint arXiv:2106.04560.\\n[6] Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., ... & Dosovitskiy,\\nA. (2021). Mlp-mixer: An all-mlp architecture for vision. arXiv preprint arXiv:2105.01601.\\n[7] El-Nouby, A., Touvron, H., Caron, M., Bojanowski, P., Douze, M., Joulin, A., ... & Jegou, H.\\n(2021). XCiT: Cross-Covariance Image Transformers. arXiv preprint arXiv:2106.09681.\\n[8] Patches Are All You Need? Anonymous ICLR 2021 submission\\n[9] Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., & Feichtenhofer, C. (2021). Multiscale\\nvision transformers. arXiv preprint arXiv:2104.11227.\\n[10] Bertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for Video\\nUnderstanding?. arXiv preprint arXiv:2102.05095.', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='Understanding?. arXiv preprint arXiv:2102.05095.\\n[11] Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M., & Luo, P. (2021). SegFormer:\\nSimple and Eﬃcient Design for Semantic Segmentation with Transformers. arXiv preprint\\narXiv:2105.15203.\\n[12] Hatamizadeh, A., Yang, D., Roth, H., & Xu, D. (2021). Unetr: Transformers for 3d medical\\nimage segmentation. arXiv preprint arXiv:2103.10504.\\n10', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'})]\n"
     ]
    }
   ],
   "source": [
    "# Load the document pertaining to a particular topic\n",
    "docs = ArxivLoader(query=\"\"\" all:\"attention mechanisms\" AND (all:\"convolutional neural networks\" OR all:\"CNN\") AND NOT all:\"transformer\" \"\"\", load_max_docs=5).load()\n",
    "\n",
    "# Split the dpocument into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=350, chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunked_documents = text_splitter.split_documents(docs)\n",
    "print(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "5\n",
      "<class 'str'>\n",
      "<class 'list'>\n",
      "199\n",
      "page_content='1\\nPulmonary Disease Classiﬁcation Using Globally\\nCorrelated Maximum Likelihood:\\nan Auxiliary Attention mechanism for\\nConvolutional Neural Networks\\nEdward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, and Faraz Hussain\\nAbstract—Convolutional neural networks (CNN) are now being\\nwidely used for classiﬁying and detecting pulmonary abnormal-\\nities in chest radiographs. Two complementary generalization\\nproperties of CNNs, translation invariance and equivariance,\\nare particularly useful in detecting manifested abnormalities\\nassociated with pulmonary disease, regardless of their spatial\\nlocations within the image. However, these properties also come\\nwith the loss of exact spatial information and global relative\\npositions of abnormalities detected in local regions. Global\\nrelative positions of such abnormalities may help distinguish\\nsimilar conditions, such as COVID-19 and viral pneumonia. In\\nsuch instances, a global attention mechanism is needed, which\\nCNNs do not support in their traditional architectures that\\naim for generalization afforded by translation invariance and\\nequivariance. Vision Transformers provide a global attention\\nmechanism, but lack translation invariance and equivariance,\\nrequiring signiﬁcantly more training data samples to match\\ngeneralization of CNNs. To address the loss of spatial information\\nand global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that\\nserves as an auxiliary attention mechanism to existing CNN\\narchitectures, in order to extract global correlations between\\nsalient features.\\nImpact Statement—We improve sensitivity of Convolutional' metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}\n"
     ]
    }
   ],
   "source": [
    "print(type(docs))\n",
    "print(len(docs))\n",
    "print(type(docs[0].page_content))\n",
    "\n",
    "print(type(chunked_documents))\n",
    "print(len(chunked_documents))\n",
    "print(chunked_documents[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat is a small, carnivorous mammal that is often kept as a pet. They are known for their independent nature, agility, and hunting abilities. Cats have sharp retractable claws, keen senses, and a flexible body, which allows them to climb and jump with ease. They come in various breeds, sizes, colors, and coat patterns. Cats are known for their grooming habits, as they clean themselves by licking their fur. They communicate through a combination of vocalizations, body language, and scent marking. Cats are popular pets due to their companionship, playful nature, and ability to provide comfort to their owners.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "response = completion(\n",
    "    api_key=apikey,\n",
    "    base_url=\"https://drchat.xyz\",\n",
    "    model = \"gpt-3.5-turbo-16k\",\n",
    "    custom_llm_provider=\"openai\",\n",
    "    messages = [{ \"content\": \"What is a cat?\",\"role\": \"user\"}],\n",
    "    temperature=0.5\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response recieved\n",
      "(\"CBAM\" OR \"Convolutional Block Attention Module\") AND \"attention in CNNs\" | \"Convolutional Block Attention Module\" AND \"mechanism\" | \"CBAM\" AND \"convolutional neural networks\"\n",
      "[Document(page_content='1\\nLearning to ignore: rethinking attention in CNNs\\nFiras Laakom*, Kateryna Chumachenko*, Jenni Raitoharju, Alexandros Iosiﬁdis, and Moncef Gabbouj\\nAbstract—Recently, there has been an increasing interest in applying attention mechanisms in Convolutional Neural Networks (CNNs)\\nto solve computer vision tasks. Most of these methods learn to explicitly identify and highlight relevant parts of the scene and pass the\\nattended image to further layers of the network. In this paper, we argue that such an approach might not be optimal. Arguably, explicitly\\nlearning which parts of the image are relevant is typically harder than learning which parts of the image are less relevant and, thus,\\nshould be ignored. In fact, in vision domain, there are many easy-to-identify patterns of irrelevant features. For example, image regions\\nclose to the borders are less likely to contain useful information for a classiﬁcation task. Based on this idea, we propose to reformulate\\nthe attention mechanism in CNNs to learn to ignore instead of learning to attend. Speciﬁcally, we propose to explicitly learn irrelevant\\ninformation in the scene and suppress it in the produced representation, keeping only important attributes. This implicit attention\\nscheme can be incorporated into any existing attention mechanism. In this work, we validate this idea using two recent attention\\nmethods Squeeze and Excitation (SE) block and Convolutional Block Attention Module (CBAM). Experimental results on different\\ndatasets and model architectures show that learning to ignore, i.e., implicit attention, yields superior performance compared to the\\nstandard approaches.\\nIndex Terms—Computer vision, CNNs, attention mechanisms, CBAM, SE\\n!\\n1\\nINTRODUCTION\\nI\\nNSPIRED by the properties of the human visual system,\\nattention mechanisms have been recently applied in the\\nﬁeld of deep learning, resulting in improved performance\\nof the existing models across multiple applications. In the\\ncontext of computer vision, learning to attend, i.e., learning\\nto highlight and emphasize relevant attributes of images,\\nhave led to development of novel approaches [1], [2] in\\nConvolutional Neural Networks (CNNs), improving their\\ncapabilities in many tasks [3], [4], [5].\\nRelated to the concept of attention, recent studies in neu-\\nroscience suggest that the ability of humans to successfully\\nperform visual tasks is related to the ability to ignore and\\nsuppress distractive information [6], [7], [8]. For example,\\nthe authors of [7] show that differences in visual working\\nmemory capacity, i.e., ability to remember visual features\\nof multiple objects, are speciﬁcally related to distractor-\\nsuppression activity in visual cortex. This idea is reinforced\\nin [8], where the authors provide evidence on an inhibitory\\nmechanism of suppression of salient distractors aimed at\\npreventing them from capturing attention and being further\\nprocessed by humans. Additional studies [9] report that\\nignoring the irrelevant information is a powerful learning\\ntool for human cognition with ubiquitous effectiveness.\\nInspired by these ﬁndings, we investigate the intuition of\\nlearning to explicitly ignore irrelevant information in the\\nﬁeld of computer vision and reformulate attention mecha-\\nnisms commonly utilized in CNNs under the framework of\\nlearning to ignore rather than learning to attend.\\nExisting attention mechanisms used in CNNs learn the\\nattention masks by directly optimizing for the high re-\\n* Equal contribution\\nF. Laakom, K. Chumachenko and M. Gabbouj are with Department of Com-\\nputing Sciences, Tampere University, Tampere, Finland, Tampere University,\\nTampere, Finland.\\nJ. Raitoharju is with the Programme for Environmental Information, Finnish\\nEnvironment Institute, Jyv¨\\naskyl¨\\na, Finland.\\nA. Iosiﬁdis is with the Department of Electrical and Computer Engineering,\\nAarhus University, Aarhus, Denmark.\\nsponse of attributes of the image that are important for\\nthe prediction and, thus, should be focused on more. The\\nlearned attention masks are applied to feature representa-\\ntions, leading to higher emphasis put on the attributes of\\ninterest, and, therefore, resulting in implicit ignoration of\\nthe irrelevant features. In our work, we propose to rethink\\nthis logic and instead explicitly focus on ignoring irrelevant\\nregions, hence achieving the attention to important regions\\nimplicitly. We argue that learning of features that should\\nbe ignored is an easier task than learning to attend and,\\ntherefore, optimization with such an objective leads to better\\ntraining. Arguably, discriminative features of samples of\\ndifferent classes are harder to capture and often require\\nmore advanced feature learning. On the other hand, irrel-\\nevant attributes or attributes common between classes are\\noften related to easy-to-identify patterns, such as borderline\\nlocations on the image or background features that can\\nalready be learned at early stages of training. Following\\nthis intuition, we design our method to explicitly optimize\\nwhich attributes of the image should be ignored, and based\\non this, the important attributes that should be attended\\nare derived implicitly. We validate this idea using two\\nrecent attention methods Squeeze and Excitation (SE) block\\nand Convolutional Block Attention Module (CBAM) and\\nshow that indeed our intuition holds and explicitly learning\\nfeatures to ignore leads to better model performance.\\nOur contributions can be summarized as follows:\\n•\\nWe propose a new perspective on attention in com-\\nputer vision where the main aim is to learn to ignore\\ninstead of learning to attend.\\n•\\nWe propose an implicit attention scheme which ex-\\nplicitly learns to identify the irrelevant parts of the\\nscene and suppress them. The proposed approach\\ncan be incorporated into any existing attention mech-\\nanism.\\n•\\nWe validate this idea using two attention mech-\\narXiv:2111.05684v1  [cs.CV]  10 Nov 2021\\n2\\nanisms. Speciﬁcally, we reformulate Squeeze-and-\\nExcitation (SE) block and Convolutional Block At-\\ntention Module (CBAM) using our paradigm, i.e.,\\nlearn to ignore, and show the superiority of such an\\napproach.\\n2\\nRELATED WORK\\nAttention mechanisms in vision. The idea of attention in\\nvision tasks stems from the properties of selective focus in\\nthe human visual system, i.e., that humans do not perceive\\nimages as a whole, but rely on certain salient parts of\\nthem. This property gave rise to a variety of attention-based\\nlearning mechanisms aimed to enhance the performance in\\ncomputer vision domain [3], [4], [10], ﬁnding its applications\\nin a variety of tasks, including sequence learning [11], image\\ncaptioning [5], and others [12], [13]. A subset of attention-\\ndriven methods is directed at CNNs and aims at selecting\\nand highlighting relevant attributes in the feature space\\nduring training [1], [2]. Conventionally, this is achieved by\\nlearning attention masks over feature representations that\\nencode the importance of different attributes in form of\\nweights and applying these masks on intermediate feature\\nrepresentations. This results in higher inﬂuence of features\\nrelevant for decision making in subsequent layers.\\nOther tasks adjacent to this line of research include\\nsaliency estimation, image segmentation, and weakly-\\nsupervised object localization. In saliency estimation, the\\ngoal is to estimate salient, i.e., signiﬁcant regions of the scene\\nwithout any prior knowledge on the scene in unsupervised\\n[14], [15] or supervised manner [16], [17], [18]. In image\\nsegmentation, the task is to partition a given image into\\na set of segments, based on either semantics (semantic\\nsegmentation) or individual objects (instance segmentation)\\n[19]. In weakly-supervised object localization, the goal is\\nto predict the location of the object given only image-level\\nlabels [20].\\nWithin the attention mechanisms utilized in CNNs, two\\nof the notable ones include Squeeze-and-Excitation block\\n(SE) [1] and Convolutional Block Attention Module (CBAM)\\n[2]. In SE, an attention mask is learned channel-wise based\\non global average-pooled features of intermediate represen-\\ntations and applied at multiple layers of the ResNet archi-\\ntecture [21]. A further extension is the CBAM method that\\nenriches the SE mechanism by additional max-pooled input\\nand learns spatial attention in addition to channel-wise\\none. The learned attention weight masks are then applied\\nchannel-wise or pixel-wise to corresponding feature maps.\\nThese methods were shown to lead to superior performance\\nacross various domains and can be incorporated in any\\nCNN architecture.\\nLearning by ignoring. Learning by ignoring is a pow-\\nerful learning paradigm, which has been used in various\\nmachine learning applications [22], [23], [24]. It has been\\nleveraged in the context of saliency estimation [14], [23],\\n[25], [26]. For example, the authors of [14] propose an unsu-\\npervised graph-based saliency estimation approach, where\\nauxiliary variables are used to encode prior knowledge on\\nregions to be ignored, such as dark regions, as it is assumed\\nthat they are less-likely to contain salient object. A similar\\napproach was proposed for the color constancy problem\\n[27]. In the context of machine translation, it has been shown\\nthat learning to ignore spurious correlations in the data\\ncan improve the performance of neural networks in zero-\\nshot translation [22]. In the context of domain adaptation,\\na learning framework assigning and learning an ’ignoring’\\nscore for each training sample and re-weighting the total\\nloss based on these scores was proposed in [24].\\n3\\nLEARNING TO IGNORE IN CNNS\\nAttention in CNNs is generally formulated in a form of a\\nlearned attention mask that emphasizes relevant informa-\\ntion in a feature map. Formally, given a feature map F,\\nattention can be deﬁned as follows:\\nF′ = F ⊗fθ(F),\\n(1)\\nwhere F′ is the attended feature map output, ⊗is the\\nelement-wise multiplication and fθ(·) is an attention func-\\ntion with learnable parameters θ, which takes as input a\\nfeature map F and returns an attention mask fθ(F) ∈[0, 1].\\nThis mask is then element-wise multiplied with the original\\nmap F in order to produce the output map F′. The mask\\nfθ(F) is expected to identify relevant spatial or channel\\ninformation and output the ’importance score’ for each\\nattribute, producing high response for most relevant regions\\nand smaller values for regions of lesser interest. This can be\\nseen as an explicit attention mechanism, where the model\\nfθ(·) learns to directly identify and highlight relevant infor-\\nmation.\\nIn this work, we develop a new formulation of the\\nconcept of attention in CNNs, where the main target is\\nlearning to ignore instead of learning to attend. By training\\nthe model to predict irrelevance of features, rather than their\\nimportance, we expect to simplify the training objective and,\\nhence, to improve the learning of the model. Our approach\\nconsists of a function which learns to identify irrelevant\\nor confusing parts of the feature map in order to suppress\\nthem, followed by inversion of predicted irrelevance scores.\\nFormally, this can be formulated as follows:\\nF′ = F ⊗T(gθ(F)),\\n(2)\\nwhere gθ(·) is a function with learned parameters θ that\\nis expected to learn to highlight information in the feature\\nmap that is irrelevant or confusing for the prediction. This\\ncan be seen as an ignoring mask that outputs high values\\nfor attributes and regions that should be suppressed in the\\nfeature map. The function T(·) is a function with an output\\nT(x) inversely proportional to x, hence ﬂipping the learned\\nignoring mask and transforming it into an attention mask.\\nSimilarly to Eq. (1), the ﬁnal feature map F′ is obtained\\nby element-wise multiplication of the input map F and the\\nﬂipped ignoring mask T(gθ(F)).\\nGiven an ignoring mask gθ(F), the function T(·) can\\nbe any function satisfying the condition of being inversely\\nproportional to its input and bounded between [0, 1]. In this\\nwork, we propose three variants:\\nT1(x) = 1 −αx,\\n(3)\\nT2(x) = sigmoid( 1\\nx),\\n(4)\\n3\\nT3(x) = sigmoid(−x).\\n(5)\\nThe ﬁrst variant T1(·) linearly converts the ignoring mask\\nto an attention one, and α is a hyper-parameter controlling\\nthis linear scaling. The extreme case α = 0 corresponds\\nto the extreme case F′ = F, i.e., none of the features\\nare emphasized or suppressed. For the second and third\\nvariants T2 and T3, a sigmoid function is applied to ensure\\nthat the output is bounded between [0, 1].\\nWe argue that formulating the objective as learning of\\nirrelevant features that should be ignored, as opposed to\\nfocusing on important features, is beneﬁcial, as optimization\\nof a model with such an objective is easier. This is due\\nto potential presence of many easy-to-identify patterns of\\nirrelevant attributes, such as borderline pixel locations, color\\nand lighting perturbations, or background properties that\\nare not correlated with the groundtruth labels. At the same\\ntime, information responsible for predictions is generally\\nlabel-speciﬁc and harder to capture. Moreover, learning of\\ndiscriminative attributes that can be regarded as important\\noften requires learning of complex feature representations\\nthat can be achieved only at latter stages of training, while\\npatterns irrelevant for decision making can often be identi-\\nﬁed already at the early stages.\\nIt can be argued that standard attention, i.e., Eq. (1),\\nis also learning to ignore as it is expected to indirectly\\nassign smaller values for less important regions. However,\\nfunction fθ(·) is optimized directly for highlighting relevant\\ninformation and, hence, this can be seen as an implicit and\\nindirect strategy of learning to ignore. In our approach,\\nEq. (2), the model gθ(·) is explicitly optimized for identifying\\nthe irrelevant or confusing parts and the function T(·)\\nsuppresses them. This can be seen as an implicit learning\\nto attend approach and explicit learning to ignore approach,\\nas opposed to the standard attention which has an explicit\\nlearning to attend formulation.\\nAs can be seen, the main difference between implicit\\nand explicit attention formulations is the presence of a\\nﬂipping function T(·). It can be seen from Eq. (1) and\\nEq. (2) that fθ(·) can be directly replaced by T(gθ(·)). This\\nmakes it straightforward to reformulate any existing explicit\\nattention method to learn to ignore instead of learning to\\nattend by applying an inversion function T(·) on top of the\\nlearned mask. This way, the model gθ(·) can be learned as\\nthe model fθ(·) in conventional attention methods, while\\nits parameters will be optimized to detect irrelevant or\\nconfusing regions instead of relevant ones. In this paper, for\\nthe choice of the function fθ(·), we consider two state-of-\\nthe-art attention mechanisms, namely SE [1] and CBAM [2]\\n, and we show how to reformulate them using our paradigm\\nin the following subsections.\\n3.1\\nIgnoring with Squeeze-and-Excitation blocks\\nSqueeze-and-Excitation (SE) block [1] presents a mechanism\\nto learn channel-wise attention, focusing on which features\\nof the representation are important for prediction. This\\nis achieved by squeezing the spatial information into a\\nchannel representation, followed by an excitation operation\\nthat highlights important channels via a bottleneck block.\\nFormally, given a feature map F, this is deﬁned as follows:\\nfθ(F) = σ(W2δ(W1GAP(F))),\\n(6)\\nwhere GAP(·) denotes Global Average Pooling, δ is a ReLU\\nactivation, σ is the sigmoid function, W1 ∈Rc× c\\nr and\\nW2 ∈R\\nc\\nr ×c are linear layers, c is the number of channels\\nin F, and r is the reduction rate in the bottleneck block.\\nGiven the output fθ(F), the attended feature map is ob-\\ntained by applying the learned mask element-wise between\\ncorresponding channels.\\nTo incorporate our ignoring paradigm into SE, we\\napply T(·) to the output fθ(F), hence transforming its\\nobjective into learning features that should be ignored.\\nSpeciﬁcally, we deﬁne the three variants as: f 1\\nθ (F) = 1 −\\nασ(W2δ(W1GAP(F))); f 2\\nθ (F) = σ(\\n1\\nσ(W2δ(W1GAP (F))));\\nf 3\\nθ (F) = σ(−W2δ(W1GAP(F))) using the deﬁnitions of\\nT1, T2, and T3, respectively. As can be noticed, in the ﬁrst\\ntwo variants T(·) is applied directly on fθ(F), while in the\\nthird case it is applied on pre-sigmoid output to ensure\\nsufﬁciently wide range for attention scores.\\n3.2\\nIgnoring with Convolutional Block Attention Mod-\\nules\\nFollowing the approach of SE, Convolutional Block Atten-\\ntion Module (CBAM) [2] extends it to incorporate spatial\\nattention as well as to enrich channel attention with an\\nadditional input representation. Under the deﬁnition of\\nattention in Eq. (1), this is formulated as follows:\\nf ch(F) = σ(W2δ(W1(GAP(F))) + W2δ(W1(GMP(F)))),\\nFch = F ⊗f ch(F),\\nf sp(Fch) = σ(Conv7×7(GAP(Fch) ⌢GMP(Fch))),\\n(7)\\nwhere f ch and f sp denote channel and spatial attention,\\nrespectively, GAP(·) and GMP(·) correspond to Global\\nAverage Pooling and Global Max Pooling, respectively, δ is\\na ReLU activation, σ is the sigmoid activation, W1 ∈Rc× c\\nr\\nand W2 ∈R\\nc\\nr ×c are linear layers, c is the number of\\nchannels in F, and r is the reduction rate in the bottleneck\\nblock, similarly to SE. Fch is the channel-wise attended\\nfeature map, Conv7×7 denotes a convolutional layer with\\n7 × 7 kernel, and ⌢denotes concatenation.\\nAs can be seen, channel and spatial attention masks are\\napplied sequentially and channel-attended feature represen-\\ntations are used as input to compute spatial attention. Fol-\\nlowing this, we transform CBAM for ignoring by addition\\nof inversion function T(·) on top of both channel function\\nf ch(·) and spatial function f sp(·) to reformulate their objec-\\ntives as learning of features and regions to ignore. In both\\ncases, variants of T1(·) and T2(·) are applied directly on the\\noutput of corresponding functions, and T3(·) is applied on\\npre-sigmoid output.\\n4\\nEXPERIMENTAL RESULTS\\n4.1\\nCIFAR10 & CIFAR100\\nWe start by validating our approach on image classiﬁcation\\ntask using CIFAR10 and CIFAR100 [28] datasets. To show\\ninvariance of the proposed approach to speciﬁc model ar-\\nchitectures, we evaluate two state-of-the-art CNNs, namely,\\n4\\nResNet50 [21] and DenseNet [29] architectures. We report\\nthe results of standard models with no attention, models\\nwith applied CBAM and SE attention blocks, and models\\nwith our proposed ignoring approach with both CBAM\\nand SE variants with the three inversion function variants\\npresented in Section 3.\\nAll the models are optimized using Stochastic Gradient\\nDescent (SGD) [30] with a momentum of 0.9 [31], weight\\ndecay of 0.0001 [32], and a batch size of 128. The initial\\nlearning rate is set to 0.1 and is then decreased by a factor\\nof 5 after 60, 120, and 160 epochs, respectively. The models\\nare trained for 200 epochs with the best performance on the\\nvalidation set used for testing. Each experiment is repeated\\nthree times and the average performance is reported. 40k\\nimages are used for training and 10k for validation. Stan-\\ndard data augmentation is used [33], [34].\\nIn Table 1, we report the experimental results of the\\nstandard model, i.e., no attention, SE, and our different\\nSE-based variants, namely, SE-Igni where i indicates the\\nﬂipping function used (T1 or T2 or T3). For the ﬁrst variant,\\ni.e., SE-Ign1, we experiment with three different values\\nof hyper-parameter α: 1, 0.8, and 0.5. We note that for\\nboth architectures applying an explicit or implicit attention\\nmechanism consistently outperforms the standard model.\\nOn CIFAR10, the best performance is achieved using our\\nthird variant, i.e., SE-Ign3, which improves the results by\\n1% compared to standard and +0.3% compared SE using\\nResNet50 architecture. On CIFAR100, the lowest top1-%\\nerror rates are achieved by SE-Ign3 and SE-Ign1(α=0.5) for\\nResNet50 and DenseNet architectures, respectively. In fact,\\non this dataset our third variant boosts the accuracy by 4%\\ncompared to the standard and 1.85% compared to SE. This\\ncan be explained by the fact that for this dataset only 500\\ntraining samples per class are available, thus making it hard\\nto directly learn the relevant visual features for each class.\\nAt the same time, the irrelevant features are more universal\\nand typically independent of the class, thus making them\\neasier to learn in a scarce data context.\\nIn Table 2, we report the empirical results for the dif-\\nferent CBAM-based variants. As can be seen, the results\\nwith this attention variant are consistent with our ﬁndings\\nusing SE. For both datasets and for both architectures,\\nlearning to ignore yields better performance compared to\\nboth the standard model and the SE attention. The top\\nperformance is achieved by either by CBAM-Ign1(α=0.5)\\nor CBAM-Ign1(α=0.8) variant. More results can be found\\nSupplementary material Table 1.\\n4.2\\nImageNet\\nTo further validate the effectiveness of our learning to\\nignore framework, we perform additional experiments on\\nImageNet dataset [35] using ResNet50. For training on\\nImageNet, optimization is done with SGD with the same\\nweight decay and momentum as used for CIFAR datasets.\\nThe initial learning rate is set to 0.1 and reduced by a factor\\nof 10 after 30, 60, and 80 epochs, respectively. The models\\nare trained for 90 epochs with batch size of 256 with the\\nresults reported on the validation set.\\nTable 3 shows the results on ImageNet dataset, where\\nTop-1 and Top-5 errors are reported. As can be seen, our\\nresults are consistent with ﬁndings on CIFAR10 and CI-\\nFAR100 datasets. Speciﬁcally, we ﬁnd that applying at-\\ntention, whether explicit or implicit, outperforms standard\\nmodel. At the same time, the proposed framework based on\\nignoring outperforms the conventional attention in a vast\\nmajority of cases. In SE variant, SE-Ign1(α=1) and SE-Ign3\\noutperform the conventional approach, while other variants\\nreport competitive results with minimal gap. Best result\\nof SE-Ign3 outperforms the standard model by 1.1%. In\\nCBAM, all variants of CBAM-Ign1 outperform conventional\\napproach on both Top-1 and Top-5 metric, and CBAM-Ign2\\nand CBAM-Ign3 outperform conventional CBAM on Top-\\n5 metric, while being competitive on Top-1 metric. More\\nresults can be found Supplementary material Table 2.\\n4.3\\nNTU-RGBD\\nTo further demonstrate the effectiveness of our approach,\\nwe additionally evaluate the proposed method in the mul-\\ntimodal fusion setting. Here, we rely on the Multimodal\\nTransfer Module (MMTM) [36] architecture for our eval-\\nuation. MMTM is a method for fusing information from\\nmultiple modalities in multiple-stream architectures, which\\nhas recently shown good performance in a variety of tasks,\\nincluding activity recognition, gesture recognition, and au-\\ndiovisual speech enhancement.\\nThe method relies on an architecture inspired from\\nSqueeze-and-Excitation blocks placed between network\\nbranches. Speciﬁcally, considering a two-stream scenario,\\nintermediate feature representations from two network\\nbranches corresponding to two modalities are ﬁrst spatially\\nsqueezed into channel descriptors by applying global av-\\nerage pooling in each branch. The squeezed representa-\\ntions are subsequently concatenated and projected into a\\njoint lower-dimensional space. The resulting features are\\ntransformed with two projection matrices corresponding\\nto each of the two modalities to the spaces of original\\ndimensionalities, and sigmoid activation is then applied to\\nobtain attention masks. The masks are further multiplied\\nelement-wise with original feature representations in each\\nbranch.\\nAs can be seen, the fusion module is essentially a multi-\\nmodal SE-block with joint squeeze and modality-speciﬁc ex-\\ncitation operations, to which we apply our ignoring frame-\\nwork as described in Section 3.1. We perform experiments\\non NTU-RGBD dataset [37] for human action recognition,\\nwhere we fuse the skeleton and RGB modalities, similarly to\\nMMTM [36]. We follow our ignoring paradigm and replace\\nthe SE attention mask in each branch with our proposed\\napproach. The rest of the architecture and training protocol\\nfollows that of MMTM. We initialize the model from Ima-\\ngeNet+Kinectics pretrained weights, ﬁnetune for 10 epochs\\nwith batch size 8, and report the test set performance of the\\nmodel that performed best on validation set. The results are\\nreported in Table 4. As can be seen, the proposed ignoring\\napproaches outperform the baseline in the vast majority of\\ncases.\\n4.4\\nDiscussion\\nAs can be seen from the experimental results in previous\\nsections, learning to ignore consistently yields superior per-\\nformance compared to the baselines. We argue that this\\n5\\nCIFAR 10\\nCIFAR 100\\nTop-1 Error%\\nTop-1 Error%\\nTop-5 Error%\\nResNet50\\nStandard\\n08.27 ± 0.54\\n34.06 ± 1.02\\n10.97 ± 0.54\\nSE\\n07.63 ± 0.37\\n32.80 ± 0.11\\n09.97 ± 0.50\\nSE-Ign1(α=1)\\n07.42 ± 0.29\\n32.50 ± 0.26\\n09.92 ± 0.37\\nSE-Ign1(α=0.5)\\n07.61 ± 0.46\\n31.40 ± 0.68\\n09.39 ± 0.19\\nSE-Ign1(α=0.8)\\n07.76 ± 0.73\\n32.71 ± 1.15\\n10.07 ± 0.64\\nSE-Ign2\\n07.66 ± 0.13\\n32.78 ± 0.77\\n10.11 ± 0.56\\nSE-Ign3\\n07.28 ± 0.17\\n30.95 ± 0.08\\n09.49 ± 0.36\\nDenseNet\\nStandard\\n07.07 ± 0.33\\n29.25 ± 0.10\\n08.26 ± 0.12\\nSE\\n06.96 ± 0.05\\n29.43 ± 0.44\\n08.36 ± 0.33\\nSE-Ign1(α=1)\\n06.94 ± 0.07\\n29.17 ± 0.07\\n08.22 ± 0.13\\nSE-Ign1(α=0.5)\\n06.69 ± 0.04\\n27.64 ± 0.30\\n07.30 ± 0.10\\nSE-Ign1(α=0.8)\\n06.95 ± 0.14\\n27.73 ± 0.41\\n07.39 ± 0.07\\nSE-Ign2\\n06.80 ± 0.09\\n28.08 ± 0.35\\n07.39 ± 0.23\\nSE-Ign3\\n06.41 ± 0.08\\n27.77 ± 0.54\\n07.65 ± 0.20\\nTABLE 1\\nResults of SE variants on CIFAR10 and CIFAR100 datasets.\\nCIFAR 10\\nCIFAR 100\\nTop-1 Error%\\nTop-1 Error%\\nTop-5 Error%\\nResNet50\\nStandard\\n08.27 ± 0.54\\n34.06 ± 1.02\\n10.97 ± 0.54\\nCBAM\\n08.04 ± 0.03\\n31.46 ± 0.20\\n09.32 ± 0.15\\nCBAM-Ign1(α=1)\\n07.78 ± 0.28\\n31.03 ± 0.25\\n09.28 ± 0.27\\nCBAM-Ign1(α=0.5)\\n07.17 ± 0.05\\n30.58 ± 0.20\\n09.25 ± 0.23\\nCBAM-Ign1(α=0.8)\\n07.40 ± 0.23\\n30.28 ± 0.39\\n09.08 ± 0.33\\nCBAM-Ign2\\n07.53 ± 0.29\\n31.42 ± 0.58\\n09.27 ± 0.21\\nCBAM-Ign3\\n07.60 ± 0.10\\n30.88 ± 0.22\\n09.38 ± 0.32\\nDenseNet\\nStandard\\n07.07 ± 0.33\\n29.25 ± 0.10\\n08.26 ± 0.12\\nCBAM\\n07.21 ± 0.23\\n30.63 ± 0.23\\n08.90 ± 0.14\\nCBAM-Ign1(α=1)\\n07.19 ± 0.26\\n29.63 ± 0.46\\n08.37 ± 0.39\\nCBAM-Ign1(α=0.5)\\n06.53 ± 0.14\\n27.92 ± 0.19\\n07.58 ± 0.27\\nCBAM-Ign1(α=0.8)\\n06.40 ± 0.14\\n27.11 ± 0.08\\n07.33 ± 0.19\\nCBAM-Ign2\\n06.80 ± 0.02\\n27.88 ± 0.59\\n07.62 ± 0.05\\nCBAM-Ign3\\n06.68 ± 0.05\\n27.94 ± 0.10\\n07.78 ± 0.21\\nTABLE 2\\nResults of CBAM variants on CIFAR10 and CIFAR100 datasets.\\nTop-1 Error%\\nTop-5 Error%\\nStandard\\n23.73\\n06.85\\nSE\\n22.70\\n06.35\\nSE-Ign1(α=1)\\n22.60\\n06.29\\nSE-Ign1(α=0.5)\\n23.03\\n06.58\\nSE-Ign1(α=0.8)\\n22.88\\n06.30\\nSE-Ign2\\n23.16\\n06.55\\nSE-Ign3\\n22.59\\n06.32\\nCBAM\\n22.91\\n06.58\\nCBAM-Ign1(α=1)\\n22.84\\n06.50\\nCBAM-Ign1(α=0.5)\\n22.84\\n06.52\\nCBAM-Ign1(α=0.8)\\n22.84\\n06.40\\nCBAM-Ign2\\n23.02\\n06.39\\nCBAM-Ign3\\n23.10\\n06.44\\nTABLE 3\\nResults of CBAM and SE with variants of ignoring on ImageNet dataset\\nstems from the fact that learning irrelevant information\\nis easier than identifying what should be attended. For\\nexample, in order to learn features that should be attended\\nto, the model needs to ﬁrst learn to extract patterns such as\\nlines and edges and make associations with the class labels\\nin order to produce a meaningful attention mask. On the\\nother hand, irrelevant patterns, such as background textures\\nand borderline pixels, are often shared across the dataset, are\\npersistent and independent of the class labels, which makes\\nthem easier to learn. Therefore, it should be possible to learn\\nthem already in the early stages of training. Figure 1 shows\\nthe validation loss curves of the baseline attention methods\\nand the best-performing ignoring methods with ResNet50\\non CIFAR100 dataset (more training curves can be found\\nin supplementary material). As can be seen, especially at\\nthe earlier stages of training, our approach results in lower\\nloss with less ﬂuctuations and more stable training, hence\\nsupporting our claim. From an optimization point of view,\\nin the case of α=1, only the gradient of the attention blocks\\nare ﬂipped, and thus in the back-propagation, when they\\nare summed with the gradient of the main block (which are\\nnot ﬂipped), the total feedback carried to the earlier layers\\nis different and does not correspond to a ﬂipped version of\\nthe total sum of the standard attention. Thus, this yields dif-\\nferent feedback and leads to a different optimal solution in\\nthe end of the training (Figure 7 in supplementary material).\\nMoreover, in Figure 2, we provide visual results of the\\nclass activation maps [38] produced by the different models\\non three different samples from validation set of ImageNet.\\nAs can be seen, the learning to ignore formulation leads to\\ndifferent attention maps compared to the explicit attention,\\ni.e., learning to attend. Noticeably, standard CBAM attention\\ntries to capture the relevant parts of the image directly,\\nleading to the prediction being made based on the small\\npart of the input that is considered by the model as the most\\nimportant. This leads to the possibility that the model can\\nmiss some important parts of the class of interest on the\\nimage. As an example, only one of the plants on the lower\\n6\\nMMTM\\nIgn1(α=1)\\nIgn1(α=0.5)\\nIgn1(α=0.8)\\nIgn2\\nIgn3\\nNTU-RGBD\\n89.98\\n89.99\\n90.52\\n88.70\\n90.21\\n90.36\\nTABLE 4\\nAccuracy on NTU-RGBD dataset\\nFig. 1. Validation loss curves of ResNet50 on CIFAR100 using the different attention approaches.\\nﬁgure is considered in CBAM model, as well as only a side\\nof the bus in the middle image. On the other hand, our ap-\\nproach by learning to identify the non-relevant background\\nregions ﬁrst and subsequently suppressing them, simpliﬁes\\nthe problem and typically results in an attention mask that\\nis broader and captures the object of interest better, hence\\nreducing the risk of suppressing relevant attributes of it.\\n5\\nCONCLUSION\\nIn this paper, we provide a new perspective on attention in\\nCNNs where the main target is learning to ignore instead\\nof learning to attend. To this end, we propose an implicit\\nattention scheme with three variants which can be incorpo-\\nrated into any existing attention mechanism. The proposed\\napproach explicitly learns to identify the irrelevant and con-\\nfusing parts of the scene and suppresses them. In addition,\\nwe reformulate two state-of-the-art attention approaches,\\nnamely SE and CBAM, using our learning paradigm. Exper-\\nimental results on three image classiﬁcation datasets show\\nthat learning to ignore, i.e., implicit attention consistently\\noutperforms standard attention across multiple models.\\nACKNOWLEDGMENTS\\nThis work has received funding from the European Union’s\\nHorizon 2020 research and innovation programme un-\\nder grant agreement No 871449 (OpenDR), and the NSF-\\nBusiness Finland Center for Visual and Decision Informatics\\n(CVDI) project AMALIA. The authors wish to acknowledge\\nCSC – IT Center for Science, Finland, for computational\\nresources.\\nREFERENCES\\n[1]\\nJ. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,”\\nin Proceedings of the IEEE conference on computer vision and pattern\\nrecognition, 2018, pp. 7132–7141.\\n[2]\\nS. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, “Cbam: Convolutional\\nblock attention module,” in Proceedings of the European conference\\non computer vision (ECCV), 2018, pp. 3–19.\\n[3]\\nL. Itti, C. Koch, and E. Niebur, “A model of saliency-based visual\\nattention for rapid scene analysis,” IEEE Transactions on pattern\\nanalysis and machine intelligence, vol. 20, no. 11, pp. 1254–1259, 1998.\\n[4]\\nM. Jiang, Y. Yuan, and Q. Wang, “Self-attention learning for person\\nre-identiﬁcation.” in British Machine Vision Conference, 2018, p. 204.\\n[5]\\nK. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,\\nR. Zemel, and Y. Bengio, “Show, attend and tell: Neural image\\ncaption generation with visual attention,” in International confer-\\nence on machine learning.\\nPMLR, 2015, pp. 2048–2057.\\n[6]\\nJ. D. Cosman, K. A. Lowe, W. Zinke, G. F. Woodman, and J. D.\\nSchall, “Prefrontal control of visual distraction,” Current biology,\\nvol. 28, no. 3, pp. 414–420, 2018.\\n[7]\\nJ. M. Gaspar, G. J. Christie, D. J. Prime, P. Jolicœur, and J. J. McDon-\\nald, “Inability to suppress salient distractors predicts low visual\\nworking memory capacity,” Proceedings of the National Academy of\\nSciences, vol. 113, no. 13, pp. 3693–3698, 2016.\\n[8]\\nN. Gaspelin and S. J. Luck, “The role of inhibition in avoiding\\ndistraction by salient stimuli,” Trends in cognitive sciences, vol. 22,\\nno. 1, pp. 79–92, 2018.\\n[9]\\nC. A. Cunningham and H. E. Egeth, “Taming the white bear: Initial\\ncosts and eventual beneﬁts of distractor inhibition,” Psychological\\nscience, vol. 27, no. 4, pp. 476–485, 2016.\\n[10] H. Larochelle and G. E. Hinton, “Learning to combine foveal\\nglimpses with a third-order boltzmann machine,” Advances in\\nneural information processing systems, vol. 23, pp. 1243–1251, 2010.\\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\\narXiv preprint arXiv:1706.03762, 2017.\\n[12] G. Wu, X. Zhu, and S. Gong, “Spatio-temporal associative rep-\\nresentation for video person re-identiﬁcation.” in British Machine\\nVision Conference, 2019, p. 278.\\n[13] F. Zhang, B. Ma, H. Chang, S. Shan, and X. Chen, “Relation-aware\\nmultiple attention siamese networks for robust visual tracking.”\\nin British Machine Vision Conference, 2019.\\n[14] C. Aytekin, A. Iosiﬁdis, and M. Gabbouj, “Probabilistic saliency\\nestimation,” Pattern Recognition, vol. 74, pp. 359–372, 2018.\\n[15] J. Zhang, T. Zhang, Y. Dai, M. Harandi, and R. Hartley, “Deep\\nunsupervised saliency detection: A multiple noisy labeling per-\\nspective,” in Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 2018, pp. 9029–9038.\\n[16] N. Liu, J. Han, and M.-H. Yang, “Picanet: Learning pixel-wise\\ncontextual attention for saliency detection,” in Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recognition, 2018,\\npp. 3089–3098.\\n[17] N. Liu, N. Zhang, K. Wan, J. Han, and L. Shao, “Visual saliency\\ntransformer,” arXiv preprint arXiv:2104.12099, 2021.\\n[18] N. Liu and J. Han, “Dhsnet: Deep hierarchical saliency network\\nfor salient object detection,” in Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, 2016, pp. 678–686.\\n7\\nFig. 2. Visual results of different CBAM-based attention mechanisms on three different samples from validation set of ImageNet. The attention\\nmasks are obtained as in [38].\\n[19] S. Minaee, Y. Y. Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz, and\\nD. Terzopoulos, “Image segmentation using deep learning: A sur-\\nvey,” IEEE Transactions on Pattern Analysis and Machine Intelligence,\\n2021.\\n[20] D. Zhang, J. Han, G. Cheng, and M.-H. Yang, “Weakly supervised\\nobject localization and detection: A survey,” IEEE transactions on\\npattern analysis and machine intelligence, 2021.\\n[21] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\\nimage recognition,” in Proceedings of the IEEE conference on computer\\nvision and pattern recognition, 2016, pp. 770–778.\\n[22] J. Gu, Y. Wang, K. Cho, and V. O. Li, “Improved zero-shot neural\\nmachine translation via ignoring spurious correlations,” arXiv\\npreprint arXiv:1906.01181, 2019.\\n[23] B. Jiang, L. Zhang, H. Lu, C. Yang, and M.-H. Yang, “Saliency\\ndetection via absorbing markov chain,” in Proceedings of the IEEE\\ninternational conference on computer vision, 2013, pp. 1665–1672.\\n[24] X. Zhao, X. He, and P. Xie, “Learning by ignoring, with application\\nto domain adaptation,” arXiv preprint arXiv:2012.14288, 2020.\\n[25] X. Li, H. Lu, L. Zhang, X. Ruan, and M.-H. Yang, “Saliency\\ndetection via dense and sparse reconstruction,” in Proceedings of\\nthe IEEE international conference on computer vision, 2013, pp. 2976–\\n2983.\\n[26] W. Zhu, S. Liang, Y. Wei, and J. Sun, “Saliency optimization from\\nrobust background detection,” in Proceedings of the IEEE conference\\non computer vision and pattern recognition, 2014, pp. 2814–2821.\\n[27] F. Laakom, J. Raitoharju, A. Iosiﬁdis, U. Tuna, J. Nikkanen, and\\nM. Gabbouj, “Probabilistic color constancy,” in 2020 IEEE Inter-\\nnational Conference on Image Processing (ICIP).\\nIEEE, 2020, pp.\\n978–982.\\n[28] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of\\nfeatures from tiny images,” 2009.\\n[29] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger,\\n“Densely connected convolutional networks,” in Proceedings of the\\nIEEE conference on computer vision and pattern recognition, 2017, pp.\\n4700–4708.\\n[30] S. Ruder, “An overview of gradient descent optimization algo-\\nrithms,” arXiv preprint arXiv:1609.04747, 2016.\\n[31] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning\\nrepresentations by back-propagating errors,” nature, vol. 323, no.\\n6088, pp. 533–536, 1986.\\n[32] A. Krogh and J. A. Hertz, “A simple weight decay can improve\\ngeneralization,” in Advances in neural information processing systems,\\n1992, pp. 950–957.\\n[33] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger,\\n“Densely connected convolutional networks,” in Proceedings of the\\nIEEE conference on computer vision and pattern recognition, 2017, pp.\\n4700–4708.\\n[34] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup:\\nBeyond empirical risk minimization,” International Conference on\\nLearning Representations 2018, 2018.\\n[35] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Im-\\nagenet: A large-scale hierarchical image database,” in 2009 IEEE\\nconference on computer vision and pattern recognition.\\nIeee, 2009, pp.\\n248–255.\\n[36] H. R. V. Joze, A. Shaban, M. L. Iuzzolino, and K. Koishida, “Mmtm:\\nMultimodal transfer module for cnn fusion,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\\n2020, pp. 13 289–13 299.\\n[37] A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, “Ntu rgb+ d: A large\\nscale dataset for 3d human activity analysis,” in Proceedings of the\\nIEEE conference on computer vision and pattern recognition, 2016, pp.\\n1010–1019.\\n[38] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\\nD. Batra, “Grad-cam: Visual explanations from deep networks via\\ngradient-based localization,” in Proceedings of the IEEE international\\nconference on computer vision, 2017, pp. 618–626.\\n', metadata={'Published': '2021-11-10', 'Title': 'Learning to ignore: rethinking attention in CNNs', 'Authors': 'Firas Laakom, Kateryna Chumachenko, Jenni Raitoharju, Alexandros Iosifidis, Moncef Gabbouj', 'Summary': 'Recently, there has been an increasing interest in applying attention\\nmechanisms in Convolutional Neural Networks (CNNs) to solve computer vision\\ntasks. Most of these methods learn to explicitly identify and highlight\\nrelevant parts of the scene and pass the attended image to further layers of\\nthe network. In this paper, we argue that such an approach might not be\\noptimal. Arguably, explicitly learning which parts of the image are relevant is\\ntypically harder than learning which parts of the image are less relevant and,\\nthus, should be ignored. In fact, in vision domain, there are many\\neasy-to-identify patterns of irrelevant features. For example, image regions\\nclose to the borders are less likely to contain useful information for a\\nclassification task. Based on this idea, we propose to reformulate the\\nattention mechanism in CNNs to learn to ignore instead of learning to attend.\\nSpecifically, we propose to explicitly learn irrelevant information in the\\nscene and suppress it in the produced representation, keeping only important\\nattributes. This implicit attention scheme can be incorporated into any\\nexisting attention mechanism. In this work, we validate this idea using two\\nrecent attention methods Squeeze and Excitation (SE) block and Convolutional\\nBlock Attention Module (CBAM). Experimental results on different datasets and\\nmodel architectures show that learning to ignore, i.e., implicit attention,\\nyields superior performance compared to the standard approaches.'})]\n",
      "[Document(page_content='IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n1\\nConvolutional Neural Network with Convolutional\\nBlock Attention Module for Finger Vein\\nRecognition\\nZhongxia Zhang, Mingwen Wang\\nAbstract\\nConvolutional neural networks have become a popular research in the ﬁeld of ﬁnger vein recognition because of their powerful\\nimage feature representation. However, most researchers focus on improving the performance of the network by increasing the\\nCNN depth and width, which often requires high computational effort. Moreover, we can notice that not only the importance of\\npixels in different channels is different, but also the importance of pixels in different positions of the same channel is different. To\\nreduce the computational effort and to take into account the different importance of pixels, we propose a lightweight convolutional\\nneural network with a convolutional block attention module (CBAM) for ﬁnger vein recognition, which can achieve a more accurate\\ncapture of visual structures through an attention mechanism. First, image sequences are fed into a lightweight convolutional neural\\nnetwork we designed to improve visual features. Afterwards, it learns to assign feature weights in an adaptive manner with the\\nhelp of a convolutional block attention module. The experiments are carried out on two publicly available databases and the\\nresults demonstrate that the proposed method achieves a stable, highly accurate, and robust performance in multimodal ﬁnger\\nrecognition.\\nIndex Terms\\nFinger vein recognition, lightweight convolutional neural network, attention mechanism, convolutional block attention module.\\nI. INTRODUCTION\\nWith the development of society, people have higher and higher requirements for identity information security. Traditional\\nidentiﬁcation technology has been difﬁcult to meet people’s needs, so it is necessary to develop biometric technology with\\nhigher security. Finger veins are hidden under the skin of the ﬁngers [1],[2]. Compared with other biometric features, its\\nstructure is complex and not available under visible light. It has high stability, concealment and anti-counterfeiting properties\\nand has a broad application prospect.\\nGenerally speaking, the ﬁnger vein recognition process includes the following four steps: image acquisition, pre-processing,\\nfeature extraction and matching [3]. Among them, feature extraction plays a crucial role. According to the different feature\\nextraction methods, the existing ﬁnger vein feature extraction methods can be roughly divided into two groups: vein pattern-\\nbased methods and local binary-based methods. These methods perform well in most cases.\\nRelative to the above-mentioned custom features, deep features learned from convolutional deep neural networks have been\\nshown to have better generalization and representation [4],[5]. Recent studies have shown that deep convolutional neural\\nnetworks (CNNs) have outstanding performance in the ﬁeld of image understanding and recognition [6],[7], which motivated\\nus to employ deep CNNs for feature learning and selection of ﬁnger vein images [8]. For example, Hong et al. [9] applied a\\npre-trained CNN model of VGG-Net-16 for ﬁnger vein validation. Yin et al. [10] ﬁne-tuned VGG-Net-16 and achieved good\\nrecognition accuracy on two publicly available databases of ﬁnger veins. Besides, there are many studies [11],[12],[13] devoted\\nto improve the results by increasing the depth or width of neural networks, although all of them achieved good results, but\\nthe network is too deep or too wide, which is a great test for the computer’s computational power.\\nZ. Zhang and M. Wang are with School of Mathematics, Southwest Jiaotong University, Chengdu, 610031, China. E-mail: 1065947699@qq.com,\\nwangmw@swjtu.edu.cn.\\nThis work was partially supported by the Fundamental Research Funds for the Central Universities under Grant 2682021ZTPY100, in part by the Science\\nand Technology Support Project of Sichuan Province under Grant 2020YFG0045 and 2020YFG0238.\\nCorresponding author: M. W. Wang.\\narXiv:2202.06673v1  [cs.CV]  14 Feb 2022\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n2\\nIt is known that an important feature of the human visual system is that it does not try to process the whole scene\\nimmediately, but selectively focuses on salient parts in order to better capture the visual structure. Attention can be directed to\\nfocus, and expressivity can be improved by using attentional mechanisms, i.e., focusing on important features and suppressing\\nunnecessary ones. The convolutional block attention module (CBAM) [14] is a simple and effective attention module for\\nfeedforward convolutional neural networks that can attend to important information in channels and spaces separately, which\\nnot only saves parameters and computational power, but also ensures its integration into existing network architectures as a\\nplug-and-play module.\\nMotivated by the success of CNN and CBAM, we propose a novel and effective feature representation of lightweight\\nCNN based on CBAM blocks for the ﬁnger recognition to save computational power. By embedding CBAM in the designed\\nlightweight CNN architecture, the accuracy is improved while ensuring small computational power. Speciﬁcally, the image is\\nfed into a lightweight base CNN architecture, and the initial features of the ﬁnger vein are extracted using the powerful feature\\nrepresentation capability of CNN. Then, CBAM blocks are embedded in the original network to infer the attention mapping\\naccording to two independent dimensions - channel and spatial order, and the attention mapping is multiplied into the input\\nfeature mapping with adaptive feature reﬁnement. Finally, the output features are classiﬁed using the softmax function, and the\\nmethod proposed in this paper has signiﬁcant improvements in two different databases. The main contributions are as follows:\\n(1) To our knowledge, this paper is the ﬁrst to successfully apply lightweight CBAM blocks into CNN for ﬁnger vein\\nrecognition.\\n(2) We combine a convolutional block attention module (CBAM) with a lightweight CNN to simulate visual attention\\nmechanisms and enhance the ﬂow of information in channels and spaces.\\n(3) Our network structure is simple and guarantees the smallest possible computation without sacriﬁcing network performance.\\nExperimental results show that the scheme in this paper has competitive potential in ﬁnger vein recognition systems.\\nThe rest of the paper is organized as follows. Section II is to describe the related work. Section III presents the theoretical\\nbackground of CNN and CBAM. Proposed approach is discussed in Section IV. In Section V, we present experimental results.\\nFinally, Section VI concludes the paper.\\nII. RELATED WORK\\nUndoubtedly, a proper feature extraction method in ﬁnger vein recognition systems can be of great beneﬁt in improving the\\nperformance [15],[16]. How to effectively extract discriminative features remains a major challenge for ﬁnger recognition. The\\nexisting ﬁnger feature extraction methods can be broadly classiﬁed into two categories [17]: traditional recognition methods\\nand deep learning methods.\\nTraditional recognition methods generally include subspace learning-based methods, vein pattern-based, detail point matching-\\nbased, and local feature-based methods.\\n(1) Subspace learning approaches are based on machine learning methods to reduce the dimensionality of global features\\nand ﬁlter noise at the same time, such as principal component analysis (PCA) [18], linear discriminant analysis (LDA)\\n[19] and sparse representation (SR) [20], all of which transform texture images into different subspaces and generate\\nfeature vectors from individual coefﬁcients of the subspace to accomplish texture recognition. Wang et al. [18] combined\\nthe traditional 2D-PCA and 2D-FLD (Fisher linear discriminant) techniques, Wu and Liu [19] implemented ﬁnger vein\\nclassiﬁcation using PCA and LDA, and Xin et al. [20] successfully applied SR to the ﬁnger vein recognition task. These\\nPCA, LDA and SR-based methods can reduce the preprocessing steps and have a smaller space occupation of the feature\\nvector. However, they extract features from a global perspective and do not provide sufﬁcient description of local feature\\ninformation.\\n(2) Vein-based methods segment the vein pattern from the ﬁnger vein image and match it by geometry or topology. Typical\\nmethods include the mean curvature method [21], the maximum curvature (MC) point method [22], the repetitive line\\ntracing method (RLT) [23], and morphological operations combined with the mean Gabor method [1],[24],[25], etc. The\\nRLT [23] method extracts the vein network by calculating the difference between the central pixel value and the pixel value\\nin the corresponding range, which has high time complexity and does not take into account the symmetry and continuity of\\nthe vein pattern. Miura et al. [22] modiﬁed the RLT method by calculating the maximum curvature information during vein\\ntracing. Subsequently, both literature [21] and [26] also used the curvature method to extract ﬁnger vein features. Kumar\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n3\\net al. [1] used Gabor ﬁlter to extract vein patterns. Although Gabor ﬁlter is powerful in image texture analysis, it causes\\ninformation loss for low quality vein images. Recently, Yang et al. [27] proposed a ﬁnger vein code indexing method and\\ncombined it with ﬁnger vein pattern matching method into an integrated framework. And the ﬁnal experimental results\\nshow that the integrated framework does improve the recognition accuracy. However, the method, like other vein-based\\nmethods, requires segmentation of ﬁnger vein images, which is greatly affected by the quality of ﬁnger images.\\n(3) Detail point matching-based methods extract stable structures such as intersections and endpoints of vessels as feature\\npoints to calculate the similarity of two matched images, including the improved Hausdorff distance matching method\\n[28], the singular value decomposition-based detail matching method [29]. Besides, the scale-invariant feature transform\\n(SIFT)-based method [30],[31] can automatically extract feature points from ﬁnger vein images for matching, and is\\nusually regarded as a method based on minutiae points. Recently, Meng et al. [32] combined detail point matching with\\nthe traditional region of interest (ROI)-based method to select details of reasonable neighborhoods for matching, which\\navoids mismatch to some extent and is more stable in detail matching. Use of above features for vein matching usually\\nshows poor performance for low-quality images due to the presence of spurious minutiae features. In addition, employing\\nminutiae features are sensitive to changes in ﬁnger pose [33].\\n(4) Local feature-based methods, such as local binary patterns (LBP) [34],[35], local line binary patterns (LLBP) [36], local line\\ndirectional patterns (LLDP) [37], and local graph structures (LGS) [38],[39], have been widely used in ﬁnger biometrics.\\nFor example, Dong et al. [39] proposed a multi-directional weighted symmetric local graph structure (MoW-SLGS)\\noperator for ﬁnger vein recognition. Unfortunately, the operator assigns different weights to symmetric pixels, which leads\\nto an unbalanced feature representation between the left and right sides. Liu et al. [40] stated a multi-directional local\\nline binary pattern (PLLBP) method that makes full use of the discriminative power of the LLBP histogram in different\\ndirections. And the method can be used to extract vein line patterns in any direction. Recently, Al-Nima et al. [41] proposed\\nthe multi-scale Sobel angular local binary pattern (MSALBP) for the feature extraction algorithm of ﬁnger texture images.\\nThis method combines the Sobel operator with a multi-scale local binary pattern, which is computed statistically for each\\nimage block to form a texture vector as an input to an artiﬁcial neural network (ANN). In general, although these local\\nfeature-based methods can better capture the local information of the image, the methods only consider the relationship\\nbetween the target pixel and its surrounding pixels, ignoring the hidden relationship between surrounding neighboring\\npixels. In other words, the uniqueness of ﬁnger images will not be effectively expressed by relying only on hand-crafted\\nfeatures.\\nAlthough the traditional recognition method has achieved good results [42], but its recognition process is more complicated,\\nand the extracted features are the shallow features of the image, which are easily affected by the image quality. Deep learning\\nmethods are not easily affected by image quality, have powerful image processing without any prior knowledge, and perform\\nwell in noisy image processing and adaptive learning of feature representations. In the ﬁeld of ﬁnger vein recognition, deep\\nlearning method has been successfully applied in recent years [43]. For example, Huang et al. [44] designed a new ﬁnger vein\\nveriﬁcation method with deep convolutional neural network, which works well for ﬁnger vein pattern matching. In [45], Das et\\nal. developed a CNN-based ﬁnger-vein identiﬁcation system, which performed an effective identiﬁcation, but did not consider\\nfactors such as training time and model size. Beisides, Ahmad Radzi et al. [46] used a four-layer CNN to recognize ﬁnger\\nveins. The recognition accuracy reached 100%, but the results were obtained in their own database, which is not universal. And\\nLi et al. [47] used improved GCNs to recognize ﬁnger veins and obtained better recognition accuracy, but image preprocessing\\nwas needed to construct a weighted map of ﬁnger veins. He et al. [48] enhanced the feature extraction ability of the network\\nby adding convolution layer, which improved the recognition accuracy with less training samples, and also increased the\\ncomplexity of the network.\\nIII. METHODOLOGY\\nCNN has been successfully applied in the ﬁeld of computer vision, demonstrating its powerful ability to represent features.\\nIt uses multiple ﬁlters that share different parameters to extract image features. Recently, the attention mechanism has received\\nincreasing attention for their focus on important details and their ability to better capture visual structure. In our study, a CNN\\nwith a convolutional attention module (CBAM) is proposed. It uses a basic CNN to extract the overall features of all input data.\\nThen, the CBAM processes the feature map and adaptively assigns weights to the channel dimensions and spatial dimensions.\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n4\\nConv\\nReLU\\nBN\\nMaxPool\\nConv 1\\nConv 2\\nBasic CNN\\nFlatten\\nDatabase\\nCAM\\nSAM\\nCBAM\\nSoftmax\\nFig. 1: General framework of the network for the proposed approach\\nBoth the basic CNN architecture and CBAM are indispensable because the basic network architecture focuses on the global\\ninformation, while CBAM highlights its features. These two parts complement each other and complete the neural network.\\nFig. 1 shows the complete framework of the proposed network, and we explain the components of the framework (i.e., the\\nbasic CNN, CBAM and output layer) in detail below.\\nA. The Basic CNN\\nIn our work, considering the problem of overﬁtting when training with small data in a too deep network, we designed\\na lightweight CNN with the network parameters set as shown in Table I. The lightweight CNN consists of a total of two\\nconvolutional modules, each with a convolutional layer, a normalization layer, an activation function layer, and a pooling layer.\\nTABLE I: Details of the proposed network framework\\nLayer Type\\nNumber of\\nﬁlters\\nSize\\nOutput size\\nInput\\n−\\n−\\n1 × 81 × 333\\nConv 1\\n16\\n5 × 5\\n16 × 81 × 333\\nBN\\n1\\n−\\n16 × 81 × 333\\nReLU\\n1\\n−\\n16 × 81 × 333\\nPool 1\\n1\\n3 × 3\\n16 × 27 × 111\\nConv 2\\n32\\n5 × 5\\n32 × 27 × 111\\nBN\\n1\\n−\\n32 × 27 × 111\\nReLU\\n1\\n−\\n32 × 27 × 111\\nPool 2\\n1\\n3 × 3\\n32 × 9 × 37\\nFor convolutional layer, features are extracted by performing a two-dimensional convolution of the input graph and the\\nconvolutional kernel, which can be speciﬁcally expressed by Eq. (1) as:\\ny1 = σ(b +\\nk−1\\nX\\nl=0\\nk−1\\nX\\nm=0\\nwl,mai+l,j+m)\\n(1)\\nwhere σ represents the activation function, such as sigmoid function, etc.; b is the offset value; w is the k × k size shared\\nweight matrix. We use matrix a to represent the input layer neurons, and ax,y to denote the neuron in the (x + 1)-th row and\\n(y + 1)-st column (note that the subscripts here are counted from 0 by default, a0,0 represents the neuron in the ﬁrst row and\\nﬁrst column), so we get Eq. (1) by adding the offset value after the matrix w linear mapping.\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n5\\nThe batch normalization (BN) layer normalizes the feature maps generated by the convolutional layers and sends these feature\\nmaps to activation function to speed up the training process. As for the activation function, rectiﬁed linear units (ReLU) were\\nchosen in Eq. (1). ReLU is introduced as the activation function of the hidden layer instead of the traditional sigmoid unit\\nbecause ReLU is better at capturing patterns in natural images and improves the ability of the neural network to solve the\\nimage denoising problem.\\nThe pooling layer compresses the information in the original feature layer so that the input representation can be more\\ncompact. In general, there are two operations: maximum pooling and average pooling. Max-pooling simply reduces the\\ndimensionality of the data by taking only the largest data, while average-pooling works in a similar way by taking the\\naverage of the inputs instead of the maximum. Based on the conceptual differences between these two methods, max-pooling\\nis sensitive to the texture information of the image, while the average pooling method retains more background information\\nof the image. Therefore, max-pooling is more beneﬁcial for extracting the feature information of the image. In this paper,\\nmax-pooling is used after calculating the ReLU output.\\nB. Convolutional Block Attention Module (CBAM)\\nThe above basic CNN network transforms the image data X into a feature map F as the input to CBAM. The CBAM\\ncontains two independent sub-modules, channel attention module (CAM) and spatial attention module (SAM), which can save\\nparameters and computational power by performing attention mechanism on channel and space respectively, as shown in Fig.\\n2.\\nCAM\\nSAM\\nFeature\\nRefined feature\\n\\uf0c4\\n\\uf0c4\\nFig. 2: Convolutional block attention module\\nCBAM inferred the attention maps one at a time along two independent dimensions (channel and space) and then multiplied\\nthe attention maps by the input feature maps for adaptive feature reﬁnement. The process of the input feature map F being\\nprocessed in CBAM can be summarized as:\\nF′ =Mc(F) ⊗F\\nF′′ =Ms(F′) ⊗F′\\n(2)\\nwhere ⊗denotes element multiplication, F′ denotes the result of multiplying the feature map with the channel attention map,\\nand F′′ is the ﬁnal reﬁned output.\\nBelow, we will detail how the two separate dimensions, CAM and SAM, work.\\n1) CAM: To accomplish feature extraction and reduce data loss, the channel attention module compresses the feature maps\\non the spatial dimension using the global average pooling layer and the global maximum pooling layer. Fig. 3 shows the\\nchannel attention module. The global average pooling layer obtains the overall information, while the global maximum pooling\\nlayer obtains the feature variance information. The combination of these two layers is better than any layer.\\nFeature\\nAvgpool\\nMaxpool\\nShared MLP\\nChannel Attention\\nFig. 3: Channel attention module\\nThe compressed FC\\navg and FC\\nmax descriptors are then sent to a shared network to generate our channel attention map\\nMc ∈RC×1×1. The shared network consists of a multi-layer perceptron (MLP) with a hidden layer. To reduce the parameter\\noverhead, the hidden activation size is set to RC/r×1×1, where r is the reduction rate. After the shared network is applied to\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n6\\neach descriptor, we use element summation to merge the output feature vectors. In short, the channel attention is calculated\\nas Eq. (3):\\nMc(F) = σ(MLP(AvgPool(F)) + MLP(MaxPool(F)))\\n= σ(W1(W0(FC\\navg)) + W1(W0(FC\\nmax)))\\n(3)\\nwhere σ denotes the sigmoid function, W0 ∈RC/r×C, and W1 ∈RC×C/r. Note that the MLP weights W0 and W1 are\\nshared between the two inputs, and the ReLU activation function is followed by W0.\\n2) SAM: The feature map F′ output from CAM is used as the input feature map of this module. First, we perform global\\nmax pooling and global average pooling based on channels to obtain two H × W × 1 feature maps, and then we do concat\\noperation (channel splicing) on these two feature maps based on channels. Next, after a 7 × 7 convolution (7 × 7 is better than\\n3×3 ) operation, it is reduced to one channel, i.e., H ×W ×1. After that, the spatial attention feature is generated by sigmoid\\nfunction. Finally, this feature is multiplied with the input feature of the module to get the ﬁnal generated feature. Speciﬁcally,\\nas shown in Fig. 4, the calculation process is as follows Eq. (4):\\nMs(F) = σ(f 7×7([AvgPool(F); MaxPool(F)]))\\n= σ(f 7×7[Fs\\navg; Fs\\nmax])\\n(4)\\nwhere σ is a sigmoid function and f 7×7 denotes a convolution operation with a ﬁlter size of 7 × 7.\\nChannel refined \\nfeature\\n[MaxPool,AvgPool]\\nSpatial attention\\nFig. 4: Spatial attention module\\nCAM utilizes the inter-channel relationships of features to generate a channel attention map. As each channel of a feature\\nmap is considered as a feature detector, channel attention focuses on ‘what’ is meaningful given an input. SAM uses spatial\\nrelationships between features to generate spatial attention maps. Unlike CAM, spatial attention focuses on the ‘where’ is an\\ninformative part, which is complementary to CAM.\\nC. Output layer\\nThe feature map F is reﬁned into F′′ after going through the CAM and SAM modules. F′′ is the ﬁnal feature obtained\\nfrom our proposed network framework. Additional dense layers are needed in our network to recreate the data structure used\\nfor classiﬁcation. Softmax is used as the ﬁnal activation because it is a sigmoid equivalent, however with traditionally better\\nresults and a normalized output essential for classiﬁcation problems with multiple classes.\\nThe deﬁnition of the Softmax function (with the i-th node output as an example) is given below:\\nSoftmax(zi) =\\nezi\\nPC\\nc=1 ezc\\n(5)\\nwhere zi is the output value of the i-th node and C is the number of output nodes, i.e., the number of categories of the\\nclassiﬁcation. The Softmax function allows converting the output values of multiple categories into a probability distribution\\nranging from [0, 1] and summing to 1. In classiﬁcation problems, we want the model to assign probabilities close to 1 to\\nthe correct category and 0 to the others, which is difﬁcult to achieve if we use linear normalization methods. Softmax has a\\n‘two-step’ strategy of discretization followed by normalization, so it has a signiﬁcant advantage in classiﬁcation problems.\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n7\\nIV. EXPERIMENTAL RESULTS AND ANALYSIS\\nA. Database\\nIn this section, we will discuss our experiments to conﬁrm the effectiveness of our proposed method and show the observed\\nresults. In our experiment, we used vein images from two public ﬁnger vein databases: the Hong Kong Polytechnic University\\ndatabase (HKPU) [1] and the University Sains Malaysia database (USM) [49]. The characteristics of the databases used in our\\nexperiment are shown in Table II.The details of these databases are given below:\\nTABLE II: The details of databases\\nDatabase\\nFinger\\nnumber\\nImage number\\nper ﬁnger\\nSize of raw\\nimage (pixels)\\nROI image\\nHKPU\\n312\\n6\\n513 × 256\\nExtract using\\nthe method of [1]\\nUSM\\n492\\n12\\n640 × 480\\nDatabase includes\\nROI images\\n(1) HKPU Database: The images of the database were collected from 156 volunteers. Every volunteer only collected the\\nindex ﬁnger and middle ﬁnger of his left hand, totaling 312 ﬁngers. Each ﬁnger collected 6 images. Each of ﬁrst 210 ﬁngers\\nhas 12 images, captured in two separate sessions, and others each has 6 images, captured in one session. All of the images\\nare 8-bit gray level BMP ﬁles with a resolution of 513 × 256 pixels. In our experiments, we use 6 samples each ﬁngers and\\na total of 312 ﬁnger types were obtained. We used the method in the literature [1] to perform ROI operations on the original\\nimages and obtained an image size of 333 × 81.\\n(2) USM Database: The database of Universiti Sains Malaysia (USM) consists of 492 ﬁngers, and every ﬁnger provided 12\\nimages. The spatial resolution of the captured ﬁnger images were 640 × 480. This database also provides the extracted ROI\\nimages with a size of 300 × 100 for ﬁnger vein recognition using their proposed algorithm described in [49].\\nB. Parameter Settings and Experiments\\nIn our method, we adopt accuracy as a measure of model performance, the formula is as follows:\\nAccuracy = Total number of correct identiﬁcations\\nTotal number of samples\\n(6)\\nWe implement the proposed method using Pytorch framework and conduct the experiments on a common desktop PC with\\ni7 3.60 GHz CPU. In the proposed model, Adam’s method is chosen as the optimizer considering the performance and training\\ntime because of its good performance in improving traditional gradient descent and facilitating hyperparameter dynamic tuning.\\nThe learning rate of the model was set to 0.0001 and the batch size was 36. In the next experiments, the number of epochs was\\ndeﬁned as 20. For training purposes, 70% of the ﬁnger vein images were randomly selected as the original training samples and\\nthe remaining 30% were considered as the test samples. As a rough estimate because the proposed networks are lightweight\\nand easily trained, it takes less than half an hour to train.\\nC. Experiments\\nIn this subsection, we will ﬁrst show our training process and the results obtained. Fig. 5 shows our loss curve for training\\non two publicly available data, and Fig. 6 shows the accuracy curve. The higher number in the number of epochs usually\\nallows the network to be well trained so that the weights of the different layers can be updated accurately. Only 20 epochs\\nare considered for all our experiments, but we can still observe encouraging results Fig. 6 that our validation set accuracy can\\nreach 100%, which shows the effectiveness of our method.\\nIn addition, both the proposed network and the basic network were tested on the HKPU database. Table III shows the\\nrecognition accuracy of the single network (basic network) and our method on HKPU. The basic network uses CNN as the\\nonly method to extract the overall features of ﬁnger veins. The accuracy of this method is 98.11%. In comparison, the proposed\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n8\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\nepoch\\n0.000000\\n0.015050\\n0.030100\\n0.045150\\n0.060200\\n0.075250\\n0.090300\\n0.105350\\n0.120400\\n0.135450\\n0.150500\\n0.165550\\nLoss\\nTrain loss\\nVal loss\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\nepoch\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nacc\\nTrain acc\\nVal acc\\n(a)\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\nepoch\\n0.000000\\n0.015050\\n0.030100\\n0.045150\\n0.060200\\n0.075250\\n0.090300\\n0.105350\\n0.120400\\n0.135450\\n0.150500\\n0.165550\\nLoss\\nTrain loss\\nVal loss\\n0\\n2\\n4\\n6\\n8\\n1\\nep\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nacc\\n(b)\\nFig. 5: Loss curves on two public databases: (a) HKPU, (b) USM.\\n10\\n12\\n14\\n16\\n18\\n20\\noch\\nTrain loss\\nVal loss\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\nepoch\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nacc\\nTrain acc\\nVal acc\\n(a)\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\nepoch\\n0.000000\\n0.015050\\n0.030100\\n0.045150\\n0.060200\\n0.075250\\n0.090300\\n0.105350\\n0.120400\\n0.135450\\n0.150500\\n0.165550\\nLoss\\nTrain loss\\nVal loss\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\nepoch\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nacc\\nTrain acc\\nVal acc\\n(b)\\nFig. 6: Accuracy curves on two public databases: (a) HKPU, (b) USM.\\nnetwork achieved 100% recognition rate. The time in the table is the total time spent for training 20 epochs plus validation,\\nand the difference in time between the two methods is 13 seconds, indicating the lightweight nature of the proposed model.\\nOverall, the network with the convolutional block attention module in the basic CNN framework improves the recognition\\nperformance.\\nTABLE III: Comparison results with basic CNN\\nArchitecture\\nAccuracy\\nTime(Training + veriﬁcation)\\nBasic CNN\\n98.11%\\n670s\\nOur approach\\n100%\\n683s\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n9\\nD. Comparison with the Existing Methods\\nFinger vein recognition technology has been developed for more than a decade and several advanced algorithms have been\\nproposed. In this experiment, we have divided the existing methods into traditional methods and deep learning methods and\\ncompared them with our proposed method.\\nTable. IV shows the results of the comparison with some classical traditional ﬁnger vein recognition algorithms. The results\\nshow that our scheme signiﬁcantly outperforms some algorithms, e.g., literature [1],[18],[20],[22],[23],[34], while there is almost\\nno difference in recognition accuracy compared to scheme [15],[16], but our method does not require additional processing of\\nﬁnger vein images, which greatly reduces the time cost.\\nTABLE IV: Comparison results with traditional methods\\nDatabase\\nMethod\\nAccuracy\\nHKPU\\nLBP [34]\\n83%\\nGabor [1]\\n90.08%\\nMC [22]\\n85.24%\\nRLT [23]\\n78.28%\\nWeighted vein indexing [15]\\n99.68%\\nThis Paper\\n100%\\nUSM\\nWLBP [16]\\n93.8%\\nPCA [18]\\n94.7%\\nSR [20]\\n95.1%\\nWeighted vein indexing [15]\\n99.93%\\nCPBFL-BCL [2]\\n99.98%\\nThis Paper\\n100%\\nTABLE V: Comparison results with neural network methods\\nMethod\\nAccuracy\\nNumber of\\nconvolution\\nNumber of\\nﬁlters\\nKernel size\\nHKPU\\nUSM\\nCNN\\nHong et al. [9]\\n−\\n95.83%\\n13\\n64, 128,\\n256, 512\\n3 × 3\\nCNN with\\noriginal images [45]\\n95.3%\\n97.53%\\n5\\n153, 512,\\n768, 1024\\n5 × 5, 4 × 15,\\n1 × 1\\nCNN with CLAHE\\nenhanced images [45]\\n94.37%\\n97.05%\\nImproved CNN [8]\\n−\\n98%\\n3\\n8, 16, 32\\n5 × 5, 3 × 3, 3 × 3\\nTwo-stream\\nCNN [13]\\n−\\n95.45%\\n3, 3\\nNet1: 20, 50, 200\\nNet2: 20, 50, 200\\nNet1: 9 × 9, 10 × 26\\nNet2: 7 × 7, 4 × 7\\nBasic CNN\\n98.11%\\n−\\n2\\n16, 32\\n5 × 5\\nOther NN\\nDeep Belief\\nNetwork CGN\\n−\\n97.4%\\n−\\n−\\n−\\nAlexNet\\n−\\n92.28%\\nResNet-18\\n−\\n96.04%\\nCAE+CNN [5]\\n−\\n99.49%\\nOur method\\n100%\\n100%\\n2\\n16, 32\\n5 × 5\\nCompared with classical networks, such as AlexNet ResNet-18, Basic CNN, the reason for the high accuracy of our scheme\\nis that the weights are assigned adaptively through the attention mechanism, which highlights the important detail information\\nof the image and the extracted features are more distinguishable. The network layer setup in literature [8] is similar to our\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n10\\nmodel structure, but it needs to extract curvature by Gaussian template and use certain methods to extract feature images as\\ninput to CNN, which is time costly and computationally not small. Other literature, such as [9],[13],[45], shows from the\\nnetwork structures listed in Table V that the structures of these schemes are not as simple as those of the proposed scheme.\\nCombining Table III and Table V, it is obvious that our scheme is less computationally intensive and takes less time to train.\\nFrom the experimental data, the ideas and methods of this paper are completely correct and effective.\\nV. CONCLUSION\\nAccording to the characteristics of ﬁnger veins, this paper adds CBAM to the basic CNN framework for ﬁnger vein\\nrecognition.The CBAM block further reﬁnes the features extracted by CNN to make the features more distinguishable. Compared\\nwith the classical network model, our model extracts features that better reﬂect image information with higher accuracy, which\\nhas obvious advantages. In the model, we use a simple network structure with lightweight features, less computation and\\nshorter training time. To make our model more practical, in future work, we will consider larger datasets, i.e., in fusing\\nmultiple publicly available datasets into one dataset for learning, which makes the model more robust due to the difference in\\nimage quality.\\nREFERENCES\\n[1] A. Kumar and Y. Zhou, “Human identiﬁcation using ﬁnger images,” IEEE Trans. Image Process, vol. 21, no. 4, pp. 2228–2244, 2012.\\n[2] H. Liu, G. Yang and Y. Yin, “Category-preserving binary feature learning and binary codebook learning for ﬁnger vein recognition,” Int. J. Mach. Learn.\\nCybern., vol. 11, no. 11, pp. 2573–2586, 2020.\\n[3] J. D. Wu and S. H. Ye, “Driver identiﬁcation using ﬁnger-vein patterns with Radon transform and neural network,” Expert Syst. Appl., vol. 36, no. 3,\\npp. 5793–5799, 2009.\\n[4] J. D. Wu and C. T. Liu, “Finger vein pattern identiﬁcation using SVM and neural network technique,” Expert Syst. Appl., vol. 38, no. 11, pp. 14284–14289,\\n2011.\\n[5] B. Hou and R. Yan, “Convolutional auto-encoder based deep feature learning for ﬁnger-vein veriﬁcation,” in Proc. IEEE Int. Symp. Med. Meas. Appl,\\n2018, pp. 1-5.\\n[6] C. Xie and A. Kumar, “Finger vein identiﬁcation using convolutional neural network and supervised discrete hashing,” Pattern Recognit. Lett., vol. 119,\\npp. 148–156, 2019.\\n[7] Z. M. Fang and Z. M. Lu, “Deep belief network based ﬁnger vein recognition using histograms of uniform local binary patterns of curvature gray\\nimages,” Int. J. Innov. Comput. Inf. Control., vol. 15, no. 5, pp. 1701–1715, 2019.\\n[8] J. Y. Zhao, J. Gong, S. T. Ma and Z. Ming Lu2, “Curvature Gray feature decomposition based ﬁnger vein recognition with an improved convolutional\\nneural network,” Int. J. Innov. Comput. Inf. Control., 16, no. 1, pp. 77–90, 2020.\\n[9] H. G. Hong, M. B. Lee and K. R. Park, “Convolutional neural network-based ﬁnger-vein recognition using NIR image sensors,” Sensors, vol. 17, no.\\n6, pp. 1297. 2017.\\n[10] Y. Yin, L. Liu and X. Sun, “ SDUMLA-HMT: a multimodal biometric database,” in Proc. The International Conference on Advanced Computer Control,\\n2011, pp. 260–268.\\n[11] F. J. Xu, V. N. Boddeti and M. Savvide, “ Local binary convolutional neural networks,” in Proc. Int. Conf. Computer Vision Pattern Recognition, pp.\\n1–10, 2017.\\n[12] S. Y. Li, B Zhang, S. P. Zhao and J. F. Yang, “Local discriminant coding based convolutional feature representation for multimodal ﬁnger recognition,”\\nInf. Sci., vol. 547, pp. 1170–1181, 2021.\\n[13] Y. Fang, Q. Wu and W. Kang, “A novel ﬁnger vein veriﬁcation system based on two-stream convolutional network learning,” Neurocomputing, vol. 290,\\npp.100–107, 2018.\\n[14] S. Woo, J. Park, J.Y. Lee and I. S. Kweon, “CBAM: convolutional block attention module,” Computer Vision-ECCV 2018, vol 11211, pp. 3–19, 2018.\\n[15] L. Yang, G. Yang, X. Xi, K. Su, Q. Chen and Y. Yin, “Finger vein code: from indexing to matching,” IEEE Trans Inf Forensics Secur, vol. 14, no. 5,\\npp. 1210–1223, 2018.\\n[16] H. C. Lee, B. J. Kang, E. C. Lee and K. J. Park, “Finger vein recognition using weighted local binary pattern code based on a support vector machine,”\\nJ. Zhejiang Univ. Sc. A, vol. 11, no. 7, pp. 514–524, 2010.\\n[17] K. X. Wang, G. H. Chen and H. J. Chu, “Finger vein recognition based on multi-receptive ﬁeld bilinear convolutional neural network,” IEEE Singal\\nProces. Lett., vol. 28, pp.1590–1594, 2021.\\n[18] J. Wang, H. Li, G. Wang, M. Li and D. Li, “Vein recognition based on 2D 2FPCA,” Internat. J. Signal Proces., Image Proces. and Pattern Recogn.,\\nvol. 6, no. 4, pp. 323–332, 2013.\\n[19] J. Wu and C. Liu, “Finger vein pattern identiﬁcation using SVM and neural network technique,” Expert Syst. Appl., vol. 38, no. 11, pp. 14284–14289,\\n2011.\\n[20] Y. Xin, Z. Liu, H. Zhang and H. Zhang, “Finger vein veriﬁcation system based on sparse representation,” Applied Optics, vol. 51, no. 25, pp. 6252–6258,\\n2012.\\n[21] W. Song, T. Kim and H. C. Kim, “A ﬁnger-vein veriﬁcation system using mean curvature,” Pattern Recognit. Lett., vol. 32, no. 8, pp. 1541–1547, 2011.\\n[22] N. Miura, A. Nagasaka and T. Miyatake, “Extraction of ﬁnger-vein patterns using maximum curvature points in image proﬁles,” IEICE Trans. Inf. Syst.,\\nvol. E90-D, no. 8, pp. 1185–1194, 2007.\\n[23] N. Miura, A. Nagasaka and T. Miyatake, “Feature extraction of ﬁnger-vein patterns based on repeated line tracking and its application to personal\\nidentiﬁcation,” Mach. Vis. Appl., vol. 15, no. 4, pp. 194–203, 2004.\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n11\\n[24] J. Yang and X. Zhang, “Feature-level fusion of ﬁngerprint and ﬁnger-vein for personal identiﬁcation,” Pattern Recognit. Lett., vol. 33, no. 5, pp. 623–628,\\n2012.\\n[25] W. Y. Han and J. C. Lee, “Palm vein recognition using adaptive Gabor ﬁlter,” Expert Syst. Appl., vol. 39, no. 18, pp. 13225-13234, 2012.\\n[26] H. Qin, X. He, X. Yao and H. Li, “Finger-vein veriﬁcation based on the curvature in radon space,” Expert Syst. Appl, vol. 82, pp. 151–161, 2017.\\n[27] L. Yang, G. Yang, X. Xi, K. Su, Q. Chen and L. Yin, “Finger vein code: from indexing to matching,” IEEE Trans. Inf. Forensics Secur., vol. 14, no. 5,\\npp. 1210–1223, 2019.\\n[28] C. Yu, H. Qin, L. Zhang, Y. Cui, “Finger-vein image recognition combining modiﬁed hausdorff distance with minutiae feature matching,” J. Biomed.\\nSci. Engineer., vol. 1, no. 3, pp. 280–289, 2009.\\n[29] F. Liu, G. Yang, Y. Yin and S. Wang, “Singular value decomposition based minutiae matching method for ﬁnger vein recognition,” Neurocomputing,\\nvol. 145, pp. 75–89, 2014.\\n[30] H. Kim, E. Lee and G. Yoon, “Illumination normalization for SIFT based ﬁnger vein authentication,” in Proc. Int. Symp. Vis. Comput., pp. 21–30, 2012.\\n[31] S. Pang, Y. Yin, G. Yang and Y. Li, “Rotation invatiant ﬁnger vein recognition,” Chinese Conference on Biometric Recognition, (CCBR), pp. 151–156,\\n2012.\\n[32] X. Meng, J. Meng, X. Xi, Q. Zhang, and Y. Zhang, “Finger vein recognition based on zone-based minutia matching,” Neurocomputing, vol. 423, pp.\\n110–123, 2021.\\n[33] J. Peng, A. El-Latif, Q. Li and X. Niu, “Multimodal biometric authentication based on score level fusion of ﬁnger biometrics,” International Journal\\nfor Light and Electron Opticsz, vol. 125, no. 23, pp. 6891–6897, 2014.\\n[34] L. Yang, G. Yang, Y. Yin and X. Xi, “Exploring soft biometric trait with ﬁnger vein recognition,” Neurocomputing, vol. 135, pp. 218–228, 2014.\\n[35] C. Liu and Y.-H. Kim, “An efﬁcient ﬁnger-vein extraction algorithm based on random forest regression with efﬁcient local binary patterns,” In Proc.\\nInt. Conf. Image Process, USA, pp. 3141–3145, 2016.\\n[36] B. A. Rosdi, C. W. Shing and S. A. Suandi, “Finger vein recognition using local line binary pattern,” Sensors, vol. 11, no. 12, pp. 11357–11371, 2011.\\n[37] Y. T. Luo, L. Y. Zhao and B. Zhang, “Local line directional pattern for palmprint recognition,” Pattern Recogn., vol. 50, 26–44, 2016.\\n[38] M. F. Abdullah, M. S. Sayeed and K. S. Muthu, “Face recognition with Symmetric Local Graph Structure (SLGS),” Expert Syst. Appl, vol. 41, no. 14,\\npp. 6131–6137, 2014.\\n[39] S. Dong, J. C. Yang and Y. Chen, “Finger vein recognition based on multi-orientation weighted symmetric local graph structure,” Ksii Trans. Internet\\nInf. Syst, vol. 9, no. 10, pp. 4126–4142, 2015.\\n[40] Y. Lu, S. J. Xie, S. Yoon and D. S. Park, “Finger vein identiﬁcation using poly directional local line binary pattern,” In Proc. International Conference\\non ICT Convergence, pp. 61–65, 2013.\\n[41] A, Nima, R. Abdullaha, M. A. Kaltakchi, M. Dlay and L. Woo, “Finger texture biometric veriﬁcation exploiting multi-scale sobel angles local binary\\npattern features and score-based fusion,” Digital Signal Proces., vol. 70, pp. 178–189, 2017.\\n[42] S. Li, B. Zhang, L. Fei and S. Zhao, “Joint discriminative feature learning for multimodal ﬁnger recognition,” Pattern Recognit., vol. 111. no. 107704,\\n2021.\\n[43] F.J. Xu, V. N. Boddeti and M. Savvide, “Local binary convolutional neural networks,” in Proc. Int. Conf. Computer Vision Pattern Recognition, pp.\\n1–10, 2017.\\n[44] H. Huang, S. Liu, H. Zheng, L. Ni, Y. Zhang and W. Li, “DeepVein: Novel ﬁnger vein veriﬁcation methods based on deep convolutional neural networks,”\\nin Proc. IEEE Int. Conf. Identity, Secur. Behav. Anal, pp. 1–8, 2017.\\n[45] R. Das, E. Piciucco, E. Maiorana and P. Campisi, “Convolutional neural network for ﬁnger-vein-based biometric identiﬁcation,” IEEE Trans. Inf. Forensics\\nSecur., vol. 14, no. 2, pp. 360–373, 2019.\\n[46] S. A. Radzi, M. Khalil-Hani and R. Bakhteri, “Finger-vein biometric identiﬁcation using convolutional neural network,” Turkish J. Elect. Eng. Comput.\\nSci, vol. 24, pp. 1863–1878, 2016.\\n[47] R. Li, Z. G. Su, H. G. Zhang and J. F. Yang, “Application of improved GCNs in feature representation of ﬁnger-vein,” J. Signal Process., vol. 36, pp.\\n550–561, 2020.\\n[48] X. He and X. Chen, “Finger vein recognition based on improved convolution neural network,” Comput. Eng. Design, vol. 40, pp. 562–566, 2019.\\n[49] M. S. M. Asaari, S. A. Suandi and B. A. Rosdi, “Fusion of band limited phase only correlation and width centroid contour distance for ﬁnger based\\nbiometrics,” Expert Syst. Appl., vol. 41, no. 7, pp. 3367-3382, 2014.\\n', metadata={'Published': '2022-02-14', 'Title': 'Convolutional Neural Network with Convolutional Block Attention Module for Finger Vein Recognition', 'Authors': 'Zhongxia Zhang, Mingwen Wang', 'Summary': 'Convolutional neural networks have become a popular research in the field of\\nfinger vein recognition because of their powerful image feature representation.\\nHowever, most researchers focus on improving the performance of the network by\\nincreasing the CNN depth and width, which often requires high computational\\neffort. Moreover, we can notice that not only the importance of pixels in\\ndifferent channels is different, but also the importance of pixels in different\\npositions of the same channel is different. To reduce the computational effort\\nand to take into account the different importance of pixels, we propose a\\nlightweight convolutional neural network with a convolutional block attention\\nmodule (CBAM) for finger vein recognition, which can achieve a more accurate\\ncapture of visual structures through an attention mechanism. First, image\\nsequences are fed into a lightweight convolutional neural network we designed\\nto improve visual features. Afterwards, it learns to assign feature weights in\\nan adaptive manner with the help of a convolutional block attention module. The\\nexperiments are carried out on two publicly available databases and the results\\ndemonstrate that the proposed method achieves a stable, highly accurate, and\\nrobust performance in multimodal finger recognition.'}), Document(page_content='A Novel Approach to Chest X-ray Lung Segmentation Using U-net \\nand Modified Convolutional Block Attention Module \\nMohammad Ali Labbaf Khaniki*, Mohammad Manthouri  \\n*Faculty of Electrical Engineering, K.N. Toosi University of Technology, Tehran, Iran \\n*mohamad95labafkh@gmail.com \\nAbstract:  \\nLung segmentation in chest X-ray images is of paramount importance as it plays a crucial role in \\nthe diagnosis and treatment of various lung diseases. This paper presents a novel approach for lung \\nsegmentation in chest X-ray images by integrating U-net with attention mechanisms. The proposed \\nmethod enhances the U-net architecture by incorporating a Convolutional Block Attention Module \\n(CBAM), which unifies three distinct attention mechanisms: channel attention, spatial attention, \\nand pixel attention. The channel attention mechanism enables the model to concentrate on the most \\ninformative features across various channels. The spatial attention mechanism enhances the \\nmodel\\'s precision in localization by focusing on significant spatial locations. Lastly, the pixel \\nattention mechanism empowers the model to focus on individual pixels, further refining the \\nmodel\\'s focus and thereby improving the accuracy of segmentation. The adoption of the proposed \\nCBAM in conjunction with the U-net architecture marks a significant advancement in the field of \\nmedical imaging, with potential implications for improving diagnostic precision and patient \\noutcomes. The efficacy of this method is validated against contemporary state-of-the-art \\ntechniques, showcasing its superiority in segmentation performance. \\nKeywords. Chest X-rays, Segmentation, Deep Learning, Attention Mechanism, Convolutional \\nBlock Attention Module. \\n1) Introduction \\nThe analysis of chest X-ray images is a multifaceted and labor-intensive process, frequently \\nrequiring the detection of several irregularities at once. This procedure is usually carried out by \\nradiologists manually, which can put a substantial burden on healthcare resources. The intricate \\nnature of the thoracic background in chest X-ray images and the subjective nature of the \\ninterpretation can lead to bias and discrepancies in the diagnostic outcomes [1]. Additionally, the \\nquality and resolution of the images, coupled with challenges related to data, can further \\ncomplicate the interpretation process. Computer-aided detection (CAD) are technological systems \\nthat provide support to doctors in interpreting medical images [2]. They process digital images or \\nvideos, identify typical patterns, and highlight areas of interest, such as potential diseases, to aid \\nin decision-making. CAD systems are a blend of artificial intelligence and computer vision \\ntechnologies, combined with radiological and pathology image processing [3]. They are commonly \\nused for tasks like tumor detection. For example, many hospitals employ CAD for routine medical \\nscreenings in mammography (for breast cancer detection), colonoscopy for polyp detection, and \\nlung cancer detection. Within CAD systems, segmentation is a key process that precisely \\ndistinguishes and separates regions of interest, like tumors or other irregularities, from the rest of \\nthe healthy tissue. This significantly improves the accuracy of further analyses, such as \\ndetermining the size of a tumor or evaluating the progression of a disease [4]. \\nMachine Learning (ML), a branch of Artificial Intelligence (AI), equips computers with the \\nability to learn from data without the need for explicit programming [5] and [6]. It has the capacity \\nto self-adjust and enhance its performance over time with little to no human intervention. ML \\nalgorithms are capable of recognizing patterns, multi-objective algorithms [7], making forecasts \\n[8], and categorizing data based on the examples given during the training phase [9]. Deep \\nLearning, conversely, is a specific subset of ML that utilizes artificial neural networks to replicate \\nthe learning process of the human brain [10]. Deep learning has been received many attentions in \\nvarious fields such as flood detection [11], anomaly detection [12],  civil engineering [13], defect \\ndetection [14],  virtual reality [15] and [16], haptic robot in surgery [17]. Advancements in \\ntechnology drive the growing necessity to integrate cutting-edge methods AI and ML into image \\nclassification and segmentation. This research [18] highlights the effectiveness of deep learning \\nmodels combined with gray level enhancement techniques for automating the classification of \\nwhite matter lesions in MS patients’ MRI scan. Image processing using deep learning techniques \\nin humanoid robots involves applying neural networks to analyze and manipulate visual data \\ncaptured by robot cameras, enabling tasks such as object recognition, navigation, and interaction \\nwith the environment [19]. \\nDeep learning has brought about a significant transformation in the domain of image \\nsegmentation, introducing a multitude of techniques that have substantially enhanced the precision \\nand efficiency of the segmentation process. One such technique is U-Net, a convolutional neural \\nnetwork specifically engineered for the segmentation of biomedical images. It possesses a U-\\nshaped architecture comprising a contracting path (encoder) and an expansive path (decoder). This \\nunique structure enables U-Net to operate with a smaller number of training images while \\ndelivering highly accurate segmentation [20]. Unet++ architecture aims to improve medical image \\nsegmentation by incorporating nested, dense skip pathways that connect the encoder and decoder \\nsub-networks. These redesigned skip pathways help reduce the semantic gap between the feature \\nmaps of the encoder and decoder, which can facilitate the learning process for the optimizer [21]. \\n[22] enhances the original U-Net structure with residual blocks to overcome the vanishing gradient \\nissue and support deeper network models. [23] presented a 3D U-Net architecture for segmenting \\nlung tumors in CT scans and X-Ray images. \\nAttention mechanisms, which are inspired by the human visual system, have proven to be \\nhighly effective in a variety of visual tasks within the realm of image processing and natural \\nlanguage processing [24]. The attention mechanism empowers the model to concentrate on various \\nsegments of the input sequence while generating an output. This effectively encapsulates the \\nrelationships between words or events, even when they are distantly placed [25]. They have found \\napplications in numerous tasks such as image classification, object detection, semantic \\nsegmentation, video comprehension, image generation, 3D vision, multi-modal tasks, and self-\\nsupervised learning. This research [26] enhances face recognition accuracy by intelligently \\ncombining features from multiple models. It achieves this by leveraging attention mechanisms and \\napplying information bottleneck principles. The study [27] aimed to evaluate the hazardous aspects \\nof an abandoned gypsum mine by exploring its location, depth, extension, and overall condition \\nusing x-ray diffraction technique. [28] introduces an innovative pedestrian detection system that \\noperates in a single stage and incorporates both channel and spatial attention mechanisms within \\nthe structure of a CNN. The researchers introduce a method called CACNN, a deep learning \\napproach that skillfully merges 2-D and 3-D CNNs to simultaneously derive spectral and spatial \\ninformation from hyperspectral images in [29]. The authors propose an end-to-end deep learning \\nmodel that integrates a multi-branch spectral-temporal CNN with an efficient channel attention \\nmechanism and the LightGBM algorithm in [30]. [31] proposed a U-Net architecture enhanced \\nwith multiple encoders for better feature extraction and an attention mechanism in decoders for \\nprecise focus on relevant features. [32] improves upon the U-Net model by incorporating multi-\\nscale spatial attention and dilated convolutions to capture context information effectively. \\nThe study introduces a novel method for lung segmentation in chest X-ray images by \\ncombining U-net with attention mechanisms. This method improves the U-net structure by \\nintegrating a Convolutional Block Attention Module (CBAM), which unifies three separate \\nattention mechanisms: Channel Attention Map, Spatial Attention Map, and Pixel Attention. \\nHere are the key advancements of this research: \\n• \\nEnhanced U-net Architecture: By adding CBAM to standard convolution layers in the \\nU-net architecture, the model can better capture global contextual information and improve \\nthe network’s attention to specific regions. This leads to improved feature representation \\nand, ultimately, better performance in tasks such as image segmentation \\n• \\nProposed CBAM with Triple Attention Mechanisms: The integration of Channel, \\nSpatial, and Pixel Attention mechanisms significantly enhances the model’s focus on \\nrelevant features within X-ray images. The channel attention mechanism emphasizes inter-\\nchannel associations, enabling the model to concentrate more on informative channels and \\nboosting its capability to identify relevant features. The spatial attention mechanism allows \\nthe model to focus on crucial spatial locations, enhancing its precision in localization by \\npaying more attention to the spatial correlations between features. Lastly, the pixel \\nattention mechanism empowers the model to focus on individual pixels, further sharpening \\nits focus and improving the accuracy of segmentation by paying more attention to the most \\ninformative pixels. \\nThe adoption of the suggested CBAM in conjunction with the U-net architecture marks a \\nsignificant progression in the field of medical imaging. This innovation is anticipated to improve \\nthe precision of diagnoses and contribute to more favorable health outcomes for patients. The \\nmethod\\'s enhanced performance is assessed using sophisticated metrics, focusing particularly on \\nDice coefficient and Jaccard similarity indices. These metrics are essential for assessing the \\nsegmentation\\'s accuracy by comparing the predicted segmentation with the actual anatomical \\nboundaries, as demonstrated in the studies by [33]. \\nThe structure of the manuscript is organized in the following manner: Section II offers a \\nfoundational overview of the dataset utilized for Chest X-ray lung segmentation and the methods \\nemployed for preprocessing. Section III delves into the methodology being proposed, detailing the \\nfusion of proposed CBAM with the U-net framework. Section IV is dedicated to the exposition of \\nthe simulation outcomes, encompassing a comprehensive account of the training and validation \\nprocedures of the method under consideration. The paper culminates with Section V, which \\nencapsulates the principal findings and the pivotal contributions put forth by this study. \\n2) Chest X-ray Lung Segmentation Dataset Description \\n2-1) Dataset Description \\nResearchers used chest X-rays and matching lung masks from Kaggle to train models that \\ncould identify lungs in X-rays [34]. The dataset under consideration constitutes a significant asset \\nfor the medical research community, especially in the realm of automated image analysis for the \\nscreening of tuberculosis. It encompasses a collection of chest X-ray images paired with \\nsegmentation masks, albeit with the caveat that some masks may be absent. It is recommended \\nthat users corroborate the availability of masks corresponding to the X-ray images to maintain \\nresearch integrity. The dataset is comprehensive, comprising 360 normal X-ray images and 344 \\nabnormal X-ray images indicative of lung infections. The images have been carefully labeled by \\nexperienced radiologists. Representative X-ray images and their corresponding masks from both \\nthe training and validation sets are shown in Figure 1. \\n \\n(a) \\n \\n(b) \\nFig. 1: Representative X-ray images with their corresponding masks from the training and validation datasets, as \\nannotated by expert radiologists. \\nThe images are rendered anonymous and are presented in the Digital Imaging and \\nCommunications in Medicine (DICOM) format, which is the universally accepted standard for the \\nmanagement, storage, reproduction, and dissemination of medical imaging information. The \\ndataset encapsulates a broad array of pulmonary abnormalities, not limited to effusions and miliary \\npatterns. The dataset is a crucial resource for developing algorithms that can detect and segment \\nlung diseases in chest X-rays. It includes a diverse range of images, both normal and abnormal, \\nproviding a comprehensive dataset for analysis. This collection is more than just images; it\\'s an \\nessential link between medical knowledge and AI technology, driving progress in automated \\ndiagnostics. The meticulous assembly and preparation of the data render it an indispensable asset \\nfor researchers striving to innovate in medical image analysis. \\n2-2) Image Augmentation and Preprocessing \\nIn the study, a series of data augmentation techniques were systematically applied to enhance \\nthe dataset for training the neural network, resulting in a sixfold increase in the dataset size. In the \\nrealm of neural network training for medical image analysis, the augmentation techniques of \\ncontrast adjustment and Gaussian blur play pivotal roles. Enhancing the contrast of images serves \\nto amplify the distinction between features, aiding the network in segmenting regions of interest \\nmore effectively, and equipping it to handle variations in lighting conditions that are common in \\nclinical environments. Concurrently, the application of Gaussian blur mitigates high-frequency \\nnoise, which could otherwise lead to overfitting, and fosters a level of generalization that enables \\nthe network to recognize features within images that may lack sharpness or have undergone \\ndifferent preprocessing steps. These techniques collectively ensure that the neural network is not \\nonly more robust but also more adept at accurately analyzing medical images under a wide array \\nof conditions. The augmentation process commenced with a contrast adjustment, where the \\ncontrast of each image was intensified using a linear formula, thereby generating a set of contrast-\\nenhanced images. Subsequently, a Gaussian blur was applied to these images, introducing a slight \\nblur effect without additional smoothing. This step produced a collection of blurred images.  \\nTo further augment the dataset and bolster the robustness of the neural network, a horizontal \\nflipping technique was employed. Horizontal flipping is an essential data augmentation technique \\nthat enhances the robustness of neural networks by introducing additional variability and \\nsymmetry to the training dataset. By mirroring images along the vertical axis, it ensures that the \\nnetwork learns to recognize and segment features irrespective of their lateral orientation, thus \\npreventing bias towards anomalies that may appear on one side of the body. This method is \\nparticularly beneficial in medical imaging, where the accurate detection and segmentation of \\nfeatures are critical, and the mirrored defects aid in reinforcing the network’s diagnostic \\nconsistency across varying patient positions and imaging angles. The same augmentation \\nprocedures were then applied to these horizontally flipped images, resulting in an additional set of \\ncontrast-adjusted and blurred images. \\nThe culmination of these augmentation steps not only expanded the dataset but also introduced \\na variety of transformations that simulate different imaging conditions. This diversity is crucial for \\ntraining a neural network that is resilient and capable of accurately segmenting lung regions in \\nchest X-ray images under various scenarios. Fig. 2 displays the enhanced images that have been \\naugmented using the specified technique, along with their respective masks. \\n \\nFig. 2: Enhanced and augmented images with their corresponding masks, utilizing the specified augmentation \\ntechnique. \\n3)   Methodology \\nIn this section, we commence with an introduction to the U-Net architecture tailored for \\nsegmenting the lung regions from radiographic images. Subsequently, we present the CBAM, \\nfollowed by a proposal for an enhanced U-Net framework that integrates the CBAM, aiming to \\nrefine the segmentation process. \\n3-1) U-Net architecture \\nThe U-Net architecture is a convolutional neural network designed for biomedical image \\nsegmentation. It features a symmetric encoder-decoder structure, which is why it\\'s called \"U-Net.\" \\nThe encoder part compresses the image into a feature-rich representation, reducing spatial \\ndimensions while increasing feature complexity through convolution and pooling operations. The \\ndecoder part then expands this representation, using transposed convolutions to increase spatial \\ndimensions for precise localization. Skip connections between the encoder and decoder transfer \\ncontext information, aiding in accurate segmentation. This combination of context from the \\nencoder and localization from the decoder makes U-Net particularly effective for medical imaging \\ntasks. Fig. 3 illustrates the U-Net architecture for lung segmentation. \\n \\nFig. 3: U-Net architecture applied for lung segmentation using chest X-ray imagery. \\n3-2) CBAM Model \\nThe CBSM is a model designed to enhance the precision of chest X-ray lung segmentation in \\nthe U-net architecture, as opposed to the conventional CNN, particularly when dealing with limited \\nsample sizes. The amalgamation of channel, spatial, and pixel attention mechanisms marks a \\nsubstantial progression in honing the focus on pertinent features within X-ray images. \\n1. Channel Attention: This mechanism emphasizes inter-channel associations, enabling the \\nmodel to concentrate more on channels that are more informative. It aids the model in \\nfocusing on the most significant features across various channels, thereby boosting the \\nmodel’s capability to identify relevant features. \\n2. Spatial Attention: This mechanism allows the model to concentrate on crucial spatial \\nlocations. It enables the model to pay more attention to the spatial correlations between \\nfeatures, thereby enhancing the model’s precision in localization. \\n3. Pixel Attention: This mechanism empowers the model to focus on individual pixels. It \\nfurther sharpens the model’s focus, enabling it to pay more attention to the most \\ninformative pixels, thereby improving the accuracy of segmentation. \\nThese three attention mechanisms collaborate to provide a more comprehensive representation \\nof features, thereby enhancing the model’s performance in tasks such as image segmentation. They \\nenable the model to capture global contextual information more effectively and improve the \\nnetwork’s attention to specific regions.  \\nConsider a feature map represented by 𝐹∈ℝ𝐻×𝑊×𝐶, where H and W are the height and width \\nof the input image, respectively, and C represents the count of spectral bands in the input map. The \\nCBSM functions as a method for dynamically adjusting weights to identify important areas within \\ncomplex scenes. This is achieved through a 1D channel attention map, denoted by 𝑀𝐶∈ℝ1×1×𝐶, \\na 2D spatial attention map denoted by 𝑀𝑆∈ℝ𝐻×𝑊×1, and a 2D pixel attention map denoted by \\n𝑀𝑃∈ℝ𝐻×𝑊×1. Throughout the operation of CBSM, the input data undergo a refinement process \\nfirst through 𝑀𝐶, then 𝑀𝑆, and finally 𝑀𝑃. Consequently, the overall process of the improved \\nCBSM can be written as follows: \\nThe first feature map refined by the channel attention map: \\n𝐹\\n𝐶= (𝑀𝐶(𝐹) + 1) × 𝐹.                                                                                                                              (1) \\nThe intermediate feature map further refined by the spatial attention map: \\n𝐹\\n𝑆= (𝑀𝑆(𝐹\\n𝐶) + 1) × 𝐹\\n𝐶.                                                                                                                          (2) \\nThe final feature map refined by the pixel attention mechanism: \\n𝐹\\n𝑃= (𝑀𝑃(𝐹\\n𝑆) + 1) × 𝐹\\n𝑆,                                                                                                                          (3) \\nwhere × represents element-wise multiplication, and + denotes element-wise addition, the refined \\nmaps 𝑀𝐶, 𝑀𝑆, and 𝑀𝑃 are broadcasted to match the dimensions of the feature maps to which they \\nare applied. The final output, 𝐹\\n𝑃, is the feature map that has been sequentially refined by the \\nchannel, spatial, and pixel attention mechanisms. This provides a more focused and detailed \\nrepresentation for chest X-ray lung segmentation. This process allows for more granular control \\nover the attention given to each pixel, potentially improving the accuracy of the segmentation. The \\nCBSM is visualized in Figure 4. \\n \\nFig. 4:  Visualization of the channel, spatial, and pixel attention mechanisms in the CBSM model. \\nBased on the Fig. 4, the formulas of the mentioned attention mechanisms are shown as \\nfollowing. \\n𝑀𝐶= 𝜎(𝐶𝑁𝑁2 (𝑅𝑒𝐿𝑈(𝐶𝑁𝑁1 (𝐺𝑃\\n𝑎𝑣𝑔(𝑥))))),                                                                                 (1) \\n𝑀𝑃= 𝜎(𝐶𝑁𝑁2 (𝑅𝑒𝐿𝑈(𝐶𝑁𝑁1(𝑥)))),                                                                                                   (2) \\n𝑀𝑆= 𝜎(𝐶𝑁𝑁(𝑐𝑜𝑛𝑐𝑎𝑡(𝐺𝑃\\n𝑚𝑎𝑥(𝑥), 𝐺𝑃\\n𝑎𝑣𝑔(𝑥)))),                                                                            (3) \\nwhere 𝑥 is the input to the attention mechanism, 𝐺𝑃\\n𝑎𝑣𝑔 and 𝐺𝑃\\n𝑚𝑎𝑥 represent the global average \\npooling and global max pooling layer that reduces spatial dimensions and retains channel \\ninformation. 𝐶𝑁𝑁1 and 𝐶𝑁𝑁2 are the convolutional neural network layers for learning channel-\\nwise dependencies. 𝑅𝑒𝐿𝑈 is the Rectified Linear Unit function. 𝜎 is the Sigmoid function that \\nnormalizes the output to a range between 0 and 1.  \\nBy applying the CBSM after each down-sampling (in the encoder) and up-sampling (in the \\ndecoder) operation in the U-Net, the network can focus on the most important features at each \\nlevel. This is because the CBSM refines the feature maps by giving more importance to the salient \\nfeatures in both the channel and spatial dimensions, and now with the addition of the pixel attention \\nmechanism, at the pixel level as well. This could potentially improve the performance of the U-\\nNet, especially when dealing with limited sample sizes, as the CBSM allows the network to make \\nbetter use of the available data by focusing on the most informative parts of the input images. \\nFigure 5 shows the U-Net architecture with CBSM applied for lung segmentation using chest X-\\nray imagery. \\n \\nFig. 5:  U-Net architecture with CBSM applied for lung segmentation using chest X-ray imagery. \\n \\n4) Simulations \\nIn this section, we evaluate the effectiveness of the proposed method for chest X-ray \\nsegmentation using metrics such as the Dice similarity coefficient and Jaccard index. We also \\ncompare the ground truth with segmentation masks. Additionally, we assess segmentation \\nperformance using various metrics, including precision, recall, and accuracy. \\n4.1) Effectiveness of the Proposed Method using Dice similarity coefficient and Jaccard index \\nSemantic segmentation, also referred to as pixel-based classification, is a pivotal technique in \\nwhich each pixel of an image is ascribed to a distinct category or class. This methodology is \\nquintessential across a multitude of disciplines, encompassing medical imaging for the delineation \\nof tissues, remote sensing for the classification of land cover, and autonomous driving for the \\ninterpretation of road scenarios. The primary objective of semantic segmentation is the allocation \\nof a label to every individual pixel, ensuring that pixels bearing identical labels exhibit shared \\nattributes. The evaluation of model performance within this context employs the Jaccard index and \\nthe Dice coefficient, which are critical metrics for assessing the accuracy of the segmentation. The \\nJaccard index and the Dice coefficient are integral metrics in the domain of semantic segmentation. \\nThey are predicated on the foundational elements of true positives (TP), false positives (FP), false \\nnegatives (FN), and true negatives (TN). True positives and true negatives refer to the tuberculosis \\nimages correctly identified as tuberculosis and the normal images correctly identified as normal, \\nrespectively. Conversely, false positives are normal images incorrectly identified as tuberculosis, \\nand false negatives are tuberculosis images incorrectly identified as normal. The Jaccard index, \\nalso recognized as the Intersection over Union (IoU), quantifies the congruence between the \\npredicted labels and the ground truth by calculating the ratio of the intersection to the union of the \\npredicted and actual labels. Mathematically, it is represented as: \\n𝐼𝑜𝑈=\\n𝑇𝑃\\n𝑇𝑃+ 𝐹𝑃+ 𝐹𝑁                                                                                                                                (4) \\nConversely, the Dice coefficient, synonymous with the Dice similarity coefficient, gauges the \\noverlap between two samples. It is computed as twice the intersection of the predicted and true \\nlabels, divided by the sum of the counts of both labels. The formula is expressed as: \\n𝐷𝑖𝑐𝑒=\\n2 × 𝑇𝑃\\n2 × 𝑇𝑃+ 𝐹𝑃+ 𝐹𝑁                                                                                                                      (5) \\nThese metrics are particularly advantageous in semantic segmentation for their ability to measure \\nthe extent of overlap between the segmentation predictions and the ground truth. The Dice \\nSimilarity Coefficient and the Jaccard Index using the are depicted in Figures 6 and 7. \\n \\nFig. 6:  Dice similarity coefficient using by U-net, U-net with the conventional CBSM[33], and U-net with the \\nproposed CBSM. \\n \\nFig. 7:  Jaccard index or IoU using by U-net, U-net with the conventional CBSM [33], and U-net with the proposed \\nCBSM. \\nThe figures in question presumably depict the comparative performance of three distinct U-net \\narchitectures tailored for the segmentation of chest X-ray images. These performances are \\nquantified using the Dice similarity coefficient and the Jaccard index. The superior performance \\nof the U-net model augmented with the Proposed CBSM can be attributed to the following reasons: \\n1. U-net without CBSM: This is the baseline model that does not include any attention \\nmechanisms. It likely performs the least effectively because it treats all channels and spatial \\nlocations equally, without focusing on the most informative features. \\n2. U-net with Conventional CBSM: This model incorporates channel and spatial attention \\nmechanisms. The channel attention allows the model to weigh channels differently, \\nprioritizing those that are more informative for the task. The spatial attention mechanism \\nenables the model to focus on specific areas within the image that are more relevant to the \\nsegmentation task. These mechanisms help the model to ignore irrelevant information and \\nconcentrate on important features, leading to better performance than the baseline U-net. \\n3. U-net with Proposed CBSM: This model includes all three attention mechanisms: \\nchannel, spatial, and pixel attention. The addition of pixel attention allows the model to \\nfocus on the most informative pixels, which is particularly useful for fine-grained \\nsegmentation tasks. This level of detail enables the model to make more accurate \\npredictions at the pixel level, resulting in a higher Dice similarity coefficient compared to \\nthe U-net with only channel and spatial attention. \\nThe graph likely shows that the U-net with the proposed CBSM achieves the highest Dice \\nsimilarity and Jaccard index coefficients across epochs, indicating that it outperforms the other \\ntwo models. The inclusion of pixel attention, along with channel and spatial attention, provides a \\nmore nuanced understanding of the image, leading to superior segmentation results. This suggests \\nthat the proposed CBSM is more adept at capturing global contextual information and focusing \\nthe network’s attention on specific, relevant regions of the chest X-ray images. \\n4.2) Comparison between the ground truth and Segmentation masks \\nVisual comparison between automated segmentation masks and manually annotated \\nground truth serves several critical purposes. It assesses accuracy, validates quantitative metrics, \\nidentifies algorithmic challenges, supports clinical decision-making, and enhances education and \\ncommunication within the medical imaging field. The Fig. 8 shows a comparative display of chest \\nX-ray segmentation results using different U-net architectures. \\n            \\nA                                   B                                     C                                   D                                     E \\n(1) \\n \\n   A                                   B                                     C                                   D                                     E \\n(2) \\n \\n    A                                   B                                     C                                   D                                     E \\n(3) \\nFig.8: Display of three samples: Chest x-ray Image (A), the reference lung mask of the input chest X-ray (ground \\ntruth) (B), the segmented region generated by U-net (C), U-net with the conventional CBSM (D), and U-net with the \\nproposed CBSM (E). \\nThe progression from panels C to E demonstrates the incremental improvements in \\nsegmentation accuracy as more sophisticated attention mechanisms are incorporated into the U-\\nnet architecture based on the mentioned reasons. The proposed CBSM’s inclusion of pixel \\nattention, in addition to channel and spatial attention, allows for a more precise and detailed \\nanalysis of the chest X-ray images, leading to the most accurate segmentation results. \\n \\n \\n4.3) Segmentation Performance Evaluation Using Various Metrics \\nIn this subsection, we embark on a comprehensive exploration of other performance indices \\nfor the effectiveness of our proposed segmentation method. These metrics are calculated using the \\nfollowing formulas: \\n• \\nAccuracy: The proportion of true results (both true positives and true negatives) among \\nthe total number of cases examined. \\n𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦= 𝑇𝑃+ 𝑇𝑁+ 𝐹𝑃+ 𝐹𝑁\\n𝑇𝑃+ 𝐹𝑁\\n                                                                                              (6) \\n• \\nSensitivity (Recall): The proportion of actual positive cases which are correctly identified. \\n𝑅𝑒𝑐𝑎𝑙𝑙 = \\n𝑇𝑃\\n𝑇𝑃+ 𝐹𝑁                                                                                                                     (7) \\n• \\nSpecificity: The proportion of actual negative cases which are correctly identified. \\n𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐𝑖𝑡𝑦= \\n𝑇𝑁\\n𝐹𝑃+ 𝑇𝑁                                                                                                             (8) \\n• \\nPrecision: The proportion of positive identifications that were actually correct. \\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = \\n𝑇𝑃\\n𝑇𝑃+ 𝐹𝑃.                                                                                                              (9) \\n• \\nF1 Score: The harmonic mean of precision and sensitivity, providing a balance between \\nthe two in cases where one may be more important than the other. \\n𝐹1 −𝑠𝑐𝑜𝑟𝑒  = 2 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 × 𝑅𝑒𝑐𝑎𝑙𝑙 \\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + 𝑅𝑒𝑐𝑎𝑙𝑙  .                                                                             (10) \\nThe metrics presented collectively offer a detailed assessment of the deep learning model’s \\nefficacy in pixel-level classification tasks within chest X-ray (CXR) imagery. Table 1 delineates \\nthe aforementioned performance indices. \\nTable 1. The performance indices using U-net, U-net with the conventional \\nCBSM [33], and U-net with the proposed CBSM \\n \\nMethod \\nAccuracy (%) \\nRecall (%) \\nSpecificity (%) \\nPrecision (%) \\nF1-score (%) \\nU-net \\n96.2 \\n95.30 \\n93.54 \\n96.68 \\n95.98 \\nU-net with the \\nconventional \\nCBSM [33] \\n97.8 \\n95.57 \\n95.81 \\n97.14 \\n96.34 \\nU-net with the \\nproposed CBSM \\n98.8 \\n97.50 \\n97.64 \\n97.68 \\n97.58 \\n \\nThe results in Table 1 can be interpreted as follows: \\n• \\nU-net: The standard U-net model shows high effectiveness with an accuracy of 96.2%, a \\nrecall of 95.30%, and a precision of 96.68%, leading to an F1-score of 95.98%. Its \\nspecificity is 93.54%, indicating it is slightly less effective at correctly identifying non-\\nrelevant instances compared to relevant ones. \\n• \\nU-net with the conventional CBSM: The addition of channel and spatial attention \\nmechanisms (CBSM) improves the model’s performance across all metrics, with notable \\nincreases in specificity and precision, resulting in an F1-score of 96.34%. \\n• \\nU-net with the proposed CBSM: Incorporating the proposed CBSM, which likely \\nincludes additional pixel attention mechanisms, further enhances the model’s performance, \\nachieving the highest accuracy of 98.8%, recall of 97.50%, and an F1-score of 97.58%. \\nThis model also shows a high specificity of 97.64%, indicating a strong ability to correctly \\nidentify non-relevant instances. \\nThe incremental improvements in these metrics suggest that the proposed CBSM provides a more \\nnuanced and detailed analysis of CXR images, leading to more accurate segmentation results. The \\nreference to [33] suggests that the conventional CBSM model was likely detailed in their work, \\nand the proposed CBSM is an advancement over this model. \\n5) Conclusion \\nThe research presents an innovative approach to lung segmentation in chest X-ray imagery, \\nenhancing the U-net architecture through the incorporation of a CBAM. This module amalgamates \\nthree distinct attention mechanisms—channel attention, spatial attention, and pixel attention—\\nrefining the model’s structural framework for improved performance. The channel, spatial, and \\npixel attention mechanisms collectively enhance a model’s segmentation accuracy. The channel \\nattention prioritizes significant channels, the spatial attention targets important spatial areas, and \\nthe pixel attention zeroes in on the most informative pixels, all contributing to a more precise and \\ndetailed segmentation output. The resulting improvements in feature representation and \\nsegmentation accuracy are not merely theoretical but have been empirically validated through \\nrigorous performance assessments. The utilization of metrics such as the Dice coefficient and \\nJaccard similarity index has provided a quantifiable measure of the method\\'s superiority over \\ntraditional models. Moreover, the comparative analysis of pixel classification metrics across \\ndifferent U-net architectures for chest X-ray image segmentation reveals a clear trend of \\nperformance enhancement with the integration of attention mechanisms. In conclusion, the \\nintegration of the proposed CBAM with the U-net architecture represents a significant leap forward \\nin the domain of medical image analysis. \\nFuture investigations aimed at refining the attention mechanisms for targeted lung pathology \\nfeatures have the potential to significantly enhance the precision of segmentation outcomes. \\nAdditionally, the exploration of integrating a U-net architecture augmented with a CBAM into \\ndiverse imaging modalities, including CT and MRI scans, holds promise for the development of a \\nmore holistic diagnostic instrument. \\n \\n6) Reference \\n[1] \\nD. I. Morís, J. de Moura, S. Aslani, J. Jacob, J. Novo, and M. Ortega, “Multi-task \\nlocalization of the hemidiaphragms and lung segmentation in portable chest X-ray images \\nof COVID-19 patients,” Digit. Heal., vol. 10, p. 20552076231225852, 2024, doi: \\n10.1177/20552076231225853. \\n[2] \\nD. N. Kumar and M. K. Joseph, “Fused Feature Vector and Dual FCM for Lung \\nSegmentation from Chest X-Ray Images.,” Int. J. Intell. Eng. Syst., vol. 17, no. 2, 2024, doi: \\n10.22266/ijies2024.0430.38. \\n[3] \\nM. A. L. Khaniki, M. Mirzaeibonehkhater, and M. Manthouri, “Enhancing Pneumonia \\nDetection using Vision Transformer with Dynamic Mapping Re-Attention Mechanism,” in \\n2023 13th International Conference on Computer and Knowledge Engineering (ICCKE), \\nIEEE, 2023, pp. 144–149. doi: 10.1109/ICCKE60553.2023.10326313. \\n[4] \\nH. Ajami, M. K. Nigjeh, and S. E. Umbaugh, “Unsupervised white matter lesion \\nidentification in multiple sclerosis ( MS ) using MRI segmentation and pattern \\nclassification\\u202f: a novel approach with CVIPtools,” vol. 12674, pp. 1–6, 2023, doi: \\n10.1117/12.2688268. \\n[5] \\nZ. Fang, J. Qajar, K. Safari, S. Hosseini, M. Khajehzadeh, and M. L. Nehdi, “Application \\nof Non-Destructive Test Results to Estimate Rock Mechanical Characteristics—A Case \\nStudy,” Minerals, vol. 13, no. 4, p. 472, 2023, doi: 10.3390/min13040472. \\n[6] \\nK. Safari and F. Imani, “A Novel Fuzzy-BELBIC Structure for the Adaptive Control of \\nSatellite Attitude.” Oct. 30, 2022. doi: https://doi.org/10.1115/IMECE2022-96034. \\n[7] \\nA. Birashk, J. K. Kordestani, and M. R. Meybodi, “Cellular teaching-learning-based \\noptimization approach for dynamic multi-objective problems,” Knowledge-Based Syst., vol. \\n141, pp. 148–177, 2018, doi: 10.1016/j.knosys.2017.11.016. \\n[8] \\nA. Samii, H. Karami, H. Ghazvinian, A. Safari, and Y. D. Ajirlou, “Comparison of DEEP-\\nLSTM and MLP Models in Estimation of Evaporation Pan for Arid Regions.,” J. Soft \\nComput. Civ. Eng., vol. 7, no. 2, 2023. \\n[9] \\nP. X. McCarthy, X. Gong, S. Eghbal, D. S. Falster, and M.-A. Rizoiu, “Evolution of \\ndiversity and dominance of companies in online activity,” PLoS One, vol. 16, no. 4, p. \\ne0249993, 2021. \\n[10] K. Safari, S. Khalfalla, and F. Imani, “Dependency Evaluation of Defect Formation and \\nPrinting Location in Additive Manufacturing,” in ASME International Mechanical \\nEngineering Congress and Exposition, American Society of Mechanical Engineers, 2022, \\np. V02AT02A016. doi: 10.1115/IMECE2022-95145. \\n[11] B. Bahrami, “Enhanced Flood Detection Through Precise Water Segmentation Using \\nAdvanced Deep Learning Models,” vol. 6, no. 1, pp. 1–8, 2024, doi: 10.61186/JCER.6.1.1. \\n[12] S. Shomal Zadeh, M. Khorshidi, and F. Kooban, “Concrete Surface Crack Detection with \\nConvolutional-based Deep Learning Models,” Int. J. Nov. Res. Civ. Struct. Earth Sci., vol. \\n10, no. 3, pp. 25–35, 2023, doi: 10.54756/IJSAR.2023.V3.10.1. \\n[13] S. S. Zadeh and N. Joushideh, “Exploring Lateral Movement Coefficient’s Influence on \\nGround Movement Patterns in Shallow Urban Tunnels,” Int. J. Sci. Acad. Res. (IJSAR), \\neISSN 2583-0279, vol. 3, no. 10, pp. 1–11, 2023. \\n[14] V. Monjezi, A. Trivedi, G. Tan, and S. Tizpaz-Niari, “Information-theoretic testing and \\ndebugging of fairness defects in deep neural networks,” in 2023 IEEE/ACM 45th \\nInternational Conference on Software Engineering (ICSE), IEEE, 2023, pp. 1571–1582. \\ndoi: 10.1109/ICSE48619.2023.00136. \\n[15] M. Salehi, N. Javadpour, B. Beisner, M. Sanaei, and S. B. Gilbert, “Innovative \\nCybersickness Detection: Exploring Head Movement Patterns in Virtual Reality,” arXiv \\nPrepr. arXiv2402.02725, 2024, doi: 10.48550/arXiv.2402.02725. \\n[16] M. Sanaei, S. B. Gilbert, N. Javadpour, H. Sabouni, M. C. Dorneich, and J. W. Kelly, “The \\nCorrelations of Scene Complexity, Workload, Presence, and Cybersickness in a Task-Based \\nVR Game,” arXiv Prepr. arXiv2403.19019, 2024, doi: 10.48550/arXiv.2403.19019. \\n[17] V. Mohammadi et al., “Development of a Two-Finger Haptic Robotic Hand with Novel \\nStiffness Detection and Impedance Control,” Sensors, vol. 24, no. 8, p. 2585, 2024, doi: \\n10.3390/s24082585. \\n[18] M. K. Nigjeh, H. Ajami, and S. E. Umbaugh, “Automated classification of white matter \\nlesions in multiple sclerosis patients ’ MRI images using gray level enhancement and deep \\nlearning,” vol. 12674, pp. 1–6, 2023, doi: 10.1117/12.2688269. \\n[19] M. Hosseini, V. Mohammadi, F. Jafari, and E. Bamdad, “RoboCup 2016 Best Humanoid \\nAward Winner Team Baset Adult-Size,” in RoboCup 2016: Robot World Cup XX 20, \\nSpringer, 2017, pp. 467–477. doi: 10.1007/978-3-319-68792-6_39. \\n[20] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical \\nimage segmentation,” in Medical image computing and computer-assisted intervention–\\nMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, \\nproceedings, part III 18, Springer, 2015, pp. 234–241. doi: 10.1007/978-3-319-24574-\\n4_28. \\n[21] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++: A nested u-net \\narchitecture for medical image segmentation,” in Deep Learning in Medical Image Analysis \\nand Multimodal Learning for Clinical Decision Support: 4th International Workshop, \\nDLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with \\nMICCAI 2018, Granada, Spain, September 20, 2018, P, Springer, 2018, pp. 3–11. doi: \\n10.1007/978-3-030-00889-5_1. \\n[22] T. Agrawal and P. Choudhary, “ReSE‐Net: Enhanced UNet architecture for lung \\nsegmentation in chest radiography images,” Comput. Intell., vol. 39, no. 3, pp. 456–477, \\n2023, doi: 10.1111/coin.12575. \\n[23] V.-L. Le and O. Saut, “RRc-UNet 3D for lung tumor segmentation from CT scans of Non-\\nSmall Cell Lung Cancer patients,” in Proceedings of the IEEE/CVF International \\nConference on Computer Vision, 2023, pp. 2316–2325. \\n[24] M. A. L. Khaniki and M. Manthouri, “Enhancing Price Prediction in Cryptocurrency Using \\nTransformer Neural Network and Technical Indicators,” arXiv Prepr. arXiv2403.03606, \\n2024, doi: 10.48550/arXiv.2403.03606. \\n[25] M. A. Labbaf-Khaniki, M. Manthouri, and H. Ajami, “Twin Transformer using Gated \\nDynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the \\nTennessee \\nEastman \\nProcess,” \\narXiv \\nPrepr. \\narXiv2403.10842, \\n2024, \\ndoi: \\n10.48550/arXiv.2403.10842. \\n[26] M. Akyash, A. Zafari, and N. M. Nasrabadi, “Trading-off Mutual Information on Feature \\nAggregation for Face Recognition,” arXiv Prepr. arXiv2309.13137, 2023, doi: \\n10.48550/arXiv.2309.13137. \\n[27] M. A. Khalil, M. Sadeghiamirshahidi, R. M. Joeckel, F. M. Santos, and A. Riahi, “Mapping \\na hazardous abandoned gypsum mine using self-potential, electrical resistivity tomography, \\nand Frequency Domain Electromagnetic methods,” J. Appl. Geophys., vol. 205, p. 104771, \\n2022, doi: 10.1016/j.jappgeo.2022.104771. \\n[28] Y. Zhang et al., “CSANet: Channel and spatial mixed attention CNN for pedestrian \\ndetection,” \\nIEEE \\nAccess, \\nvol. \\n8, \\npp. \\n76243–76252, \\n2020, \\ndoi: \\n10.1109/ACCESS.2020.2986476. \\n[29] H. Guo, J. Liu, J. Yang, Z. Xiao, and Z. Wu, “Deep collaborative attention network for \\nhyperspectral image classification by combining 2-D CNN and 3-D CNN,” IEEE J. Sel. \\nTop. Appl. Earth Obs. Remote Sens., vol. 13, pp. 4789–4802, 2020, doi: \\n10.1109/JSTARS.2020.3016739. \\n[30] H. Jia et al., “A model combining multi branch spectral-temporal CNN, Efficient Channel \\nattention, and LightGBM for MI-BCI classification,” IEEE Trans. Neural Syst. Rehabil. \\nEng., vol. 31, pp. 1311–1320, 2023, doi: 10.1109/TNSRE.2023.3243992. \\n[31] I. Aboussaleh, J. Riffi, K. El Fazazy, M. A. Mahraz, and H. Tairi, “Efficient U-Net \\narchitecture with multiple encoders and attention mechanism decoders for brain tumor \\nsegmentation,” Diagnostics, vol. 13, no. 5, p. 872, 2023, doi: 10.3390/diagnostics13050872. \\n[32] A. Amer, T. Lambrou, and X. Ye, “MDA-unet: a multi-scale dilated attention U-net for \\nmedical image segmentation,” Appl. Sci., vol. 12, no. 7, p. 3676, 2022, doi: \\n10.3390/app12073676. \\n[33] G. Gaál, B. Maga, and A. Lukács, “Attention U-net based adversarial architectures for chest \\nX-ray lung segmentation,” CEUR Workshop Proc., vol. 2692, pp. 1–7, 2020, doi: \\n10.48550/arXiv.2003.10304. \\n[34] T. Rahman et al., “Reliable tuberculosis detection using chest X-ray with deep learning, \\nsegmentation and visualization,” IEEE Access, vol. 8, pp. 191586–191601, 2020, doi: \\n10.1109/ACCESS.2020.3031384. \\n \\n', metadata={'Published': '2024-05-07', 'Title': 'A Novel Approach to Chest X-ray Lung Segmentation Using U-net and Modified Convolutional Block Attention Module', 'Authors': 'Mohammad Ali Labbaf Khaniki, Mohammad Manthouri', 'Summary': \"Lung segmentation in chest X-ray images is of paramount importance as it\\nplays a crucial role in the diagnosis and treatment of various lung diseases.\\nThis paper presents a novel approach for lung segmentation in chest X-ray\\nimages by integrating U-net with attention mechanisms. The proposed method\\nenhances the U-net architecture by incorporating a Convolutional Block\\nAttention Module (CBAM), which unifies three distinct attention mechanisms:\\nchannel attention, spatial attention, and pixel attention. The channel\\nattention mechanism enables the model to concentrate on the most informative\\nfeatures across various channels. The spatial attention mechanism enhances the\\nmodel's precision in localization by focusing on significant spatial locations.\\nLastly, the pixel attention mechanism empowers the model to focus on individual\\npixels, further refining the model's focus and thereby improving the accuracy\\nof segmentation. The adoption of the proposed CBAM in conjunction with the\\nU-net architecture marks a significant advancement in the field of medical\\nimaging, with potential implications for improving diagnostic precision and\\npatient outcomes. The efficacy of this method is validated against contemporary\\nstate-of-the-art techniques, showcasing its superiority in segmentation\\nperformance.\"}), Document(page_content='LIGHT-FIELD VIEW SYNTHESIS USING A CONVOLUTIONAL BLOCK ATTENTION\\nMODULE\\nM. Shahzeb Khan Gul†, M. Umair Mukati⋆, Michel B¨\\natz†, Søren Forchhammer⋆, Joachim Keinert†∗\\n† Moving Picture Technologies, Fraunhofer IIS, 91058, Erlangen, Germany\\n⋆DTU Fotonik, Technical University of Denmark, Ørsteds Plads, Kgs. Lyngby - 2800, Denmark\\nABSTRACT\\nConsumer light-ﬁeld (LF) cameras suffer from a low or limited reso-\\nlution because of the angular-spatial trade-off. To alleviate this draw-\\nback, we propose a novel learning-based approach utilizing attention\\nmechanism to synthesize novel views of a light-ﬁeld image using a\\nsparse set of input views (i.e., 4 corner views) from a camera ar-\\nray. In the proposed method, we divide the process into three stages,\\nstereo-feature extraction, disparity estimation, and ﬁnal image re-\\nﬁnement. We use three sequential convolutional neural networks\\nfor each stage.\\nA residual convolutional block attention module\\n(CBAM) is employed for ﬁnal adaptive image reﬁnement. Attention\\nmodules are helpful in learning and focusing more on the important\\nfeatures of the image and are thus sequentially applied in the channel\\nand spatial dimensions. Experimental results show the robustness of\\nthe proposed method. Our proposed network outperforms the state-\\nof-the-art learning-based light-ﬁeld view synthesis methods on two\\nchallenging real-world datasets by 0.5 dB on average. Furthermore,\\nwe provide an ablation study to substantiate our ﬁndings.\\nIndex Terms— Light-ﬁeld, View synthesis, Deep-learning,\\n1. INTRODUCTION\\nTraditional cameras capture only the intensities of the incident light-\\nrays at the photosensor, whereas, light-ﬁelds collect massive infor-\\nmation of the scene by additionally acquiring the directional infor-\\nmation of the incident light-rays. This additional angular informa-\\ntion becomes beneﬁcial in many applications such as depth estima-\\ntion, post-capture refocusing, and 3D reconstruction.\\nLight-ﬁelds can be captured through many different ways. Con-\\nventional methods include camera arrays [1], which can capture a\\nLF simultaneously in time but have a limited angular resolution be-\\ncause of the numbers of cameras and the necessary spacing. Gantry\\nsystems [2], which sample a high angular resolution light-ﬁeld by\\nsequentially capturing each viewpoint, cannot capture dynamic con-\\ntent. On the other hand, hand-held light-ﬁeld cameras, such as from\\nLytro and Raytrix [3], offer a feasible solution for light-ﬁeld cap-\\nture, based on the plenoptic camera designs [4]. However, it suffers\\nfrom the spatial and angular resolution trade-off due to the sensor\\nlimitation.\\n∗Published in the IEEE 2021 International Conference on Image Process-\\ning (IEEE ICIP 2021), scheduled for 19-22 September 2021 in Anchorage,\\nAlaska, United States. Personal use of this material is permitted. However,\\npermission to reprint/republish this material for advertising or promotional\\npurposes or for creating new collective works for resale or redistribution to\\nservers or lists, or to reuse any copyrighted component of this work in other\\nworks, must be obtained from the IEEE. Contact: Manager, Copyrights and\\nPermissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscat-\\naway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966.\\nIn recent years, different light-ﬁeld view synthesis approaches\\nhave been published aiming to solve the limited angular resolution\\nproblem. The success of deep learning compared to traditional meth-\\nods for numerous computer vision and image processing problems,\\nsuch as multi-view stereo [5,6], optical ﬂow [7], and super-resolution\\n[8], inspired the researchers to develop learning-based view interpo-\\nlation methods [9–14]. These methods can be broadly divided into\\ntwo categories: image-based rendering (IBR) and depth image-based\\nrendering (DIBR) methods. The IBR methods synthesizing the novel\\nviews without explicitly modeling the scene depth [13, 14], tend to\\nfail with increasing baseline of the input views, whereas, depth-\\nbased methods can handle inputs with wider baseline (i.e. large dis-\\nparities) [11,15].\\nIn this paper, we demonstrate for the ﬁrst time that CBAM used\\nin a different context, i.e., object detection, can be used for light-\\nﬁeld view synthesis. We divide the whole process into three stages,\\ni.e., stereo-feature extraction, disparity estimation, and ﬁnal image\\nreﬁnement. In the ﬁrst step, we extract features directly from stereo\\npairs and we feed these stereo features into a disparity estimation\\nnetwork to estimate target disparity maps corresponding each in-\\nput view. Multiple disparity vectors per target view alleviate the\\ninterpolation error due to the warping of input images. In the end,\\nan image reﬁnement network using a convolutional block attention\\nmodule takes the warped images as input, and outputs the ﬁnal novel\\ntarget view. Attention modules have been studied and used to a great\\nextent in the literature [16], [17], [18]. Attention mechanisms not\\nonly guide the network to focus on important features, but they also\\nimprove the representation of the regions of interest.\\nThe remainder of the paper is structured as follows. In Sec-\\ntion 2, we provide a comprehensive literature review on state-of-\\nthe-art view synthesis algorithms. Section 3 explains the proposed\\nnetwork architecture. Experimental results are then presented and\\ndiscussed in Section 4 before the paper is concluded in Section 5.\\n2. RELATED WORK\\n2.1. Image-based rendering (IBR) method\\nIn literature, some methods approach the light-ﬁeld view synthesis\\nproblem as a speciﬁc case of signal reconstruction. They exploit the\\nintrinsic sparsity of the light-ﬁeld images. In [19], a captured light-\\nﬁeld is considered to be sparse in the continuous Fourier domain;\\nhence, novel views can be reconstructed from a small number of 1D\\nviewpoint trajectories. Levin and Durand [20] synthesize the novel\\nviews using 3D focal stack sequences with an assumption that the\\nlight-ﬁeld contains only Lambertian surfaces.\\nMany researchers used sparse coding as a tool to interpolate\\nnovel views. In [21], a global dictionary is learned from light-ﬁeld\\npatches to synthesize novel views from a set of input light-ﬁeld\\n© IEEE 2021.\\narXiv:2012.01900v2  [eess.IV]  31 May 2021\\nFig. 1: Overall ﬂow of the proposed network. Here, IL, IR, and IB are the selected input images as shown in Fig. 2. U and V are of the size\\nof the input views and repeatedly contain the u and v coordinate of the target view to be rendered, respectively.\\nFig. 2: Based on the angular coordinate of the target view, we select\\nthree input views from a given sparse set of four corner views. In-\\nput views with the least absolute distance from the target view are\\nselected. To maintain the correspondence direction constant among\\ndifferent input combinations, we ﬂip the selected inputs views in\\nhorizontal or vertical direction accordingly to make an upper left tri-\\nangle shape.\\nviews. On the other hand, a local dictionary learned using the cen-\\ntral region of the light-ﬁeld images is successfully applied for the\\ninterpolation of novel views in [22], [23].\\nIn [24] and [10], the authors approached the light-ﬁeld view\\nsynthesis problem as angular domain super-resolution. In [24], two\\nsequential convolutional neural networks (CNNs) take sub-aperture\\nviews as inputs to increase the spatial and angular resolution of the\\nlight-ﬁeld consecutively. On the other hand, [10] applied the CNNs\\ndirectly on lenslet images to increase the resolution in both spatial\\nand angular domains. Wang et al. [14] proposed to use stacked 3D\\nvolumes of epipolar plane images in a CNN framework. In [13],\\nspatio-angular alternating convolutions (i.e., pseudo 4D-CNN) are\\nperformed to incorporate 4D light-ﬁeld data for novel view inter-\\npolation. On the other hand, our proposed network synthesizes the\\nnovel view by explicitly modeling the scene geometry.\\n2.2. Depth image-based rendering (DIBR) method\\nTypically, depth-based view synthesis methods are divided into two\\nsteps. At ﬁrst, these methods estimate the disparity at a target view\\nusing the sparse light-ﬁeld views and then utilize it to synthesize\\nthe target view by warping of the input views. In [9], two CNNs\\nare trained to jointly estimate the depth and synthesize a view in an\\nend-to-end fashion. In [11], a feature extraction CNN is introduced\\nas the ﬁrst part of the network architecture. Moreover, the methods\\nestimates four disparity maps corresponding to each input view and\\nthen utilize a selection CNN for the selection of pixels from warped\\nimages. Jin et al. [25] proposed CNN estimating 4D depth maps for\\na high angular resolution light-ﬁeld, i.e., providing depth for each\\nlight ray in the 4D light-ﬁeld. The resulting 4D depth is then utilized\\nto synthesize all novel views by backward warping. As a ﬁnal step,\\na CNN light-ﬁeld blending module is employed to get the ﬁnal light-\\nﬁeld.\\nRecently, a view synthesis method based on multi-plane image\\n(MPI) [26] was introduced, where a scene is represented by a stack\\nof RGBA planes at different depths. In [27], this idea is extended to\\nlight-ﬁeld using a 3D-CNN to infer an MPI. Flynn et al. [28] pro-\\nposed to use the variational optimization framework in conjunction\\nwith deep learning to improve the reconstruction quality of MPIs.\\nWhile our algorithm is inspired by [11], it introduces a new com-\\nbination of attention mechanism to efﬁciently deal with the occluded\\nregions as shown in Fig. 4.\\n2.3. Attention mechanism\\nVery recently, attention mechanisms have become a very popular\\nmethod to enhance a deep neural network. Attention modules try\\nto make a neural network focus on salient regions of its feature rep-\\nresentation. In [16], a residual attention network is used for im-\\nage classiﬁcation. The Squeeze-and-excitation (SE) block attention\\nmodule introduced in [17], improves the classiﬁcation accuracy by\\nexploiting channel inter-dependencies. In [18], the authors intro-\\nduced a CBAM by adding a spatial attention mechanism to a SE\\nblock to validate performance of object detection. In [29] a variant\\nof CBAM is used in video frame interpolation to reﬁne the inter-\\npolated frame. Our proposed method uses the attention mechanism\\nfor a similar purpose, i.e., image reﬁnement. We integrate CBAM\\nas proposed in [18] into our light-ﬁeld view interpolation pipeline,\\nusing both channel and spatial attention.\\n3. PROPOSED METHOD\\nLet L(x, y, u, v) denote a light-ﬁeld according to two-plane parametriza-\\ntion as shown in Fig. 2, where (x, y) are the spatial coordinates and\\n(u, v) are the angular coordinates. Each sub-aperture view in the\\nlight-ﬁeld is denoted as Iu,v.\\n3.1. Network Architecture\\nThe architecture of our proposed network is shown in Fig. 1. To\\nsynthesize high-quality novel views, we divide the proposed method\\ninto three stages: stereo feature extraction, disparity estimation, and\\nimage reﬁnement. Let LS be four corner views of a light-ﬁeld, our\\nproposed method makes use of the three closest corner views to re-\\nconstruct a high-quality image Iut,vt at the target position:\\nˆ\\nIut,vt = f (IL, IR, IB, ut, vt),\\n(1)\\nwhere f is the function that we aim to learn and which should esti-\\nmate a view ˆ\\nIut,vt as similar as possible to the ground-truth image\\n2\\nIut,vt. IL, IR, and IB are the three selected corner views for the tar-\\nget position (ut, vt). Out of the four, the three reference views are\\nselected as input to the network based on their minimum distance to\\nthe target view (as depicted in Fig. 2). Selecting three corner views\\nas opposed to four reduces the overall complexity of the network. To\\nmaintain constant geometry among different inputs, we ﬂip the im-\\nages left-to-right and/or up-to-down accordingly so that we always\\nhave an upper left triangle shape of the selected input views (see Fig.\\n2). Note that the ﬂipping operation will also change the angular co-\\nordinate of the target view. Similarly, we ﬂip back the output of the\\nnetwork to get the ﬁnal reconstructed image.\\n3.1.1. Stereo feature extraction network\\nWe ﬁrst extract stereo features from the sparse set of selected input\\nviews. The architecture of the feature extraction module fe is based\\non [11]. The network fe consists of six convolutional layers with\\n3×3 kernels and two average pooling layers with 16×16 and 8×8\\nkernels. However, instead of extracting features for each image in-\\ndividually, as done in [11], we extract features directly from stereo\\npairs. We group the input views into three stereo pairs, namely hor-\\nizontal (IL,IR), vertical (IL,IB), and diagonal (IB,IR). Horizontal\\nand vertical stereo pairs use the same architecture for feature ex-\\ntraction. The reason for having a shared network here is that the\\ngeometry between the images can be made identical to the hori-\\nzontal case when the vertical stereo pair is rotated by 90◦counter-\\nclockwise. Furthermore, this reduces the complexity of the network\\nand thus simpliﬁes the trainability. In addition to the stereo pair\\nas input, the network also takes U and/or V image planes, where\\nU(x, y) = ut, V (x, y) = vt\\n∀(x, y). These image planes aid the\\nnetwork in collecting appropriate information from each stereo pair\\nbased on the position of the novel view.\\nLet Fhorz\\n=\\nfe(IL, IR, U), Fvert\\n=\\nfe(IL, IB, V ), and\\nFdiag = fe(IB, IR, U, V ) be the computed feature volumes for\\nthe three stereo pairs. These 32-channel feature volumes are then\\nconcatenated and passed to the disparity estimation network.\\n3.1.2. Disparity estimation network\\nThe goal of this module is to estimate a disparity map per input view\\nvalid at the target position. Multiple disparity values per target view\\npixel allow for a more effective handling of occluded regions as com-\\npared to [9] where a single disparity map is estimated for all input\\nviews. The architecture of the disparity estimation network fd is in-\\nspired by [11]. The network consists of seven convolutional layers\\nwith 3×3 ﬁlter size. The ﬁrst four layers use dilated convolutions\\nat rates 2, 4, 8, and 16, respectively. The network takes the features\\nextracted from the stereo pairs and angular coordinate image planes\\nU and V as input and output disparity maps Di, for i ∈{L, R, B}.\\n3.1.3. Image reﬁnement network\\nThe estimated disparity maps are used to warp each corner view in\\norder to have them registered with the target one. After warping the\\ninput views Iw\\ni , for i ∈{L, R, B}, at the target location using the\\nestimated disparity maps, we estimate the ﬁnal image using a resid-\\nual CBAM reﬁnement network. The architecture of the reﬁnement\\nnetwork consists of a head and tail 3×3 convolution layer with ﬁve\\nresidual groups in between, referred to as ResGroups from now, each\\nof which consists of three CBAMs [18]. As shown in Fig. 3 (a), each\\nCBAM has two 3×3 convolution layers with ReLU activation in be-\\ntween, followed by a channel attention (CA) and spatial attention\\nFig. 3: Illustration of ResGroup, CA module, and SA module.\\n(SA) module before the residual connection. Both attention mod-\\nules, channel and spatial, compute complementary attentions focus-\\ning on ‘what’ and ‘where’ respectively. The architecture of the CA\\nmodule is shown in Fig. 3 (c). We ﬁrst aggregate the spatial infor-\\nmation using average-pooling and max-pooling operations. Features\\ndescriptors from both operations are then forwarded to a shared two\\n1×1 convolution layer network to produce a 1D channel attention\\nmap. Similarly, the architecture of the SA module is illustrated in\\nFig. 3 (b). We aggregate the channel information using two pool-\\ning operations followed by a 7×7 convolution layer producing a 2D\\nspatial attention map. The ﬁnal output image at the target position\\nis obtained by adding the output of the reﬁnement network and the\\naverage of the warped views.\\n3.2. Loss Function\\nThe model is trained by optimizing the sum of the following loss\\nfunctions:\\nLfinal = ∥Iu,v −ˆ\\nIu,v∥1 + λ1∥∇Iu,v −∇ˆ\\nIu,v∥1,\\n(2)\\nLwarp = λ2\\nX\\ni\\n∥Iu,v −Iw\\ni ∥1 + λ3\\nX\\ni\\n∥∇Iu,v −∇Iw\\ni ∥1.\\n(3)\\nIw\\ni is the warped view from one stereo pair using the estimated dis-\\nparity map, where i ∈{L, R, B}. Iu,v and ˆ\\nIu,v are ground-truth\\nand estimated novel views, respectively. Moreover, we experimen-\\ntally set λ1 = 0.5, λ2 = 0.25, and λ3 = 0.125. The ﬁrst term\\nin both losses is the L1 loss between the predicted and the ground-\\ntruth images, whereas, the second term preserves texture by taking\\nL1-norm between spatial gradients.\\n3.3. Training\\nThe proposed model is implemented using TensorFlow as frame-\\nwork. The model is trained on the light-ﬁeld dataset Flowers pre-\\nsented in [30]. This dataset consists of 3343 images of ﬂowers. We\\n3\\nDiverse [9]\\nFlowers [30]\\nMethod\\nPSNR\\n(dB)\\nMS-SSIM\\n(dB)\\nPSNR\\n(dB)\\nMS-SSIM\\n(dB)\\nKalantari et al. [9]\\n39.56\\n24.31\\n40.22\\n22.47\\nNavarro et al. [11]\\n40.75\\n25.57\\n42.67\\n25.04\\nLFVS (ours)\\n41.14\\n25.89\\n43.13\\n25.44\\nLFVS-AM (ours)\\n41.33\\n26.05\\n43.17\\n25.54\\nTable 1: Quantitative results on the test datasets Flowers and Di-\\nverse. For each row, average PSNR (dB) and MS-SSIM (dB) values\\nfor Y channel of the image achieved on all intermediate rendering\\npositions are listed for all methods.\\n(a) 29.28 dB\\n(b) 30.37 dB\\n(c) 31.75 dB\\n(d) 33.46 dB\\n(e)\\n(f) 40.07 dB\\n(g) 42.74 dB\\n(h) 42.94 dB\\n(i) 43.38 dB\\n(j)\\nFig. 4: Visual comparison of the central view of the scenes. First\\nand second row contains the LEAVES scene and its zoomed-in re-\\ngion from Diverse dataset, whereas, third and fourth row contains the\\nIMG 7824 scene and its zoomed-in region from Flowers dataset.\\n(a)(f) Kalantari et al. [9], (b)(g) Navarro et al. [11], (c)(h) LFVS,\\n(d)(i) LFVS-AM, and (e)(j) Ground-truth.\\nrandomly select 100 images for testing and used the remaining im-\\nages for training the network. Each light-ﬁeld in the dataset has\\na spatial resolution of 540×372 and an angular resolution 14×14.\\nDuring the experiment, we only consider the center 7×7 grid of\\nviews because the corner views are affected by vignetting. As a data\\naugmentation, we ﬁrst randomly crop inputs to a size of 192×192\\nthen randomly perform gamma correction using a value from the\\nrange [0.4, 1]. The pixel values are normalized to a range of [-1, 1].\\nThe optimization of the proposed network is done with the ADAM\\noptimizer where β1 and β2 are set to 0.9 and 0.99. The learning rate\\nis set to 0.0001, while the batch size is 8. The model converges af-\\nter 1000 epochs, and it takes approximately 2.5 days to train on a\\nGeForce GTX 1080 Ti GPU.\\n4. EXPERIMENTAL RESULTS\\nIn this section, we compare the proposed method with state-of-the-\\nart methods both quantitatively and qualitatively. We evaluate all the\\nmethods on two test datasets, one contains 100 light-ﬁelds of ﬂowers\\nthat we randomly select from [30]. The other test set Diverse con-\\nsists of 30 light-ﬁelds [9] – mostly outdoor scenes. Besides, we also\\nconducted ablation studies to assess the effectiveness of the various\\ncomponents of our network.\\n4.1. Comparison with the state-of-the-art\\nTo evaluate the performance of the proposed method, named LFVS-\\nAM (Light-Field View Synthesis using Attention Module), we com-\\npare it with state-of-the-art methods including Kalantari et al. [9]\\n# ResGroups\\n# CBAMs\\nPSNR (dB)\\nMS-SSIM (dB)\\n3\\n3\\n41.16\\n25.76\\n3\\n5\\n41.25\\n25.95\\n5\\n3\\n41.33\\n26.05\\nTable 2: Quantitative results for three different models with varying\\nnumber of ResGroups and CBAMs on Diverse [9] dataset.\\nand Navarro et al. [11]. For quantitative evaluation, we calculate\\nthe PSNR and MS-SSIM (dB) = −10 log10(1−MS-SSIM) values\\nfor the luminance channel of the images between the estimation and\\nthe ground-truth. Table 1 shows the PSNR and MS-SSIM results\\naveraged over all intermediate target positions – 7×7 views exclud-\\ning the 4 corner views which served as input. It is evident that the\\nproposed method produces better results compared to both state-of-\\nthe-art methods across all metrics on both the datasets. Our method\\noutperforms [9] by 2.3 dB and [11] by 0.5 dB on average. Moreover,\\nvisual results in Fig. 4 demonstrates that Kalantari et al. [9] and\\nNavarro et al. [11] failed to generate plausible results in the presence\\nof occlusions. For example in Fig. 4a and 4b, both of these methods\\nare not able to generate the pole between the two leaves, which is\\noccluded in the input views. In comparison, our method better es-\\ntimates the occluded region which is closer to the ground-truth, as\\nshown in Fig. 4d.\\n4.2. Ablation Study\\n4.2.1. Effects of hyperparameters\\nIn Table 2, we compare three variants of LFVS-AM models having a\\ndifferent number of ResGroups and CBAMs. The metrics in the table\\nare calculated for Y channel of the image. From the results, we can\\nconclude that the model with 5 ResGroup and 3 CBAM performs the\\nbest among the three models, but the differences in performance are\\nnot that signiﬁcant when the total number of CBAM blocks becomes\\nthe same i.e., (# ResGroups)×(# CBAMs)=15.\\n4.2.2. Effect of the attention mechanism\\nTo evaluate the effect of the attention mechanism in the reﬁnement\\nnetwork, we compare the proposed method with the model having\\nno attention mechanism. For the model without attention mecha-\\nnism (LFVS), we remove the CA and SA modules in Fig. 3 (a) to\\nchange the CBAM to a residual block. In Table 1, average PSNR\\nand MS-SSIM (dB) values on all intermediate rendering positions\\nare presented. The results indicate the effectiveness of the atten-\\ntion mechanism in our proposed reﬁnement network. Our LFVS\\nmodel without attention mechanism is better than SoTA because of\\nthe residual connections in the network but still can not handle occlu-\\nsions, as shown in Fig. 4c. In contrast, the attention module guides\\nthe network to efﬁciently predict the occluded regions, see Fig. 4d.\\n5. CONCLUSION\\nIn this work, we proposed a novel end-to-end CBAM light-ﬁeld view\\nsynthesis method to reconstruct dense light-ﬁelds from sparse input\\ndata. Our model consists of stereo feature extraction, disparity esti-\\nmation, and residual CBAM image reﬁnement networks. All these\\ncomponents are modelled using three sequential CNNs. We pre-\\nsented quantitative and qualitative results of the proposed method on\\ntwo different datasets. Experimental results show that our model out-\\nperforms state-of-the-art approaches by 0.5 dB on average. Future\\n4\\nwork will include to analyze the potential of our proposed method in\\nthe context of light-ﬁeld compression frameworks.\\n6. ACKNOWLEDGMENT\\nWe thank Milan Stepanov (L2S, CentraleSup´\\nelec) for his valuable\\nhelp during the re-implementation of [11]. This project has received\\nfunding from the European Union’s Horizon 2020 research and inno-\\nvation programme under the Marie Skłodowska-Curie grant agree-\\nment No. 765911 (RealVision).\\n7. REFERENCES\\n[1] Bennett Wilburn, Neel Joshi, Vaibhav Vaish, Eino-Ville Talvala, Emilio\\nAntunez, Adam Barth, Andrew Adams, Mark Horowitz, and Marc\\nLevoy,\\n“High performance imaging using large camera arrays,”\\nin\\nProc. of ACM Trans. on Graphics, pp. 765–776. July 2005, Los Ange-\\nles, California.\\n[2] MSK Gul, Thorsten Wolf, Michel B¨\\natz, Matthias Ziegler, and Joachim\\nKeinert, “A high-resolution high dynamic range light-ﬁeld dataset with\\nan application to view synthesis and tone-mapping,” in Proc. of IEEE\\nInt. Conf. on Multimedia and Expo Workshops, July, 2020, pp. 1–6,\\nLondon, UK.\\n[3] “Raytrix: Light ﬁled technology,” 2018, https://raytrix.de/technology/.\\n[4] Ren Ng, Marc Levoy, Mathieu Br´\\nedif, Gene Duval, Mark Horowitz,\\nand Pat Hanrahan, Light ﬁeld photography with a hand-held plenoptic\\ncamera, Ph.D. thesis, Stanford University, 2005.\\n[5] Matteo Poggi, Davide Pallotti, Fabio Tosi, and Stefano Mattoccia,\\n“Guided stereo matching,” in Proc. of IEEE Int. Conf. on Computer\\nVision and Pattern Recognition, June 2019, pp. 979–988, Long Beach,\\nUSA.\\n[6] MSK Gul, Michel B¨\\natz, and Joachim Keinert, “Pixel-wise conﬁdences\\nfor stereo disparities using recurrent neural networks,” in British Ma-\\nchine Vision Conf., Sep. 2019, p. 23, Cardiff, UK.\\n[7] Pengpeng Liu, Michael Lyu, Irwin King, and Jia Xu, “Selﬂow: Self-\\nsupervised learning of optical ﬂow,” in Proc. of IEEE Int. Conf. on\\nComputer Vision and Pattern Recognition, June 2019, pp. 4571–4580,\\nLong Beach, USA.\\n[8] Thomas K¨\\nohler, Michel B¨\\natz, Farzad Naderi, Andr´\\ne Kaup, Andreas\\nMaier, and Christian Riess,\\n“Toward bridging the simulated-to-real\\ngap: Benchmarking super-resolution on real data,”\\nIEEE Trans. on\\nPattern Analysis and Machine Intelligence, vol. 42, no. 11, pp. 2944–\\n2959, 2020.\\n[9] Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ramamoorthi,\\n“Learning-based view synthesis for light ﬁeld cameras,” in ACM Trans.\\non Graphics, July 2016, pp. 1–10, Anaheim, USA.\\n[10] MSK Gul and Bahadir K Gunturk,\\n“Spatial and angular resolution\\nenhancement of light ﬁelds using convolutional neural networks,” IEEE\\nTrans. on Image Processing, vol. 27, no. 5, pp. 2146–2159, 2018.\\n[11] Julia Navarro and Neus Sabater, “Learning occlusion-aware view syn-\\nthesis for light ﬁelds,” arXiv preprint arXiv:1905.11271, 2019.\\n[12] Lu Liu, Zicheng Nian, and Cheolkon Jung, “DCM-CNN: Densely con-\\nnected multiloss convolutional neural networks for light ﬁeld view syn-\\nthesis,” IEEE Access, vol. 8, pp. 78542–78552, 2020.\\n[13] Henry Wing Fung Yeung, Junhui Hou, Jie Chen, Yuk Ying Chung, and\\nXiaoming Chen, “Fast light ﬁeld reconstruction with deep coarse-to-\\nﬁne modeling of spatial-angular clues,” in Proc. of the European Conf.\\non Computer Vision, Sep. 2018, pp. 137–152, Munich, Germany.\\n[14] Yunlong Wang, Fei Liu, Zilei Wang, Guangqi Hou, Zhenan Sun, and\\nTieniu Tan, “End-to-end view synthesis for light ﬁeld imaging with\\npseudo 4DCNN,” in Proc. of the European Conf. on Computer Vision,\\nSep. 2018, pp. 333–348, Munich, Germany.\\n[15] Dingcheng Yue, MSK Gul, Michel B¨\\natz, Joachim Keinert, and Rafał\\nMantiuk,\\n“A benchmark of light ﬁeld view interpolation methods,”\\nin Proc. of IEEE Int. Conf. on Multimedia and Expo Workshops, July,\\n2020, pp. 1–6, London, UK.\\n[16] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Hong-\\ngang Zhang, Xiaogang Wang, and Xiaoou Tang, “Residual attention\\nnetwork for image classiﬁcation,” in Proc. of IEEE Int. Conf. Computer\\nVision and Pattern Recognition, July, 2017, pp. 3156–3164, Hawaii,\\nUSA.\\n[17] Jie Hu, Li Shen, and Gang Sun, “Squeeze-and-excitation networks,” in\\nProc. of IEEE Int. Conf. on Computer Vision and Pattern Recognition,\\nJuly, 2018, pp. 7132–7141, Utah, USA.\\n[18] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon,\\n“CBAM: Convolutional block attention module,” in Proc. of the Eu-\\nropean Conf. on Computer Vision, Sep. 2018, pp. 3–19, Munich, Ger-\\nmany.\\n[19] Lixin Shi, Haitham Hassanieh, Abe Davis, Dina Katabi, and Fredo Du-\\nrand, “Light ﬁeld reconstruction using sparsity in the continuous fourier\\ndomain,” ACM Trans. on Graphics, pp. 1–13, Aug. 2014, Vancouver,\\nUSA.\\n[20] Anat Levin and Fredo Durand, “Linear view synthesis using a dimen-\\nsionality gap light ﬁeld prior,” in Proc. of IEEE Int. Conf. on Computer\\nVision and Pattern Recognition, June, 2010, pp. 1831–1838, San Fran-\\ncisco, USA.\\n[21] Kshitij Marwah, Gordon Wetzstein, Yosuke Bando, and Ramesh\\nRaskar, “Compressive light ﬁeld photography using overcomplete dic-\\ntionaries and optimized projections,”\\nACM Trans. on Graphics, pp.\\n1–12, July 2013, Anaheim, USA.\\n[22] David C Schedl, Clemens Birklbauer, and Oliver Bimber, “Directional\\nsuper-resolution by means of coded sampling and guided upsampling,”\\nin Proc. of IEEE Int. Conf. on Computational Photography, April 2015,\\npp. 1–10, Houston, USA.\\n[23] David C Schedl, Clemens Birklbauer, and Oliver Bimber, “Optimized\\nsampling for view interpolation in light ﬁelds using local dictionaries,”\\nElsevier Computer Vision and Image Understanding, pp. 93–103, 2018.\\n[24] Youngjin Yoon, Hae-Gon Jeon, Donggeun Yoo, Joon-Young Lee, and\\nIn So Kweon, “Light-ﬁeld image super-resolution using convolutional\\nneural network,” IEEE Signal Processing Letters, vol. 24, no. 6, pp.\\n848–852, 2017.\\n[25] Jing Jin, Junhui Hou, Hui Yuan, and Sam Kwong, “Learning light ﬁeld\\nangular super-resolution via a geometry-aware network,” in AAAI, Feb.\\n2020, pp. 11141–11148, New York, USA.\\n[26] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah\\nSnavely, “Stereo magniﬁcation: learning view synthesis using multi-\\nplane images,” ACM Trans. on Graphics, Aug. 2018, Vancouver, USA.\\n[27] Ben\\nMildenhall,\\nPratul\\nP\\nSrinivasan,\\nRodrigo\\nOrtiz-Cayon,\\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Ab-\\nhishek Kar, “Local light ﬁeld fusion: Practical view synthesis with\\nprescriptive sampling guidelines,” ACM Trans. on Graphics, Jul. 2019,\\nLos Angeles, USA.\\n[28] John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham\\nFyffe, Ryan Overbeck, Noah Snavely, and Richard Tucker,\\n“Deep-\\nview: View synthesis with learned gradient descent,” in Proc. of IEEE\\nInt. Conf. on Computer Vision and Pattern Recognition, June 2019, pp.\\n2367–2376, Long Beach, USA.\\n[29] Keito Suzuki and Masaaki Ikehara, “Residual learning of video frame\\ninterpolation using convolutional LSTM,”\\nIEEE Access, vol. 8, pp.\\n134185–134193, 2020.\\n[30] Pratul P Srinivasan, Tongzhou Wang, Ashwin Sreelal, Ravi Ramamoor-\\nthi, and Ren Ng, “Learning to synthesize a 4d RGBD light ﬁeld from a\\nsingle image,” in Proc. of IEEE Int. Conf. Computer Vision and Pattern\\nRecognition, July, 2017, pp. 2243–2251, Hawaii, USA.\\n5\\n', metadata={'Published': '2021-05-31', 'Title': 'Light-field view synthesis using convolutional block attention module', 'Authors': 'M. Shahzeb Khan Gul, Umair Mukati, Michel Bätz, Søren Forchhammer, Joachim Keinert', 'Summary': 'Consumer light-field (LF) cameras suffer from a low or limited resolution\\nbecause of the angular-spatial trade-off. To alleviate this drawback, we\\npropose a novel learning-based approach utilizing attention mechanism to\\nsynthesize novel views of a light-field image using a sparse set of input views\\n(i.e., 4 corner views) from a camera array. In the proposed method, we divide\\nthe process into three stages, stereo-feature extraction, disparity estimation,\\nand final image refinement. We use three sequential convolutional neural\\nnetworks for each stage. A residual convolutional block attention module (CBAM)\\nis employed for final adaptive image refinement. Attention modules are helpful\\nin learning and focusing more on the important features of the image and are\\nthus sequentially applied in the channel and spatial dimensions. Experimental\\nresults show the robustness of the proposed method. Our proposed network\\noutperforms the state-of-the-art learning-based light-field view synthesis\\nmethods on two challenging real-world datasets by 0.5 dB on average.\\nFurthermore, we provide an ablation study to substantiate our findings.'}), Document(page_content='AN ENHANCED PROHIBITED ITEMS RECOGNITION MODEL\\nTianze Rong, Hongxiang Cai, Yichao Xiong\\ndominirong@gmail.com, chxlll@126.com, xyc_sjtu@163.com\\nAugust 11, 2021\\n1\\nIntroduction\\nMost of the security inspection contains prohibited items recognition. And the approach to solve the package checking\\nis to scan the package or bag to acquire the ability of looking inside to recognize and locate the prohibited items. But\\nit is high-maintenance and high-time-consuming to keep professional staff to read X-ray images and recognize the\\nitems in the images. Nowadays X-ray reading is rapid and automatic, beneﬁting from the development of deep neural\\nnetworks and computer vision.\\nWe investigated data of this ﬁeld, SIXray as a typical dataset, to probe the characteristics of X-ray images and prohibited\\nitems recognition.\\nIn this work we found the primary factor that restricts the model performance is that the object scale is so small that\\nreins the enhancing module. Then we investigated a bundle of data augmentation and enhancing modules to improve\\nthe performance of the model.\\nOur model can achieve a state of the art performance at SIXray——the best mAP of our model at SIXray10 is 0.899, at\\nSIXray100 is 0.748.\\n2\\nMethod\\n2.1\\nPerspective of Data\\n2.1.1\\nData Description\\nThe description and analysis to data can be representative of the data from similar tasks. In this case, we analyzed\\nthe statistics of SIXray dataset [1] and regard it as a typical pattern to research the bottleneck of prohibited items\\nrecognition via X-ray.\\nAmount of Images\\nSIXray has three subsets in different imbalance levels called SIXray10, SIXray100 and\\nSIXray1000. The imbalance level of SIXray1000 is almost as high as SIXray100. We only investigate the SIXray10\\nand SIXray100. Following Table 1 is the size of the dataset and its subsets.\\nSIXray10\\nSIXray100\\nTrain Set\\n74959\\n749574\\nTest Set\\n13411\\n133201\\nTotal\\n88370\\n882775\\nTable 1: Quantitative Statistics of SIXray\\nStatistics of Labels\\nLabel imbalanced is one of dominant characteristics in prohibited items detection. It is intuitive\\nto assume that prohibited items are much more rare than regular ones so that it is inevitable to estimate the level of\\nimbalance. Table 2 shows the amount of each label.\\narXiv:2102.12256v2  [cs.CV]  10 Aug 2021\\nAUGUST 11, 2021\\nGun\\nKnife\\nWrench\\nPliers\\nScissors\\nNegative\\nCounts\\n2705\\n1748\\n2012\\n3434\\n807\\n67464\\nPercentage(%)\\n3.60\\n2.33\\n2.68\\n4.58\\n1.08\\n90.0\\nTable 2: Label Distribution of SIXray on Train Set\\n2.2\\nOther Features of Dataset\\n2.2.1\\nSmall Object Recognition\\nSome of the prohibited items are fairly small-sized to be recognized so we investigated the dataset to estimate the\\nscale of the object. Fortunately, the author of SIXray has built the detection annotations. For each of bounding box we\\ncalculated the scale by the formula:\\nscale =\\np\\nwidth ∗height\\nThen we displayed the scales from different categories in their own histogram as in Figure 1 It is apparent that pliers\\nand scissors are much smaller than the other categories. The most likely scale of scissors is about 50 pixels.\\nFigure 1: Scale Histograms of Categories\\n2.2.2\\nOverlapping\\nAccording to [1], X-ray images are transparent and even the item is occluded which can also be seen. The absorption of\\nX-ray traveling through obeys Lambert-Beer’s Law which points out that the attenuation of light(X-ray included) in a\\ntransparent object is linear:\\nA = ϵlc\\nWhere A is the absorbance of light, ϵ is a coefﬁcient related to attenuating species, l is the traveling length of light, c\\nis the concentration of the attenuating species. Hence, it is natural to blend two images by linear overlay due to the\\ntransparency of X-ray images.\\n2.3\\nSolution to SIXray\\n2.3.1\\nData Augmentation\\nRandom Flipping\\nSince the X-ray image is perspective and taken from the vertical view which makes the pose of\\npackages can be variant and arbitrary. At least, the vertical ﬂipping and horizontal ﬂipping will not harm the semantics.\\nRandom Rotation\\nThe same reason with random ﬂipping but the random rotation can be more variant than random\\nﬂipping.\\nRandom Cropping\\nThe X-ray image mostly has a blank margin around the X-ray, meanwhile the object to be\\nrecognized is not placed right in the middle of the image. As we draw the random crop into the data augmentation,\\nwhether the margin or the position of the object can be a prerequisite. In addition, we sampled some image to check the\\npossibility, which is seldom occur, of cropping the object out.\\n2\\nAUGUST 11, 2021\\nFigure 2: Data Augmentation about Overlapping\\nImage Synthesis\\nSince the data is imbalanced, oversampling is a good way to ease the imbalance, what’s more,\\nadding two samples with weight can be an option beneﬁting from the transparency of X-ray image. Therefore, we use\\nthe weighted adding as an augmentation, which is called blending by us. You can see in Figure 2. The mixup is also an\\noption to enlarge the dataset capacity:\\nImageBlend = Blend(Image1, Image2, λ) = λImage1 + (1 −λ)Image2\\nHere λ is a super-parameter.\\n2.3.2\\nImbalanced Label\\nWe designed a rescoring mechanism against the imbalance between classes [2]. As we showed in Table 2, the amount of\\nnegative sample is more than any of the amount of a single category of prohibited items, which means as we decouple\\nthe imbalance between positive and negative samples and the imbalance between positive classes can somehow ease the\\nimbalance among the whole dataset. It can be regarded as a rather weak hierarchy structure.\\nTo represent the imbalance between positive sample and negative sample, we adopted the probability of positive sample\\nas the objectness score.\\nAnd based on the objectness score, we regress probability of the corresponding class by the formula:\\nP(classi) = P(classi|object) ∗P(object)\\n2.3.3\\nAttention Mechanism\\nProhibited items are mostly local and partial from the whole image. In a way, the majority of the image is uninformative\\nor should be ignored. Attention mechanisms, especially spatial attention, can lead the model to focus on the local region\\nto promote the performance of the model.\\nSpatial attention [3] is widely used as a plug and play module to promote the performance of models which distribute a\\nweight to each of the pixel height-wise and weight-wise. Prohibited items should be weighted more in this case. Channel-\\nwise attention [4] is another form of attention on feature-channel. The Convolutional Block Attention Module(CBAM)\\nis combined with the channel-wise and spatial attention.\\n3\\nExperiments\\n3.1\\nSIXray10\\n3.1.1\\nBaseline\\nBackbone\\nAn ordinary ResNet-34 [6] architecture pre-trained on ImageNet [7], but with sigmoid function to output\\nthe conﬁdence as multi-label.\\n3\\nAUGUST 11, 2021\\nFigure 3: CBAM Structure [5]\\nLoss Function\\nIntuitively we adopted the binary cross entropy loss as the loss function since this task is multi-label\\nclassiﬁcation.\\nOptimizer\\nThe optimizer is Nesterov Accelerated Gradient(NAG), learning rate is set to 0.01 without learning rate\\nscheduler. Also the momentum parameter is set to 0.9.\\nData Augmentation\\nThe input data will be random ﬂipping on both vertical and horizontal direction with a probability\\nof 0.5. Besides, all images will be resized to (224, 224).\\nTraining Procedure\\nThe batch size is 128 and training for 60 epochs. The training duration is optimal and selected\\nas the best setting. The metrics of our baseline is shown below in Figure 3.\\nAP(%)\\nGun\\nKnife\\nWrench\\nPliers\\nScissors\\nmean\\nResNet34 [1]\\n89.7\\n85.5\\n62.8\\n83.5\\n53.0\\n74.8\\nDenseNet-CHR [1]\\n87.1\\n85.9\\n70.5\\n88.3\\n66.1\\n79.6\\nBaseline(Ours)\\n89.5\\n91.7\\n76.4\\n88.5\\n66.5\\n82.3\\nTable 3: Metrics on Baseline Setting Trained on SIXray10\\n3.1.2\\nData Augmentation\\nRandom Crop\\nInstead of directly resize to (224, 224), we ﬁrstly resize the image to a size of (256, 256), then\\nrandomly cropped to (224, 224).\\nRandom Rotate\\nTo rotate the image with a random degree between (−15◦, 15◦), and resize the image to keep all\\npixels are still in the region of image.\\nMixUp\\nMixUp [8] is to mix two group images up with a partition coefﬁcient λ which is submitting to a beta\\ndistribution B(α, β). We add two different image via adding with:\\nImagei\\nblending = λImagei\\noriginal + (1 −λ)Imagei\\nshuffled\\nTheir loss is calculated by the formula:\\nLoss = λLossoriginal + (1 −λ)Lossshuffled\\nBlending\\nDue to the overlapping property, directly add two images with a constant coefﬁcient λ, hereImagei\\nshuffled\\nmeans images from a certain batch after shufﬂing:\\nImagei\\nblending = λImagei\\noriginal + (1 −λ)Imagei\\nshuffled\\ncorresponding label is:\\nlabelblending = labeloriginal|labelshuffled\\nwhere | is dimension-wise-or.\\n3.1.3\\nAttention Mechanism\\nAs stated before, the prohibited item could be aimed by attention mechanism. We employed the Convolutional Block\\nAttention Module(CBAM) [5] as the attention mechanism into our model. CBAM is an attention module with both\\nchannel-wise and pixel-wise attention. But we modiﬁed the implement from the original one, the structure is like Figure\\n4 following.\\n4\\nAUGUST 11, 2021\\nMethod\\nBaseline\\nRandom Crop\\nMixUp(0.2, 0.2)\\nMixUp(0.2, 0.2)\\nBlend(0.5)\\nmAP(%)\\n82.3\\n82.5\\n81.9\\n82.2\\n66.0\\nTable 4: Metrics on Different Data Augmentation\\nFigure 4: Implementation of CBAM Structure\\n3.1.4\\nInput Scale\\nAfter the experiment we mentioned, we found it is counter-intuitive that the normal and universal methods to promote\\nour model are all disabled. We checked most of the potential factors to check out the bottleneck. It can be clearly\\ndelivered from the baseline Table 3 and P-R curve in Figure 5 that the category of scissors has a lower performance\\nthan the others and similarly the label of pliers has the second lower performance, particularly the recall of scissors is\\nﬂopping on the curve.\\nTo associate with the statistics of objects, the label of scissors is exactly the smallest category while the pliers is second.\\nWe change the input scale into (384, 384) and (512, 512) and enlarge the crop size with equal ratio.\\n3.1.5\\nSummary\\nAfter the scale-related experiment, we found the scale of images maybe a performance bottleneck holds the accuracy\\noff. We ran a set of experiments to release the restriction and revalidate those methods that did not work before. Here is\\nsome description of notes might be used:\\n• Input scale: The edge length of images after resizing.\\n• Crop scale: The edge length of images after random cropping.\\n• Flip: Using the random ﬂip on vertical and horizontal direction with probability of 0.5.\\n• Rotation: Using the random rotation with a rotating angle between (−15◦, 15◦).\\n• Synthesis: Using the image synthesized with MixUp or blending.\\n• CBAM: Using CBAM module on classiﬁcation head.\\nFinally, we considered that the ﬁnal setting is:\\n• Scale: Input scale is (512, 512).\\nBaseline\\nCBAM\\nmAP(%)\\n82.3\\n83.0\\nTable 5: Result of CBAM\\n5\\nAUGUST 11, 2021\\nFigure 5: P-R curve of Baseline\\nInput Size\\nCrop Size\\nmAP(%)\\n256\\n224\\n82.3\\n384\\n336\\n85.8\\n512\\n448\\n87.8\\nTable 6: Result under Different Input Scale\\n• Random Crop: Cropping scale is (448, 448).\\n• Random Flip: Using the random ﬂip on vertical and horizontal direction with probability of 0.5.\\n• CBAM Module on classiﬁcation head.\\n• Mixup: Alpha and beta is 0.4.\\n3.2\\nSIXray100\\n3.2.1\\nBaseline\\nWe tested the best model trained on SIXray10 and a model whose setting is inherited from the best SIXray10 but trained\\non SIXray 100.\\n3.2.2\\nRescoring\\nBy analyzing the results of baseline on SIXray100, the set trained on SIXray100 is even worse than SIXray10 against\\nthe common sense that the bigger the data is, the better the model works. Furthermore the variant is controlled it is\\nreasonable to believe that the model degradation is due to the higher imbalance level. The solution we designed is the\\nrescoring mechanism.\\nWe modiﬁed the output layer of the FC-layer to adapt the rescoring mechanism.\\n6\\nAUGUST 11, 2021\\nInput Scale\\nCrop Scale\\nFlip\\nRotation\\nSynthesis\\nCBAM\\nmAP\\n224\\n✓\\n82.3\\n256\\n224\\n✓\\n82.5\\n256\\n224\\n✓\\n✓\\n83.0\\n256\\n224\\n✓\\n✓\\n81.8\\n256\\n224\\n✓\\nMixUp(0.2, 0.2)\\n81.9\\n256\\n224\\n✓\\nMixUp(0.4, 0.4)\\n82.2\\n256\\n224\\n✓\\nBlend(0.5)\\n66.0\\n384\\n336\\n✓\\n85.8\\n512\\n448\\n✓\\n87.8\\n512\\n448\\n✓\\n✓\\n88.7\\n512\\n448\\n✓\\n✓\\n88.0\\n512\\n448\\n✓\\nMixUp(0.4, 0.4)\\n89.9\\n512\\n448\\n✓\\nBlend(0.5)\\n86.5\\n512\\n448\\n✓\\n✓\\nMixUp(0.4, 0.4)\\n86.5\\n512\\n448\\n✓\\nMixUp(0.4, 0.4)\\n✓\\n89.9\\nTable 7: Results and Conditions of All Experiment\\nAP(%)\\nGun\\nKnife\\nWrench\\nPliers\\nScissors\\nmean\\nResNet34 [1]\\n83.1\\n78.8\\n30.5\\n55.2\\n16.1\\n52.7\\nDenseNet-CHR [1]\\n82.1\\n78.8\\n43.2\\n66.8\\n28.8\\n60.0\\nTrained on SIXray100\\n82.0\\n85.8\\n64.4\\n77.1\\n53.4\\n72.6\\nTrained on SIXray10\\n85.1\\n86.8\\n61.4\\n77.1\\n57.0\\n73.5\\nTable 8: Metrics on Baseline Setting Trained on SIXray100\\nAccording to the label of SIXray the dimension of the output layer should be 5, equal to the number of categories. We\\nmodiﬁed the dimension into 6. The surplus is the objectness, which is to predict the probability if there is a prohibited\\nitem in the image but without classiﬁcation. The consequent probability of each categories is the ﬁve components\\nmultiply with the objectness as the formula we mentioned before:\\nP(classi) = P(classi|object) ∗P(object)\\nAP(%)\\nGun\\nKnife\\nWrench\\nPliers\\nScissors\\nmean\\nTrained on SIXray10\\n85.1\\n86.8\\n61.4\\n77.1\\n57.0\\n73.5\\nrescoring\\n87.4\\n86.6\\n61.6\\n80.1\\n58.4\\n74.8\\nTable 9: Metrics of Rescoring Mechanism\\n4\\nConclusion\\nConclusively, we adopt a input scale of 512, crop scale of 448, and with random ﬂip, CBAM and mix up whose alpha\\nand beta is 0.4 as basic conﬁguration can achieve a mAP of 89.9% on SIXray10. As for SIXray100 need rescoring\\nmechanism additionally, which achieve a mAP of 74.8% on SIXray100.\\nReferences\\n[1] Caijing Miao, Lingxi Xie, Fang Wan, Chi Su, Hongye Liu, Jianbin Jiao, and Qixiang Ye. Sixray: A large-scale\\nsecurity inspection x-ray benchmark for prohibited item discovery in overlapping images. In 2019 IEEE/CVF\\nConference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n[2] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. In IEEE Conference on Computer Vision &\\nPattern Recognition, pages 6517–6525, 2017.\\n7\\nAUGUST 11, 2021\\n[3] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discriminative localization.\\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2921–2929, 2016.\\n[4] Jie Hu, Li Shen, Gang Sun, and Samuel Albanie. Squeeze-and-excitation networks. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, PP(99), 2017.\\n[5] Sanghyun Woo, Jongchan Park, Joon Young Lee, and In So Kweon. Cbam: Convolutional block attention module.\\n2018.\\n[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The\\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: a large-scale hierarchical image\\ndatabase. pages 248–255, 06 2009.\\n[8] Hongyi Zhang, Moustapha Cisse, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization.\\n10 2017.\\n8\\n', metadata={'Published': '2021-08-10', 'Title': 'An Enhanced Prohibited Items Recognition Model', 'Authors': 'Tianze Rong, Hongxiang Cai, Yichao Xiong', 'Summary': 'We proposed a new modeling method to promote the performance of prohibited\\nitems recognition via X-ray image. We analyzed the characteristics of\\nprohibited items and X-ray images. We found the fact that the scales of some\\nitems are too small to be recognized which encumber the model performance. Then\\nwe adopted a set of data augmentation and modified the model to adapt the field\\nof prohibited items recognition. The Convolutional Block Attention Module(CBAM)\\nand rescoring mechanism has been assembled into the model. By the modification,\\nour model achieved a mAP of 89.9% on SIXray10, mAP of 74.8%.'}), Document(page_content='Enhancing Ship Classification in Optical Satellite Imagery: \\nIntegrating Convolutional Block Attention Module with ResNet \\nfor Improved Performance \\nRunning head: Ship Classification with Enhanced CBAM-ResNet (ECBAM-RN) \\nRyan Donghan Kwon1*, Member, IEEE, Gangjoo Robin Nam2, Jisoo Tak3, \\nJunseob Shin4, Hyerin Cha5, Yeom Hyeok6  and Seung Won Lee3* \\n1Department of Science and Informatics, Hana Academy Seoul, Seoul, 03305 South Korea \\n2Department of Research and Development, The Coala Inc., Seoul, 06190 South Korea \\n3Sungkyunkwan University School of Medicine, Gyeonggi-do, 16419 South Korea \\n4Korea Science Academy of KAIST, Busan, 47162 South Korea \\n5Seoul National University School of Earth and Environ. Sciences, Seoul, 08826 South Korea \\n6Drone Defense System Development Team, Daeji P&I, Gyeonggi-do, 14148 South Korea \\n \\n* Correspondence: \\nryankwon@ieee.org ; lsw2920@gmail.com  \\nKeywords: Convolutional Neural Network, Satellite Imagery Classification, \\nConvolutional Block Attention Module, Ship Classification, ResNet, Optical Satellite \\nImagery, Attention Mechanisms, Multi-scale Feature Integration, Depthwise Separable \\nConvolutions, Dilated Convolutions \\nAbstract \\nThis study presents an advanced Convolutional Neural Network (CNN) architecture for ship \\nclassification from optical satellite imagery, significantly enhancing performance through the \\nintegration of the Convolutional Block Attention Module (CBAM) and additional architectural \\ninnovations. Building upon the foundational ResNet50 model, we first incorporated a standard \\nCBAM to direct the model\\'s focus towards more informative features, achieving an accuracy \\nof 87% compared to the baseline ResNet50\\'s 85%. Further augmentations involved multi-scale \\nfeature integration, depthwise separable convolutions, and dilated convolutions, culminating \\nin the Enhanced ResNet Model with Improved CBAM. This model demonstrated a remarkable \\naccuracy of 95%, with precision, recall, and f1-scores all witnessing substantial improvements \\nacross various ship classes. The bulk carrier and oil tanker classes, in particular, showcased \\nnearly perfect precision and recall rates, underscoring the model\\'s enhanced capability in \\naccurately identifying and classifying ships. Attention heatmap analyses further validated the \\nimproved model\\'s efficacy, revealing a more focused attention on relevant ship features, \\nregardless of background complexities. These findings underscore the potential of integrating \\nattention mechanisms and architectural innovations in CNNs for high-resolution satellite \\nimagery classification. The study navigates through the challenges of class imbalance and \\ncomputational costs, proposing future directions towards scalability and adaptability in new or \\nrare ship type recognition. This research lays a groundwork for the application of advanced \\ndeep learning techniques in the domain of remote sensing, offering insights into scalable and \\nefficient satellite image classification. \\n1 \\nIntroduction \\nThe advent of high-resolution optical satellite imagery has revolutionized the way we observe \\nand understand the Earth\\'s surface, offering unprecedented opportunities for monitoring, \\nanalysis, and decision-making across a wide range of applications. Among these applications, \\nship classification plays a crucial role in maritime surveillance, traffic management, and \\nenvironmental monitoring, providing essential information for ensuring maritime safety, \\noptimizing shipping routes, and preventing illegal activities at sea. With the proliferation of \\noptical remote sensing (ORS) technologies, the ability to accurately classify ships from satellite \\nimages has become increasingly important. However, the task is challenging due to the \\ncomplex and dynamic nature of the maritime environment, including variations in ship size, \\ntype, and appearance, as well as the influence of sea state, weather conditions, and image \\nresolution. \\nIn recent years, Convolutional Neural Networks (CNNs) have emerged as a powerful tool for \\nimage analysis, demonstrating remarkable success in various fields, including image \\nrecognition, object detection, and classification tasks. The deep learning architecture of CNNs \\nenables them to learn hierarchical feature representations from data, making them particularly \\nwell-suited for extracting and classifying intricate patterns in images. This capability has led \\nto the exploration of CNN models for ship classification in optical satellite imagery, aiming to \\nleverage their potential for automatic and accurate identification of ship types from space. \\nResidual Networks (ResNets) represent a significant advancement in CNN architecture, \\nintroducing the concept of residual learning to address the challenges of training very deep \\nneural networks. By integrating shortcut connections that allow layers to learn residual \\nfunctions with reference to the input, ResNets facilitate the training of deeper networks, thereby \\nenhancing their capacity for feature representation and improving classification performance. \\nThe introduction of ResNet has marked a milestone in deep learning, setting new standards for \\nimage recognition and classification tasks [1]. \\nBuilding upon the foundation of CNNs and ResNets, the Convolutional Block Attention \\nModule (CBAM) introduces an attention mechanism that further refines the feature maps \\nproduced by CNNs. By sequentially applying attention across both spatial and channel \\ndimensions, CBAM enables the network to focus on the most informative features, thereby \\nimproving the efficiency and effectiveness of feature representation. This attention-based \\napproach has shown to enhance the performance of CNN architectures across a range of \\nclassification and detection tasks, demonstrating the potential of integrating attention \\nmechanisms in deep learning models for image analysis [2]. \\nThis study proposes a novel framework for ship classification using transfer learning on optical \\nsatellite imagery, integrating ResNet with the Convolutional Block Attention Module (CBAM) \\nto leverage the strengths of both architectures. By combining the depth and residual learning \\ncapability of ResNet with the selective attention mechanism of CBAM, our approach aims to \\nachieve superior classification performance, addressing the complexities of ship identification \\nin high-resolution satellite images. Furthermore, we explore the integration of additional \\nenhancements, including multi-scale feature integration, depthwise separable convolutions, \\nand dilated convolutions, to further improve the model\\'s capability to capture and classify \\ndiverse ship types accurately. Our research contributes to the ongoing efforts to harness the \\npower of deep learning for advanced image analysis, offering insights and methodologies that \\ncan be applied to a wide range of applications beyond maritime surveillance. \\n2 \\nRelated Work \\nThe classification of ships using optical satellite imagery has been an area of growing interest \\nin the remote sensing community, especially with the advent of high-resolution imagery and \\nadvanced convolutional neural networks (CNNs). Several studies have explored different CNN \\narchitectures and methodologies to enhance the accuracy and efficiency of ship classification \\nfrom satellite imagery. This section reviews related work in the domain of ship classification \\nusing CNN models, highlighting significant contributions and methodologies that have shaped \\ncurrent research directions. \\nMorgan [3] employed fine-tuning of state-of-the-art CNN architectures for ship detection in \\nremote sensing optical imagery, achieving a notable accuracy of 94.8% with a fine-tuned \\nXception model. This study underscored the potential of leveraging pre-existing CNN \\narchitectures, adapted through fine-tuning, to meet the specific demands of ship classification \\ntasks. Similarly, Ma et al. [4] developed a novel CNN model tailored for marine target \\nclassification using GF-3 SAR images, demonstrating its effectiveness on a patch level and \\nproviding a comprehensive scheme for marine target detection in large-scale SAR images. This \\nwork exemplifies the adaptability of CNN models to different types of satellite imagery, \\nincluding synthetic aperture radar (SAR), for ship classification. \\nIn a different approach, Gadamsetty et al. [5] introduced a deep learning methodology \\nincorporating hashing with SHA-256 to enhance model integrity and security, facilitating the \\nsecure transmission of confidential images with detected ships in satellite imagery. This \\ninnovative method highlights the importance of not only classification accuracy but also data \\nsecurity in remote sensing applications. Venkataramani et al. [6] trained a CNN-based image \\nsegmentation architecture on harmonized optical and SAR satellite imagery, effectively \\nreducing false positives and negatives in water/not-water classification. Though not directly \\nfocused on ship classification, this study demonstrates the utility of CNNs in distinguishing \\ncomplex features in mixed data sources. \\nTienin et al. [7] introduced a Spatial–Channel Attention with Bilinear Pooling Network \\n(SCABPNet), leveraging a unique dataset combining SAR and optical satellite imagery to \\nenhance feature representation and achieve superior classification performance. The \\nintegration of attention mechanisms within CNN architectures, as demonstrated in SCABPNet, \\nillustrates a promising direction for enhancing model sensitivity to relevant features in ship \\nclassification tasks. DenseNet-161 was used by Sola et al. [8] for identifying sea ice ridging in \\nSAR imagery, achieving a ROC-AUC score of 92.3 and outperforming previous methods. This \\napplication of DenseNet to SAR imagery for detecting specific physical phenomena further \\nunderscores the versatility of CNN models in remote sensing. \\nSong et al. [9] proposed a hierarchical object detection method using an improved saliency \\ndetection model to probe suspected regions in complex remote sensing images, followed by an \\nefficient neural network for precise object categorization and localization. This hierarchical \\napproach, aimed at improving recall and precision rates for ship detection, showcases the \\npotential of combining saliency detection with CNN models to tackle the challenges of object \\ndetection in large-scale optical satellite imagery. \\nThese studies collectively highlight the diverse methodologies and advancements in CNN-\\nbased ship classification and detection from optical and SAR satellite imagery. They reflect the \\nongoing efforts to refine CNN architectures, integrate novel data sources, and address the \\ncomplexities of remote sensing applications, setting the stage for future research in this rapidly \\nevolving field. \\n3 \\nMethodology \\n3.1. Dataset Preparation and Preprocessing \\nThe cornerstone of our study on ship classification using Convolutional Neural Networks \\n(CNNs) is the Optical Remote Sensing (ORS) ship dataset. This dataset comprises eight distinct \\nship classes: bulk carrier, car carrier, cargo, chemical tanker, container, dredge, oil tanker, and \\ntug, encompassing a total of 8678 images. These images have been meticulously collected via \\nGoogle Earth, ensuring sub-meter resolution to capture the intricate details of the ships. \\nGiven the diverse and comprehensive nature of the ORS dataset, it presents an ideal test bed \\nfor evaluating the efficacy of deep learning models in classifying high-resolution satellite \\nimagery into meaningful ship categories. However, to optimize the dataset for our experimental \\nframework, certain preparatory and preprocessing steps were necessary. \\nPreprocessing Steps: \\n1. Data Cleaning: The initial step involved removing any potential anomalies or \\nirrelevant entries from the dataset. This included verifying the integrity of image data \\nand ensuring that each class label accurately reflected the contents of the image. \\n2. Dataset Segmentation: Each image within the dataset was carefully segmented to \\nensure that it contained only one ship. This segmentation process was critical for \\nminimizing background noise and focusing the model\\'s learning on the ships \\nthemselves, thereby reducing the complexity of the classification task. \\n3. Class Selection: Given the uneven distribution of images across the eight ship classes, \\na decision was made to exclude classes where the test set, based on an 80:20 train-test \\nsplit, would contain 100 or fewer images. This criterion was applied to maintain a \\nbalance in the dataset and ensure that the models were trained and tested on sufficiently \\nrepresentative samples from each class. \\n4. Train-Test Split: The dataset was split into training and testing sets following an 80:20 \\nratio, ensuring that the models had access to a diverse and comprehensive set of images \\nfor learning while reserving a substantial subset for evaluation purposes. \\n5. Image Resizing and Normalization: To accommodate the input requirements of the \\nCNN models, all images were resized to a uniform dimension of 224x224 pixels. This \\nresizing step was accompanied by normalization, adjusting pixel values to a common \\nscale to facilitate more stable and faster convergence during model training. \\n6. Data Augmentation: To further enhance the model\\'s robustness and ability to \\ngeneralize across various imaging conditions, data augmentation techniques such as \\nrandom horizontal flips, random vertical flips, and random rotations were applied. This \\nstep not only expanded the diversity of the training data but also simulated a broader \\nrange of imaging scenarios that the model might encounter in real-world applications. \\nThrough these preprocessing steps, we aimed to create a well-structured, balanced, and \\ncomprehensive dataset that would enable the effective training and evaluation of our CNN \\nmodels for ship classification. The meticulous preparation of the ORS dataset underscores our \\ncommitment to leveraging high-quality, real-world data to advance the state of the art in \\nmaritime object detection and classification using optical satellite imagery. \\n3.2. Model Architecture and Training \\nOur research aims to push the boundaries of ship classification from optical satellite imagery \\nthrough the deployment of advanced CNN models. The core of our methodology lies in \\nleveraging a modified ResNet50 architecture, augmented with the Convolutional Block \\nAttention Module (CBAM), and further enhanced with additional architectural innovations \\nsuch as multi-scale feature integration, depthwise separable convolutions, and dilated \\nconvolutions. These modifications are designed to increase the model\\'s accuracy and efficiency, \\nenabling it to capture a wide range of features from complex satellite images. \\n3.2.1. ResNet50-based Transfer Learning \\nWe commenced with the ResNet50 model, renowned for its residual learning framework, \\nwhich is crucial for training very deep networks. By incorporating shortcut connections, \\nResNet50 facilitates the training process, allowing the network to learn residual functions with \\nreference to the input, rather than unreferenced functions. This foundational model was chosen \\nfor its proven track record in image recognition tasks, providing a solid basis for our \\nmodifications. \\n3.2.2. Convolutional Block Attention Module (CBAM) \\nTo further refine the feature maps produced by ResNet50, we integrated the Convolutional \\nBlock Attention Module (CBAM) into our architecture. CBAM systematically infers attention \\nmaps along both channel and spatial dimensions, enabling the network to focus adaptively on \\nmore informative features. This attention mechanism enhances the model\\'s interpretability and \\nefficiency in processing satellite imagery, leading to improved classification performance. \\n3.2.3. Additional Architectural Innovations \\n• \\nMulti-scale Feature Integration: Inspired by the work of Bi et al. [10] and others, we \\nemployed multi-scale depthwise separable dilated convolutions to capture features at \\nvarious scales and resolutions effectively. This approach is particularly advantageous \\nfor satellite imagery, where ships can appear at different sizes and orientations. \\n• \\nDepthwise Separable Convolutions: Following the insights from Tinega et al. [11] \\nand Wang et al. [12], we utilized depthwise separable convolutions within our \\narchitecture. This technique reduces the computational cost while maintaining the \\nmodel\\'s ability to learn complex features, making it ideal for processing large datasets \\nof satellite images. \\n• \\nDilated Convolutions: To expand the model\\'s receptive field without increasing the \\nnumber of parameters significantly, we incorporated dilated convolutions as suggested \\nby Wu et al. [13]. This allows for a broader context to be considered when classifying \\neach pixel, enhancing the model\\'s sensitivity to spatial relationships and features \\nindicative of specific ship types. \\n3.2.4. Training and Evaluation \\nThe training process was conducted using a dataset split into training and testing subsets, with \\nimages resized to 224x224 pixels and normalized. We employed data augmentation techniques, \\nsuch as random horizontal flips and rotations, to increase the diversity of the training data. The \\nmodel was trained using cross-entropy loss and optimized with the Adam optimizer, with a \\nlearning rate set to 1e-4 and adjusted dynamically based on performance metrics. \\nThe effectiveness of the proposed modifications was evaluated through rigorous testing, \\nincluding quantitative assessments of classification accuracy, precision, recall, and F1 scores. \\nComparative analysis against baseline models and state-of-the-art approaches in ship \\nclassification demonstrated the superiority of our model in handling the complexities of \\nsatellite imagery. \\n3.3. Evaluation and Performance Analysis \\nIn the pursuit of advancing ship classification from optical satellite imagery, the efficacy of our \\nproposed CNN models was rigorously assessed through a comprehensive evaluation and \\nperformance analysis framework. Our methodology for evaluation is grounded in a \\ncomparative analysis, juxtaposing the baseline ResNet50 model, ResNet50 integrated with the \\nConvolutional Block Attention Module (CBAM) in its standard form, and our enhanced \\nResNet model featuring an improved CBAM with additional architectural innovations. This \\ncomparative approach allowed us to meticulously gauge the incremental benefits introduced \\nby each modification, thereby validating the effectiveness of our proposed enhancements. \\n3.3.1. Evaluation Metrics \\nThe performance of each model variant was evaluated against a set of widely recognized \\nmetrics in the domain of image classification: \\n• \\nAccuracy: The proportion of correctly classified instances out of the total number of \\ninstances, providing a high-level view of the model\\'s overall performance. \\n• \\nPrecision and Recall: These metrics offer insights into the model\\'s ability to correctly \\nidentify positive instances while minimizing false positives, essential for applications \\nrequiring high confidence in classification results. \\n• \\nF1-Score: The harmonic mean of precision and recall, offering a balanced measure that \\nconsiders both the precision and the recall of the model. It is particularly useful when \\ndealing with imbalanced datasets. \\n• \\nConfusion Matrix: A detailed breakdown of the model\\'s performance across different \\nclasses, providing insights into specific areas of strength and weakness in classification. \\n3.3.2. Comparative Analysis Methodology \\nThe comparative analysis was structured as follows: \\n1. Baseline Model (ResNet50): Initially, the performance of the unmodified ResNet50 \\nmodel served as our baseline. This allowed us to establish a reference point for \\nassessing the incremental improvements brought about by the integration of CBAM \\nand our subsequent enhancements. \\n2. ResNet50 + Standard CBAM: The second phase involved evaluating a ResNet50 \\nmodel integrated with a standard CBAM. This step aimed to quantify the impact of \\nadding an attention mechanism to the baseline model, focusing on how attention to \\nspatial and channel features could improve classification accuracy. \\n3. Enhanced ResNet Model with Improved CBAM: Finally, we assessed our enhanced \\nResNet model, which not only incorporated the CBAM but also introduced additional \\narchitectural innovations aimed at optimizing the model for satellite imagery \\nclassification. This included multi-scale feature integration, depthwise separable \\nconvolutions, and dilated convolutions, among others. \\n3.3.3. Performance Analysis Process \\nFor each model variant, we conducted a series of experiments under identical conditions to \\nensure fairness in comparison. The ORS dataset, prepared and preprocessed as described in the \\nprevious sections, served as the basis for training and testing across all models. Data \\naugmentation techniques were consistently applied to mitigate overfitting and enhance the \\nmodels\\' generalization capabilities. \\nFollowing training, each model was subjected to evaluation using the designated testing dataset, \\nwith the results captured across the specified metrics. This process facilitated a nuanced \\nunderstanding of each model\\'s capabilities and limitations, particularly in the context of ship \\nclassification from high-resolution optical satellite imagery. \\n4 \\nExperiments and Results \\n4.1. Experimental Setup \\nIn the pursuit of advancing ship classification through convolutional neural networks (CNNs), \\na rigorous experimental framework was established to assess the performance of our proposed \\nmodels. This section delineates the specifics of our experimental setup, ensuring \\nreproducibility and providing clarity on the conditions under which our experiments were \\nconducted. \\n4.1.1. Dataset and Preprocessing \\nThe experiments utilized the Optical Remote Sensing (ORS) ship dataset, meticulously \\nprepared as described previously. This dataset encompasses a variety of ship types, each \\nrepresented in high-resolution satellite imagery. The final dataset, after preprocessing, included \\nimages across four main ship classes, with data augmentation techniques applied to enhance \\nmodel robustness and generalizability. The dataset was split into training and testing sets with \\nan 80:20 ratio, ensuring a comprehensive evaluation of model performance. \\n4.1.2. Hardware and Software Configuration \\nThe experiments were executed on a high-performance computing cluster equipped with the \\nfollowing specifications to accommodate the computational demands of training deep learning \\nmodels: \\n• \\nCPU: Utilization of an AMD EPYC 7R32 processor to efficiently manage dataset \\nloading, preprocessing tasks, and model training orchestration. \\n• \\nGPU: Deployment of an NVIDIA A10G graphics processing unit, offering substantial \\ncomputational power and 24GB memory to facilitate rapid model training and \\nexperimentation. \\n• \\nMemory: Access to 128 GB of system memory, ensuring sufficient capacity for \\nhandling large datasets, model parameters, and intermediate computations. \\n• \\nOperating System: Utilization of Ubuntu 20.04.1, a stable and widely supported Linux \\ndistribution, ensuring a reliable and consistent software environment. \\n4.1.3. Deep Learning Framework \\nOur models were developed and trained using PyTorch, a flexible and powerful deep learning \\nlibrary favored for its dynamic computation graph and extensive model development \\necosystem. PyTorch\\'s support for CUDA enabled the leveraging of GPU acceleration, \\nsignificantly enhancing training efficiency and experiment throughput. \\n4.1.4. Evaluation Metrics \\nModel performance was quantitatively evaluated using a suite of metrics, including accuracy, \\nprecision, recall, F1-score, and the construction of confusion matrices. These metrics were \\nchosen for their ability to provide a comprehensive overview of model efficacy, from overall \\naccuracy to the balance between precision and recall. \\n4.2. Training Details \\nTo ensure a fair and thorough evaluation of each model\\'s performance on the ship classification \\ntask using the Optical Remote Sensing (ORS) dataset, we meticulously designed the training \\nregimen. This section outlines the training details, including the specific configurations and \\nmethodologies employed for each of the model variants under investigation: the baseline \\nResNet50, ResNet50 integrated with a standard Convolutional Block Attention Module \\n(CBAM), and our enhanced ResNet model with an improved CBAM and additional \\narchitectural innovations. \\n4.2.1. Training Configuration \\nAll models were trained using the following unified configuration to facilitate an equitable \\ncomparison: \\n• \\nBatch Size: Set to 128, this size was chosen to balance the trade-off between memory \\nutilization and the stability of gradient updates. \\n• \\nEpochs: Each model was trained for a total of 30 epochs. This duration was determined \\nto be sufficient for the models to converge based on preliminary experiments. \\n• \\nLearning Rate: Initiated at 1e-4 for all models. This learning rate was selected to \\nensure steady convergence without overshooting the minima in the loss landscape. \\n• \\nOptimizer: The Adam optimizer was utilized for its adaptive learning rate capabilities, \\naiding in the efficient convergence of the models. \\n• \\nLearning Rate Scheduler: A step decay learning rate scheduler was employed, \\nreducing the learning rate by a factor of 0.1 every 10 epochs to fine-tune the models as \\nthey approached convergence. \\n• \\nLoss Function: Cross-entropy loss was used as the primary criterion for training due \\nto its effectiveness in classification tasks and its ability to handle multiple classes \\nsmoothly. \\n4.2.2. Data Augmentation \\nTo enhance the models\\' ability to generalize to unseen data and to mitigate the risk of \\noverfitting, data augmentation techniques were applied to the training dataset. These included \\nrandom horizontal flips, random rotations within a range of ±10 degrees, and random vertical \\nflips. These transformations introduced variability into the training process, simulating a wider \\nrange of imaging conditions without requiring additional labeled data. \\n4.2.3. Training Environment \\nTraining was conducted using PyTorch on a high-performance computing setup, as detailed in \\nthe \"Experimental Setup\" section. The utilization of an NVIDIA A10G GPU allowed for \\nefficient training, significantly reducing the time required to train each model variant. \\n4.2.4. Model Evaluation \\nModel performance was continuously monitored throughout the training process using a \\nvalidation set carved out from the original training data. This validation set, comprising 20% \\nof the training data, enabled the early identification of overfitting and the assessment of model \\ngeneralizability. Model checkpoints were saved at the end of each epoch, with the final model \\nselection based on the best performance on the validation set across all epochs. \\n4.3. Performance of ResNet50 with Standard CBAM \\nIntegrating the Convolutional Block Attention Module (CBAM) into the ResNet50 architecture \\nmarked a significant step towards enhancing the model\\'s ability to focus on relevant features \\nfor ship classification. The addition of CBAM improved the model\\'s precision, recall, and f1-\\nscore across all ship classes when compared to the baseline ResNet50 model. Specifically, the \\nprecision and recall for the bulk carrier class saw noticeable improvements, indicating an \\nenhanced capability in distinguishing this class from others. The overall accuracy of the model \\nincreased to 0.87, up from 0.85 in the baseline model, underscoring the effectiveness of \\nincorporating attention mechanisms into CNNs for satellite imagery classification (Figure 1). \\nThe weighted average precision and recall across all classes also saw improvements, affirming \\nthe model\\'s balanced performance enhancement. \\n \\nPrecision Recall \\nF1-Score \\nSupport \\nBulk Carrier \\n0.83 \\n0.81 \\n0.82 \\n411 \\nCargo \\n0.93 \\n0.90 \\n0.91 \\n308 \\nContainer \\n0.85 \\n0.76 \\n0.80 \\n258 \\nOil Tanker \\n0.88 \\n0.94 \\n0.91 \\n691 \\n Accuracy \\n \\n \\n0.87 \\n1668 \\nMacro Avg. \\n0.87 \\n0.85 \\n0.86 \\n1668 \\nWeighted Avg. \\n0.87 \\n0.87 \\n0.87 \\n1668 \\n[Table] Figure 1. Enhanced Ship Classification Performance with CBAM in ResNet50 \\n4.4. Enhanced ResNet Model with Improved CBAM Performance \\nThe Enhanced ResNet Model with Improved CBAM demonstrated a notable leap in \\nperformance metrics, showcasing the substantial impact of the additional architectural \\ninnovations. The model achieved a remarkable overall accuracy of 0.95, with significant \\nimprovements in precision and recall across all ship classes. The bulk carrier and oil tanker \\nclasses, in particular, exhibited nearly perfect precision and recall, indicating the model\\'s \\nexceptional ability to identify and classify these ships accurately. The macro and weighted \\naverages for precision, recall, and f1-score exceeded those of both the baseline ResNet50 and \\nthe ResNet50 with standard CBAM models, highlighting the value of our enhancements in \\ndealing with the intricacies of satellite-based ship classification (Figure 2). \\nThe comparative analysis of these models elucidates the incremental benefits conferred by each \\nlayer of modification. The Enhanced ResNet Model with Improved CBAM not only capitalized \\non the strengths of CBAM but also leveraged multi-scale feature integration, depthwise \\nseparable convolutions, and dilated convolutions to address the specific challenges posed by \\nhigh-resolution satellite imagery. This comprehensive approach resulted in a model that not \\nonly excels in classification accuracy but also demonstrates a nuanced understanding of the \\nspatial and feature-specific complexities inherent in satellite images of maritime vessels. \\n \\nPrecision Recall \\nF1-Score \\nSupport \\nBulk Carrier \\n0.94 \\n0.95 \\n0.94 \\n405 \\nCargo \\n0.94 \\n0.93 \\n0.94 \\n330 \\nContainer \\n0.91 \\n0.90 \\n0.90 \\n254 \\nOil Tanker \\n0.98 \\n0.98 \\n0.98 \\n679 \\n Accuracy \\n \\n \\n0.95 \\n1668 \\nMacro Avg. \\n0.94 \\n0.94 \\n0.94 \\n1668 \\nWeighted Avg. \\n0.95 \\n0.95 \\n0.95 \\n1668 \\n[Table] Figure 2. The Enhanced ResNet Model Achieves a 0.95 Accuracy, Showcasing \\nExceptional Precision and Recall in Ship Classification \\n4.4.1. Attention Heatmaps Analysis \\nTo further understand the impact of the Enhanced CBAM on model performance, attention \\nheatmaps were generated for a subset of the test images. These heatmaps provide visual \\nevidence of the model\\'s focus areas, revealing that the improved CBAM enables the model to \\nconcentrate more effectively on the ships within the images, irrespective of their size, \\norientation, or background complexity. The heatmaps underscore the model\\'s capability to \\ndiscern subtle features indicative of different ship classes, a testament to the enhanced attention \\nmechanism\\'s efficacy (Figure 3). \\n \\nFigure 3. Heatmaps Highlight the Enhanced CBAM\\'s Ability to Focus on Key \\nFeatures for Accurate Ship Classification \\n4.5. Figures and Confusion Matrices \\nThe performance distinctions between the models are visually represented in the graphs and \\nconfusion matrices provided. These figures illustrate not only the quantitative advancements \\nachieved by each subsequent model iteration but also the qualitative improvements in the \\nmodel\\'s classification logic and attention to detail. The confusion matrices, in particular, \\nhighlight the reduction in misclassifications and the models\\' increasing reliability in correctly \\nidentifying ship types, with the Enhanced ResNet Model with Improved CBAM showcasing \\nthe most significant advancements (Figures 4, 5, and 6). \\n \\nFigure 4. Confusion Matrix of ResNet50 Baseline Model \\n \\nFigure 5. Confusion Matrix of ResNet50 + Standard CBAM \\n \\nFigure 6. Confusion Matrix of Enhanced ResNet Model with Improved CBAM \\n5 \\nDiscussion of Findings and Limitations \\nThe experimental results present a compelling narrative about the effectiveness of integrating \\nattention mechanisms and architectural innovations in CNN models for the classification of \\nships from optical satellite imagery. The Enhanced ResNet Model with Improved CBAM \\nnotably outperformed the baseline ResNet50 and the ResNet50 integrated with a standard \\nCBAM in terms of accuracy, precision, recall, and f1-score across all ship classes. This \\nimprovement underscores the value of attention mechanisms, such as CBAM, which enable \\nthe model to focus on the most informative features of an image, thus improving classification \\nperformance. Furthermore, the incorporation of multi-scale feature integration, depthwise \\nseparable convolutions, and dilated convolutions addressed the specific challenges posed by \\nhigh-resolution satellite images, such as scale variation and complex backgrounds. \\nThe attention heatmap analysis provided additional insights into the model\\'s performance, \\nvisually demonstrating how the improved CBAM guides the model to focus on relevant \\nfeatures for classification. This capability is critical for the accurate classification of ships in \\nsatellite imagery, where the presence of diverse features and noise can significantly affect \\nperformance. \\n5.1. Limitations \\nThe experimental results present a compelling narrative about the effectiveness of integrating \\nattention mechanisms and architectural innovations in CNN models for the classification of \\nships from optical satellite imagery. The Enhanced ResNet Model with Improved CBAM \\nnotably outperformed the baseline ResNet50 and the ResNet50 integrated with a standard \\nCBAM in terms of accuracy, precision, recall, and f1-score across all ship classes. This \\nimprovement underscores the value of attention mechanisms, such as CBAM, which enable \\nthe model to focus on the most informative features of an image, thus improving classification \\nperformance. Furthermore, the incorporation of multi-scale feature integration, depthwise \\nseparable convolutions, and dilated convolutions addressed the specific challenges posed by \\nhigh-resolution satellite images, such as scale variation and complex backgrounds. \\nThe attention heatmap analysis provided additional insights into the model\\'s performance, \\nvisually demonstrating how the improved CBAM guides the model to focus on relevant \\nfeatures for classification. This capability is critical for the accurate classification of ships in \\nsatellite imagery, where the presence of diverse features and noise can significantly affect \\nperformance. \\n5.2. Challenges \\nA recurring challenge in satellite image classification is the variability in image quality and \\nconditions, such as lighting, weather, and seasonal changes. While the proposed model \\ndemonstrates robustness against these variations, further work is needed to enhance its \\nadaptability to extreme conditions. \\nMoreover, the reliance on labeled datasets for training poses a bottleneck for scalability and \\nadaptability to new or rare ship types. The labor-intensive process of labeling satellite imagery \\nlimits the dataset size and diversity, potentially hindering the model\\'s comprehensive \\nunderstanding of global maritime traffic. \\n6 \\nConclusion \\nIn conclusion, the findings from this study highlight the potential of using advanced CNN \\narchitectures, augmented with attention mechanisms and architectural innovations, for the \\nclassification of ships in optical satellite imagery. The Enhanced ResNet Model with Improved \\nCBAM represents a significant step forward in this domain, offering improved accuracy and a \\ndeeper understanding of the visual features crucial for classification. \\nMoving forward, addressing the limitations and challenges identified will be crucial for further \\nadvancements. This includes exploring strategies to mitigate class imbalance, reduce \\ncomputational costs, and enhance model robustness against variable image conditions. \\nAdditionally, leveraging unsupervised or semi-supervised learning techniques could alleviate \\nthe dependency on large labeled datasets, paving the way for more scalable and adaptable \\nsatellite image classification models. \\nAcknowledgement \\nThis research was supported by KI Cloud, Division of National Supercomputing Center, Korea \\nInstitute of Science and Technology Information (KISTI). \\nReferences \\n[1] \\nK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" \\nin Proceedings of the IEEE conference on computer vision and pattern recognition, \\n2016, pp. 770-778.  \\n[2] \\nS. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, \"CBAM: Convolutional Block Attention \\nModule,\" in Computer Vision – ECCV 2018, Cham, V. Ferrari, M. Hebert, C. \\nSminchisescu, and Y. Weiss, Eds., 2018// 2018: Springer International Publishing, pp. \\n3-19.  \\n[3] \\nT. J. Morgan, \"Ship Detection in Remote Sensing Optical Imagery using Machine \\nLearning,\" The UNSW Canberra at ADFA Journal of Undergraduate Engineering \\nResearch, vol. 12, no. 2, 2019. \\n[4] \\nM. Ma, J. Chen, W. Liu, and W. Yang, \"Ship classification and detection based on CNN \\nusing GF-3 SAR images,\" Remote Sensing, vol. 10, no. 12, p. 2043, 2018. \\n[5] \\nS. Gadamsetty, R. Ch, A. Ch, C. Iwendi, and T. R. Gadekallu, \"Hash-based deep \\nlearning approach for remote sensing satellite imagery detection,\" Water, vol. 14, no. \\n5, p. 707, 2022. \\n[6] \\nK. Venkataramani et al., \"Harmonizing SAR and Optical Data to Map Surface Water \\nExtent: A Deep Learning Approach,\" in IGARSS 2023-2023 IEEE International \\nGeoscience and Remote Sensing Symposium, 2023: IEEE, pp. 3349-3352.  \\n[7] \\nB. W. Tienin, G. Cui, R. Mba Esidang, Y. A. Talla Nana, and E. Z. Moniz Moreira, \\n\"Heterogeneous Ship Data Classification with Spatial–Channel Attention with Bilinear \\nPooling Network,\" Remote Sensing, vol. 15, no. 24, p. 5759, 2023. \\n[8] \\nD. Sola, A. S. Nagi, and K. A. Scott, \"Identifying Sea Ice Ridging in SAR Imagery \\nUsing Convolutional Neural Networks,\" in IGARSS 2020-2020 IEEE International \\nGeoscience and Remote Sensing Symposium, 2020: IEEE, pp. 6930-6933.  \\n[9] \\nZ. Song, H. Sui, and L. Hua, \"A hierarchical object detection method in large-scale \\noptical remote sensing satellite imagery using saliency detection and CNN,\" \\nInternational Journal of Remote Sensing, vol. 42, no. 8, pp. 2827-2847, 2021. \\n[10] \\nX. Bi, Z. Chen, X. Li, and J. Yue, \"Efficient Single Image De-raining Using Multi-scale \\nDepthwise Separable Dilated Convolution,\" in 2021 7th International Conference on \\nComputer and Communications (ICCC), 2021: IEEE, pp. 699-703.  \\n[11] \\nH. C. Tinega, E. Chen, and D. O. Nyasaka, \"Improving Feature Learning in Remote \\nSensing Images Using an Integrated Deep Multi-Scale 3D/2D Convolutional Network,\" \\nRemote Sensing, vol. 15, no. 13, p. 3270, 2023. \\n[12] \\nQ. Wang, J. R. Hopgood, and M. Vallejo, \"Fluorescence lifetime imaging \\nendomicroscopy-based ex-vivo lung cancer prediction using multi-scale concatenated-\\ndilation convolutional neural networks,\" in Medical Imaging 2021: Computer-Aided \\nDiagnosis, 2021, vol. 11597: SPIE, pp. 617-626.  \\n[13] \\nX. Wu, J. Lu, J. Wu, and Y. Li, \"Multi-Scale Dilated Convolution Transformer for \\nSingle Image Deraining,\" in 2023 IEEE 25th International Workshop on Multimedia \\nSignal Processing (MMSP), 2023: IEEE, pp. 1-6.  \\n \\n', metadata={'Published': '2024-04-08', 'Title': 'Enhancing Ship Classification in Optical Satellite Imagery: Integrating Convolutional Block Attention Module with ResNet for Improved Performance', 'Authors': 'Ryan Donghan Kwon, Gangjoo Robin Nam, Jisoo Tak, Junseob Shin, Hyerin Cha, Yeom Hyeok, Seung Won Lee', 'Summary': \"This study presents an advanced Convolutional Neural Network (CNN)\\narchitecture for ship classification from optical satellite imagery,\\nsignificantly enhancing performance through the integration of the\\nConvolutional Block Attention Module (CBAM) and additional architectural\\ninnovations. Building upon the foundational ResNet50 model, we first\\nincorporated a standard CBAM to direct the model's focus towards more\\ninformative features, achieving an accuracy of 87% compared to the baseline\\nResNet50's 85%. Further augmentations involved multi-scale feature integration,\\ndepthwise separable convolutions, and dilated convolutions, culminating in the\\nEnhanced ResNet Model with Improved CBAM. This model demonstrated a remarkable\\naccuracy of 95%, with precision, recall, and f1-scores all witnessing\\nsubstantial improvements across various ship classes. The bulk carrier and oil\\ntanker classes, in particular, showcased nearly perfect precision and recall\\nrates, underscoring the model's enhanced capability in accurately identifying\\nand classifying ships. Attention heatmap analyses further validated the\\nimproved model's efficacy, revealing a more focused attention on relevant ship\\nfeatures, regardless of background complexities. These findings underscore the\\npotential of integrating attention mechanisms and architectural innovations in\\nCNNs for high-resolution satellite imagery classification. The study navigates\\nthrough the challenges of class imbalance and computational costs, proposing\\nfuture directions towards scalability and adaptability in new or rare ship type\\nrecognition. This research lays a groundwork for the application of advanced\\ndeep learning techniques in the domain of remote sensing, offering insights\\ninto scalable and efficient satellite image classification.\"})]\n",
      "[Document(page_content='CBAM: Convolutional Block Attention Module\\nSanghyun Woo*1, Jongchan Park*†2, Joon-Young Lee3, and In So Kweon1\\n1 Korea Advanced Institute of Science and Technology, Daejeon, Korea\\n{shwoo93, iskweon77}@kaist.ac.kr\\n2 Lunit Inc., Seoul, Korea\\njcpark@lunit.io\\n3 Adobe Research, San Jose, CA, USA\\njolee@adobe.com\\nAbstract. We propose Convolutional Block Attention Module (CBAM),\\na simple yet eﬀective attention module for feed-forward convolutional\\nneural networks. Given an intermediate feature map, our module se-\\nquentially infers attention maps along two separate dimensions, channel\\nand spatial, then the attention maps are multiplied to the input feature\\nmap for adaptive feature reﬁnement. Because CBAM is a lightweight and\\ngeneral module, it can be integrated into any CNN architectures seam-\\nlessly with negligible overheads and is end-to-end trainable along with\\nbase CNNs. We validate our CBAM through extensive experiments on\\nImageNet-1K, MS COCO detection, and VOC 2007 detection datasets.\\nOur experiments show consistent improvements in classiﬁcation and de-\\ntection performances with various models, demonstrating the wide ap-\\nplicability of CBAM. The code and models will be publicly available.\\nKeywords: Object recognition, attention mechanism, gated convolu-\\ntion\\n1\\nIntroduction\\nConvolutional neural networks (CNNs) have signiﬁcantly pushed the perfor-\\nmance of vision tasks [1–3] based on their rich representation power. To en-\\nhance performance of CNNs, recent researches have mainly investigated three\\nimportant factors of networks: depth, width, and cardinality.\\nFrom the LeNet architecture [4] to Residual-style Networks [5–8] so far, the\\nnetwork has become deeper for rich representation. VGGNet [9] shows that stack-\\ning blocks with the same shape gives fair results. Following the same spirit,\\nResNet [5] stacks the same topology of residual blocks along with skip connec-\\ntion to build an extremely deep architecture. GoogLeNet [10] shows that width\\nis another important factor to improve the performance of a model. Zagoruyko\\nand Komodakis [6] propose to increase the width of a network based on the\\nResNet architecture. They have shown that a 28-layer ResNet with increased\\n*Both authors have equally contributed.\\n†The work was done while the author was at KAIST.\\narXiv:1807.06521v2  [cs.CV]  18 Jul 2018\\n2\\nWoo, Park, Lee, Kweon\\nChannel \\nAttention \\nModule\\nSpatial\\nAttention\\nModule\\nConvolutional Block Attention Module\\nInput Feature\\nRefined Feature\\nFig. 1: The overview of CBAM. The module has two sequential sub-modules:\\nchannel and spatial. The intermediate feature map is adaptively reﬁned through\\nour module (CBAM) at every convolutional block of deep networks.\\nwidth can outperform an extremely deep ResNet with 1001 layers on the CI-\\nFAR benchmarks. Xception [11] and ResNeXt [7] come up with to increase the\\ncardinality of a network. They empirically show that cardinality not only saves\\nthe total number of parameters but also results in stronger representation power\\nthan the other two factors: depth and width.\\nApart from these factors, we investigate a diﬀerent aspect of the architecture\\ndesign, attention. The signiﬁcance of attention has been studied extensively in\\nthe previous literature [12–17]. Attention not only tells where to focus, it also\\nimproves the representation of interests. Our goal is to increase representation\\npower by using attention mechanism: focusing on important features and sup-\\npressing unnecessary ones. In this paper, we propose a new network module,\\nnamed “Convolutional Block Attention Module”. Since convolution operations\\nextract informative features by blending cross-channel and spatial information\\ntogether, we adopt our module to emphasize meaningful features along those two\\nprincipal dimensions: channel and spatial axes. To achieve this, we sequentially\\napply channel and spatial attention modules (as shown in Fig. 1), so that each\\nof the branches can learn ‘what’ and ‘where’ to attend in the channel and spatial\\naxes respectively. As a result, our module eﬃciently helps the information ﬂow\\nwithin the network by learning which information to emphasize or suppress.\\nIn the ImageNet-1K dataset, we obtain accuracy improvement from various\\nbaseline networks by plugging our tiny module, revealing the eﬃcacy of CBAM.\\nWe visualize trained models using the grad-CAM [18] and observe that CBAM-\\nenhanced networks focus on target objects more properly than their baseline\\nnetworks. Taking this into account, we conjecture that the performance boost\\ncomes from accurate attention and noise reduction of irrelevant clutters. Finally,\\nwe validate performance improvement of object detection on the MS COCO and\\nthe VOC 2007 datasets, demonstrating a wide applicability of CBAM. Since we\\nhave carefully designed our module to be light-weight, the overhead of parame-\\nters and computation is negligible in most cases.\\nContribution. Our main contribution is three-fold.\\n1. We propose a simple yet eﬀective attention module (CBAM) that can be\\nwidely applied to boost representation power of CNNs.\\nConvolutional Block Attention Module\\n3\\n2. We validate the eﬀectiveness of our attention module through extensive ab-\\nlation studies.\\n3. We verify that performance of various networks is greatly improved on the\\nmultiple benchmarks (ImageNet-1K, MS COCO, and VOC 2007) by plug-\\nging our light-weight module.\\n2\\nRelated Work\\nNetwork engineering. “Network engineering” has been one of the most impor-\\ntant vision research, because well-designed networks ensure remarkable perfor-\\nmance improvement in various applications. A wide range of architectures has\\nbeen proposed since the successful implementation of a large-scale CNN [19].\\nAn intuitive and simple way of extension is to increase the depth of neural\\nnetworks [9]. Szegedy et al.\\n[10] introduce a deep Inception network using a\\nmulti-branch architecture where each branch is customized carefully. While a\\nnaive increase in depth comes to saturation due to the diﬃculty of gradient\\npropagation, ResNet [5] proposes a simple identity skip-connection to ease the\\noptimization issues of deep networks. Based on the ResNet architecture, various\\nmodels such as WideResNet [6], Inception-ResNet [8], and ResNeXt [7] have been\\ndeveloped. WideResNet [6] proposes a residual network with a larger number of\\nconvolutional ﬁlters and reduced depth. PyramidNet [20] is a strict generalization\\nof WideResNet where the width of the network gradually increases. ResNeXt [7]\\nsuggests to use grouped convolutions and shows that increasing the cardinality\\nleads to better classiﬁcation accuracy. More recently, Huang et al. [21] propose\\na new architecture, DenseNet. It iteratively concatenates the input features with\\nthe output features, enabling each convolution block to receive raw information\\nfrom all the previous blocks. While most of recent network engineering methods\\nmainly target on three factors depth [19, 9, 10, 5], width [10, 22, 6, 8], and cardi-\\nnality [7, 11], we focus on the other aspect, ‘attention’, one of the curious facets\\nof a human visual system.\\nAttention mechanism. It is well known that attention plays an important\\nrole in human perception [23–25]. One important property of a human visual\\nsystem is that one does not attempt to process a whole scene at once. Instead,\\nhumans exploit a sequence of partial glimpses and selectively focus on salient\\nparts in order to capture visual structure better [26].\\nRecently, there have been several attempts [27, 28] to incorporate attention\\nprocessing to improve the performance of CNNs in large-scale classiﬁcation tasks.\\nWang et al. [27] propose Residual Attention Network which uses an encoder-\\ndecoder style attention module. By reﬁning the feature maps, the network not\\nonly performs well but is also robust to noisy inputs. Instead of directly com-\\nputing the 3d attention map, we decompose the process that learns channel\\nattention and spatial attention separately. The separate attention generation\\nprocess for 3D feature map has much less computational and parameter over-\\n4\\nWoo, Park, Lee, Kweon\\nhead, and therefore can be used as a plug-and-play module for pre-existing base\\nCNN architectures.\\nMore close to our work, Hu et al. [28] introduce a compact module to exploit\\nthe inter-channel relationship. In their Squeeze-and-Excitation module, they use\\nglobal average-pooled features to compute channel-wise attention. However, we\\nshow that those are suboptimal features in order to infer ﬁne channel attention,\\nand we suggest to use max-pooled features as well. They also miss the spatial\\nattention, which plays an important role in deciding ‘where’ to focus as shown in\\n[29]. In our CBAM, we exploit both spatial and channel-wise attention based on\\nan eﬃcient architecture and empirically verify that exploiting both is superior to\\nusing only the channel-wise attention as [28]. Moreover, we empirically show that\\nour module is eﬀective in detection tasks (MS-COCO and VOC). Especially, we\\nachieve state-of-the-art performance just by placing our module on top of the\\nexisting one-shot detector [30] in the VOC2007 test set.\\n3\\nConvolutional Block Attention Module\\nGiven an intermediate feature map F ∈RC×H×W as input, CBAM sequentially\\ninfers a 1D channel attention map Mc ∈RC×1×1 and a 2D spatial attention\\nmap Ms ∈R1×H×W as illustrated in Fig. 1. The overall attention process can\\nbe summarized as:\\nF′ = Mc(F) ⊗F,\\nF′′ = Ms(F′) ⊗F′,\\n(1)\\nwhere ⊗denotes element-wise multiplication. During multiplication, the atten-\\ntion values are broadcasted (copied) accordingly: channel attention values are\\nbroadcasted along the spatial dimension, and vice versa. F′′ is the ﬁnal reﬁned\\noutput. Fig. 2 depicts the computation process of each attention map. The fol-\\nlowing describes the details of each attention module.\\nChannel attention module. We produce a channel attention map by exploit-\\ning the inter-channel relationship of features. As each channel of a feature map\\nis considered as a feature detector [31], channel attention focuses on ‘what’ is\\nmeaningful given an input image. To compute the channel attention eﬃciently,\\nwe squeeze the spatial dimension of the input feature map. For aggregating spa-\\ntial information, average-pooling has been commonly adopted so far. Zhou et al.\\n[32] suggest to use it to learn the extent of the target object eﬀectively and Hu et\\nal. [28] adopt it in their attention module to compute spatial statistics. Beyond\\nthe previous works, we argue that max-pooling gathers another important clue\\nabout distinctive object features to infer ﬁner channel-wise attention. Thus, we\\nuse both average-pooled and max-pooled features simultaneously. We empiri-\\ncally conﬁrmed that exploiting both features greatly improves representation\\npower of networks rather than using each independently (see Sec. 4.1), showing\\nthe eﬀectiveness of our design choice. We describe the detailed operation below.\\nConvolutional Block Attention Module\\n5\\nMaxPool\\nAvgPool\\nChannel Attention\\nMC\\nChannel Attention Module\\n[MaxPool, AvgPool]\\nSpatial Attention\\nMS\\nSpatial Attention Module\\nInput feature F\\nChannel-refined \\nfeature F’\\nShared MLP\\nconv\\nlayer\\nFig. 2: Diagram of each attention sub-module. As illustrated, the channel\\nsub-module utilizes both max-pooling outputs and average-pooling outputs with\\na shared network; the spatial sub-module utilizes similar two outputs that are\\npooled along the channel axis and forward them to a convolution layer.\\nWe ﬁrst aggregate spatial information of a feature map by using both average-\\npooling and max-pooling operations, generating two diﬀerent spatial context de-\\nscriptors: Fc\\navg and Fc\\nmax, which denote average-pooled features and max-pooled\\nfeatures respectively. Both descriptors are then forwarded to a shared network\\nto produce our channel attention map Mc ∈RC×1×1. The shared network is\\ncomposed of multi-layer perceptron (MLP) with one hidden layer. To reduce\\nparameter overhead, the hidden activation size is set to RC/r×1×1, where r is\\nthe reduction ratio. After the shared network is applied to each descriptor, we\\nmerge the output feature vectors using element-wise summation. In short, the\\nchannel attention is computed as:\\nMc(F) = σ(MLP(AvgPool(F)) + MLP(MaxPool(F)))\\n= σ(W1(W0(Fc\\navg)) + W1(W0(Fc\\nmax))),\\n(2)\\nwhere σ denotes the sigmoid function, W0 ∈RC/r×C, and W1 ∈RC×C/r. Note\\nthat the MLP weights, W0 and W1, are shared for both inputs and the ReLU\\nactivation function is followed by W0.\\nSpatial attention module. We generate a spatial attention map by utilizing\\nthe inter-spatial relationship of features. Diﬀerent from the channel attention,\\nthe spatial attention focuses on ‘where’ is an informative part, which is com-\\nplementary to the channel attention. To compute the spatial attention, we ﬁrst\\napply average-pooling and max-pooling operations along the channel axis and\\nconcatenate them to generate an eﬃcient feature descriptor. Applying pooling\\noperations along the channel axis is shown to be eﬀective in highlighting informa-\\ntive regions [33]. On the concatenated feature descriptor, we apply a convolution\\n6\\nWoo, Park, Lee, Kweon\\nChannel attention\\nResBlock + CBAM\\nNext\\nconv blocks\\nPrevious\\nconv blocks\\nF\\nSpatial attention\\nconv\\nMC\\nMS\\nF’\\nF’’\\nFig. 3: CBAM integrated with a ResBlock in ResNet[5]. This ﬁgure shows\\nthe exact position of our module when integrated within a ResBlock. We apply\\nCBAM on the convolution outputs in each block.\\nlayer to generate a spatial attention map Ms(F) ∈RH×W which encodes where\\nto emphasize or suppress. We describe the detailed operation below.\\nWe aggregate channel information of a feature map by using two pooling\\noperations, generating two 2D maps: Fs\\navg ∈R1×H×W and Fs\\nmax ∈R1×H×W .\\nEach denotes average-pooled features and max-pooled features across the chan-\\nnel. Those are then concatenated and convolved by a standard convolution layer,\\nproducing our 2D spatial attention map. In short, the spatial attention is com-\\nputed as:\\nMs(F) = σ(f 7×7([AvgPool(F); MaxPool(F)]))\\n= σ(f 7×7([Fs\\navg; Fs\\nmax])),\\n(3)\\nwhere σ denotes the sigmoid function and f 7×7 represents a convolution opera-\\ntion with the ﬁlter size of 7 × 7.\\nArrangement of attention modules. Given an input image, two attention\\nmodules, channel and spatial, compute complementary attention, focusing on\\n‘what’ and ‘where’ respectively. Considering this, two modules can be placed\\nin a parallel or sequential manner. We found that the sequential arrangement\\ngives a better result than a parallel arrangement. For the arrangement of the\\nsequential process, our experimental result shows that the channel-ﬁrst order\\nis slightly better than the spatial-ﬁrst. We will discuss experimental results on\\nnetwork engineering in Sec. 4.1.\\n4\\nExperiments\\nWe evaluate CBAM on the standard benchmarks: ImageNet-1K for image clas-\\nsiﬁcation; MS COCO and VOC 2007 for object detection. In order to perform\\nbetter apple-to-apple comparisons, we reproduced all the evaluated networks [5–\\n7, 34, 28] in the PyTorch framework [35] and report our reproduced results in the\\nwhole experiments.\\nTo thoroughly evaluate the eﬀectiveness of our ﬁnal module, we ﬁrst perform\\nextensive ablation experiments. Then, we verify that CBAM outperforms all the\\nConvolutional Block Attention Module\\n7\\nbaselines without bells and whistles, demonstrating the general applicability of\\nCBAM across diﬀerent architectures as well as diﬀerent tasks. One can seam-\\nlessly integrate CBAM in any CNN architectures and jointly train the combined\\nCBAM-enhanced networks. Fig. 3 shows a diagram of CBAM integrated with a\\nResBlock in ResNet [5] as an example.\\n4.1\\nAblation studies\\nIn this subsection, we empirically show the eﬀectiveness of our design choice. For\\nthis ablation study, we use the ImageNet-1K dataset and adopt ResNet-50 [5] as\\nthe base architecture. The ImageNet-1K classiﬁcation dataset [1] consists of 1.2\\nmillion images for training and 50,000 for validation with 1,000 object classes.\\nWe adopt the same data augmentation scheme with [5, 36] for training and apply\\na single-crop evaluation with the size of 224×224 at test time. The learning rate\\nstarts from 0.1 and drops every 30 epochs. We train the networks for 90 epochs.\\nFollowing [5, 36, 37], we report classiﬁcation errors on the validation set.\\nOur module design process is split into three parts. We ﬁrst search for the\\neﬀective approach to computing the channel attention, then the spatial attention.\\nFinally, we consider how to combine both channel and spatial attention modules.\\nWe explain the details of each experiment below.\\nChannel attention. We experimentally verify that using both average-pooled\\nand max-pooled features enables ﬁner attention inference. We compare 3 vari-\\nants of channel attention: average pooling, max pooling, and joint use of both\\npoolings. Note that the channel attention module with an average pooling is the\\nsame as the SE [28] module. Also, when using both poolings, we use a shared\\nMLP for attention inference to save parameters, as both of aggregated channel\\nfeatures lie in the same semantic embedding space. We only use channel attention\\nmodules in this experiment and we ﬁx the reduction ratio to 16.\\nExperimental results with various pooling methods are shown in Table 1. We\\nobserve that max-pooled features are as meaningful as average-pooled features,\\ncomparing the accuracy improvement from the baseline. In the work of SE [28],\\nhowever, they only exploit the average-pooled features, missing the importance\\nDescription\\nParameters GFLOPs Top-1 Error(%) Top-5 Error(%)\\nResNet50 (baseline)\\n25.56M\\n3.86\\n24.56\\n7.50\\nResNet50 + AvgPool (SE [28])\\n25.92M\\n3.94\\n23.14\\n6.70\\nResNet50 + MaxPool\\n25.92M\\n3.94\\n23.20\\n6.83\\nResNet50 + AvgPool & MaxPool\\n25.92M\\n4.02\\n22.80\\n6.52\\nTable 1: Comparison of diﬀerent channel attention methods. We observe\\nthat using our proposed method outperforms recently suggested Squeeze and\\nExcitation method [28].\\n8\\nWoo, Park, Lee, Kweon\\nDescription\\nParam. GFLOPs Top-1 Error(%) Top-5 Error(%)\\nResNet50 + channel (SE [28])\\n28.09M\\n3.860\\n23.14\\n6.70\\nResNet50 + channel\\n28.09M\\n3.860\\n22.80\\n6.52\\nResNet50 + channel + spatial (1x1 conv, k=3) 28.10M\\n3.862\\n22.96\\n6.64\\nResNet50 + channel + spatial (1x1 conv, k=7) 28.10M\\n3.869\\n22.90\\n6.47\\nResNet50 + channel + spatial (avg&max, k=3) 28.09M\\n3.863\\n22.68\\n6.41\\nResNet50 + channel + spatial (avg&max, k=7) 28.09M\\n3.864\\n22.66\\n6.31\\nTable 2: Comparison of diﬀerent spatial attention methods. Using the\\nproposed channel-pooling (i.e. average- and max-pooling along the channel axis)\\nalong with the large kernel size of 7 for the following convolution operation\\nperforms best.\\nDescription\\nTop-1 Error(%) Top-5 Error(%)\\nResNet50 + channel (SE [28])\\n23.14\\n6.70\\nResNet50 + channel + spatial\\n22.66\\n6.31\\nResNet50 + spatial + channel\\n22.78\\n6.42\\nResNet50 + channel & spatial in parallel\\n22.95\\n6.59\\nTable 3: Combining methods of channel and spatial attention. Using both\\nattention is critical while the best-combining strategy (i.e. sequential, channel-\\nﬁrst) further improves the accuracy.\\nof max-pooled features. We argue that max-pooled features which encode the de-\\ngree of the most salient part can compensate the average-pooled features which\\nencode global statistics softly. Thus, we suggest to use both features simulta-\\nneously and apply a shared network to those features. The outputs of a shared\\nnetwork are then merged by element-wise summation. We empirically show that\\nour channel attention method is an eﬀective way to push performance further\\nfrom SE [28] without additional learnable parameters. As a brief conclusion, we\\nuse both average- and max-pooled features in our channel attention module with\\nthe reduction ratio of 16 in the following experiments.\\nSpatial attention. Given the channel-wise reﬁned features, we explore an eﬀec-\\ntive method to compute the spatial attention. The design philosophy is symmet-\\nric with the channel attention branch. To generate a 2D spatial attention map,\\nwe ﬁrst compute a 2D descriptor that encodes channel information at each pixel\\nover all spatial locations. We then apply one convolution layer to the 2D descrip-\\ntor, obtaining the raw attention map. The ﬁnal attention map is normalized by\\nthe sigmoid function.\\nWe compare two methods of generating the 2D descriptor: channel pooling\\nusing average- and max-pooling across the channel axis and standard 1 × 1 con-\\nvolution reducing the channel dimension into 1. In addition, we investigate the\\neﬀect of a kernel size at the following convolution layer: kernel sizes of 3 and 7.\\nIn the experiment, we place the spatial attention module after the previously\\nConvolutional Block Attention Module\\n9\\ndesigned channel attention module, as the ﬁnal goal is to use both modules\\ntogether.\\nTable 2 shows the experimental results. We can observe that the channel\\npooling produces better accuracy, indicating that explicitly modeled pooling\\nleads to ﬁner attention inference rather than learnable weighted channel pooling\\n(implemented as 1 × 1 convolution). In the comparison of diﬀerent convolution\\nkernel sizes, we ﬁnd that adopting a larger kernel size generates better accuracy\\nin both cases. It implies that a broad view (i.e. large receptive ﬁeld) is needed\\nfor deciding spatially important regions. Considering this, we adopt the channel-\\npooling method and the convolution layer with a large kernel size to compute\\nspatial attention. In a brief conclusion, we use the average- and max-pooled\\nfeatures across the channel axis with a convolution kernel size of 7 as our spatial\\nattention module.\\nArrangement of the channel and spatial attention. In this experiment,\\nwe compare three diﬀerent ways of arranging the channel and spatial attention\\nsubmodules: sequential channel-spatial, sequential spatial-channel, and parallel\\nuse of both attention modules. As each module has diﬀerent functions, the order\\nmay aﬀect the overall performance. For example, from a spatial viewpoint, the\\nchannel attention is globally applied, while the spatial attention works locally.\\nAlso, it is natural to think that we may combine two attention outputs to build\\na 3D attention map. In the case, both attentions can be applied in parallel, then\\nthe outputs of the two attention modules are added and normalized with the\\nsigmoid function.\\nTable 3 summarizes the experimental results on diﬀerent attention arrang-\\ning methods. From the results, we can ﬁnd that generating an attention map\\nsequentially infers a ﬁner attention map than doing in parallel. In addition, the\\nchannel-ﬁrst order performs slightly better than the spatial-ﬁrst order. Note that\\nall the arranging methods outperform using only the channel attention indepen-\\ndently, showing that utilizing both attentions is crucial while the best-arranging\\nstrategy further pushes performance.\\nFinal module design. Throughout the ablation studies, we have designed the\\nchannel attention module, the spatial attention module, and the arrangement of\\nthe two modules. Our ﬁnal module is as shown in Fig. 1 and Fig. 2: we choose\\naverage- and max-pooling for both channel and spatial attention module; we use\\nconvolution with a kernel size of 7 in the spatial attention module; we arrange\\nthe channel and spatial submodules sequentially. Our ﬁnal module(i.e. ResNet50\\n+ CBAM) achieves top-1 error of 22.66%, which is much lower than SE [28](i.e.\\nResNet50 + SE), as shown in Table 4.\\n4.2\\nImage Classiﬁcation on ImageNet-1K\\nWe perform ImageNet-1K classiﬁcation experiments to rigorously evaluate our\\nmodule. We follow the same protocol as speciﬁed in Sec. 4.1 and evaluate our\\n10\\nWoo, Park, Lee, Kweon\\nArchitecture\\nParam. GFLOPs Top-1 Error (%) Top-5 Error (%)\\nResNet18 [5]\\n11.69M\\n1.814\\n29.60\\n10.55\\nResNet18 [5] + SE [28]\\n11.78M\\n1.814\\n29.41\\n10.22\\nResNet18 [5] + CBAM\\n11.78M\\n1.815\\n29.27\\n10.09\\nResNet34 [5]\\n21.80M\\n3.664\\n26.69\\n8.60\\nResNet34 [5] + SE [28]\\n21.96M\\n3.664\\n26.13\\n8.35\\nResNet34 [5] + CBAM\\n21.96M\\n3.665\\n25.99\\n8.24\\nResNet50 [5]\\n25.56M\\n3.858\\n24.56\\n7.50\\nResNet50 [5] + SE [28]\\n28.09M\\n3.860\\n23.14\\n6.70\\nResNet50 [5] + CBAM\\n28.09M\\n3.864\\n22.66\\n6.31\\nResNet101 [5]\\n44.55M\\n7.570\\n23.38\\n6.88\\nResNet101 [5] + SE [28]\\n49.33M\\n7.575\\n22.35\\n6.19\\nResNet101 [5] + CBAM\\n49.33M\\n7.581\\n21.51\\n5.69\\nWideResNet18 [6] (widen=1.5)\\n25.88M\\n3.866\\n26.85\\n8.88\\nWideResNet18 [6] (widen=1.5) + SE [28] 26.07M\\n3.867\\n26.21\\n8.47\\nWideResNet18 [6] (widen=1.5) + CBAM 26.08M\\n3.868\\n26.10\\n8.43\\nWideResNet18 [6] (widen=2.0)\\n45.62M\\n6.696\\n25.63\\n8.20\\nWideResNet18 [6] (widen=2.0) + SE [28] 45.97M\\n6.696\\n24.93\\n7.65\\nWideResNet18 [6] (widen=2.0) + CBAM 45.97M\\n6.697\\n24.84\\n7.63\\nResNeXt50 [7] (32x4d)\\n25.03M\\n3.768\\n22.85\\n6.48\\nResNeXt50 [7] (32x4d) + SE [28]\\n27.56M\\n3.771\\n21.91\\n6.04\\nResNeXt50 [7] (32x4d) + CBAM\\n27.56M\\n3.774\\n21.92\\n5.91\\nResNeXt101 [7] (32x4d)\\n44.18M\\n7.508\\n21.54\\n5.75\\nResNeXt101 [7] (32x4d) + SE [28]\\n48.96M\\n7.512\\n21.17\\n5.66\\nResNeXt101 [7] (32x4d) + CBAM\\n48.96M\\n7.519\\n21.07\\n5.59\\n* all results are reproduced in the PyTorch framework.\\nTable 4: Classiﬁcation results on ImageNet-1K. Single-crop validation er-\\nrors are reported.\\nmodule in various network architectures including ResNet [5], WideResNet [6],\\nand ResNext [7].\\nTable 4 summarizes the experimental results. The networks with CBAM\\noutperform all the baselines signiﬁcantly, demonstrating that the CBAM can\\ngeneralize well on various models in the large-scale dataset. Moreover, the models\\nwith CBAM improve the accuracy upon the one of the strongest method – SE [28]\\nwhich is the winning approach of the ILSVRC 2017 classiﬁcation task. It implies\\nthat our proposed approach is powerful, showing the eﬃcacy of new pooling\\nmethod that generates richer descriptor and spatial attention that complements\\nthe channel attention eﬀectively.\\nFig. 4 depicts the error curves of various networks during ImageNet-1K train-\\ning. We can clearly see that our method exhibits lowest training and validation\\nerror in both error plots. It shows that CBAM has greater ability to improve\\ngeneralization power of baseline models compared to SE [28].\\nWe also ﬁnd that the overall overhead of CBAM is quite small in terms of both\\nparameters and computation. This motivates us to apply our proposed module\\nCBAM to the light-weight network, MobileNet [34]. Table 5 summarizes the\\nexperimental results that we conducted based on the MobileNet architecture.\\nWe have placed CBAM to two models, basic and capacity-reduced model(i.e.\\nadjusting width multiplier(α) to 0.7). We observe similar phenomenon as shown\\nConvolutional Block Attention Module\\n11\\n(a) ResNet50 [5]\\n(b) MobileNet [34]\\nFig. 4: Error curves during ImageNet-1K training. Best viewed in color.\\nArchitecture\\nParameters GFLOPs Top-1 Error (%) Top-5 Error (%)\\nMobileNet [34] α = 0.7\\n2.30M\\n0.283\\n34.86\\n13.69\\nMobileNet [34] α = 0.7 + SE [28]\\n2.71M\\n0.283\\n32.50\\n12.49\\nMobileNet [34] α = 0.7 + CBAM\\n2.71M\\n0.289\\n31.51\\n11.48\\nMobileNet [34]\\n4.23M\\n0.569\\n31.39\\n11.51\\nMobileNet [34] + SE [28]\\n5.07M\\n0.570\\n29.97\\n10.63\\nMobileNet [34] + CBAM\\n5.07M\\n0.576\\n29.01\\n9.99\\n* all results are reproduced in the PyTorch framework.\\nTable 5: Classiﬁcation results on ImageNet-1K using the light-weight\\nnetwork, MobileNet [34]. Single-crop validation errors are reported.\\nin Table 4. CBAM not only boosts the accuracy of baselines signiﬁcantly but also\\nfavorably improves the performance of SE [28]. This shows the great potential\\nof CBAM for applications on low-end devices.\\n4.3\\nNetwork Visualization with Grad-CAM [18]\\nFor the qualitative analysis, we apply the Grad-CAM [18] to diﬀerent networks\\nusing images from the ImageNet validation set. Grad-CAM is a recently proposed\\nvisualization method which uses gradients in order to calculate the importance\\nof the spatial locations in convolutional layers. As the gradients are calculated\\nwith respect to a unique class, Grad-CAM result shows attended regions clearly.\\nBy observing the regions that network has considered as important for predicting\\na class, we attempt to look at how this network is making good use of features.\\nWe compare the visualization results of CBAM-integrated network (ResNet50 +\\nCBAM) with baseline (ResNet50) and SE-integrated network (ResNet50 + SE).\\nFig. 5 illustrate the visualization results. The softmax scores for a target class\\nare also shown in the ﬁgure.\\nIn Fig. 5, we can clearly see that the Grad-CAM masks of the CBAM-\\nintegrated network cover the target object regions better than other methods.\\nThat is, the CBAM-integrated network learns well to exploit information in\\ntarget object regions and aggregate features from them. Note that target class\\n12\\nWoo, Park, Lee, Kweon\\nP = 0.96340\\nP=0.87240\\nP=0.80736\\nP = 0.19994\\nP=0.14643\\nP=0.11857\\nP = 0.93707\\nP=0.77550\\nP=0.65681\\nP = 0.35248\\nP=0.25093\\nP=0.22357\\nP = 0.87490\\nP=0.70827\\nP=0.64185\\nP = 0.53005\\nP=0.15367\\nP=0.14763\\nP = 0.99085\\nP=0.97166\\nP=0.92236\\nP = 0.59662\\nP=0.26611\\nP=0.01176\\nP = 0.96039\\nP=0.89962\\nP=0.58732\\nP = 0.59790\\nP=0.14804\\nP=0.08126\\nP = 0.84387\\nP=0.72659\\nP=0.67128\\nP = 0.71000\\nP=0.61595\\nP=0.56834\\nP = 0.98482\\nP=0.96575\\nP=0.85873\\nP = 0.90806\\nP=0.79829\\nP=0.62856\\nP = 0.78636\\nP=0.73723\\nP=0.68065\\nP = 0.98567\\nP=0.92250\\nP=0.07429\\nTailed frog\\nToilet tissue\\nLoudspeaker\\nSpider web\\nAmerican egret\\nTank\\nSeawall\\nSpace heater\\nCroquet ball\\nEel\\nHammerhead\\nEskimo dog\\nSnow leopard\\nBoat paddle\\nDaddy longlegs\\nSchool bus\\nInput \\nimage\\nResNet50\\nResNet50 \\n+ SE\\nResNet50 \\n+ CBAM\\nInput \\nimage\\nResNet50\\nResNet50 \\n+ SE\\nResNet50 \\n+ CBAM\\nFig. 5: Grad-CAM [18] visualization results. We compare the visualiza-\\ntion results of CBAM-integrated network (ResNet50 + CBAM) with baseline\\n(ResNet50) and SE-integrated network (ResNet50 + SE). The grad-CAM visu-\\nalization is calculated for the last convolutional outputs. The ground-truth label\\nis shown on the top of each input image and P denotes the softmax score of each\\nnetwork for the ground-truth class.\\nConvolutional Block Attention Module\\n13\\nBackbone\\nDetector\\nmAP@.5 mAP@.75 mAP@[.5, .95]\\nResNet50 [5]\\nFaster-RCNN [41]\\n46.2\\n28.1\\n27.0\\nResNet50 [5] + CBAM\\nFaster-RCNN [41]\\n48.2\\n29.2\\n28.1\\nResNet101 [5]\\nFaster-RCNN [41]\\n48.4\\n30.7\\n29.1\\nResNet101 [5] + CBAM Faster-RCNN [41]\\n50.5\\n32.6\\n30.8\\n* all results are reproduced in the PyTorch framework.\\nTable 6: Object detection mAP(%) on the MS COCO validation set. We\\nadopt the Faster R-CNN [41] detection framework and apply our module to the\\nbase networks. CBAM boosts mAP@[.5, .95] by 0.9 for both baseline networks.\\nBackbone\\nDetector\\nmAP@.5 Parameters (M)\\nVGG16 [9]\\nSSD [39]\\n77.8\\n26.5\\nVGG16 [9]\\nStairNet [30]\\n78.9\\n32.0\\nVGG16 [9]\\nStairNet [30] + SE [28]\\n79.1\\n32.1\\nVGG16 [9]\\nStairNet [30] + CBAM\\n79.3\\n32.1\\nMobileNet [34] SSD [39]\\n68.1\\n5.81\\nMobileNet [34] StairNet [30]\\n70.1\\n5.98\\nMobileNet [34] StairNet [30] + SE [28]\\n70.0\\n5.99\\nMobileNet [34] StairNet [30] + CBAM\\n70.5\\n6.00\\n* all results are reproduced in the PyTorch framework.\\nTable 7: Object detection mAP(%) on the VOC 2007 test set. We adopt\\nthe StairNet [30] detection framework and apply SE and CBAM to the detectors.\\nCBAM favorably improves all the strong baselines with negligible additional\\nparameters.\\nscores also increase accordingly. From the observations, we conjecture that the\\nfeature reﬁnement process of CBAM eventually leads the networks to utilize\\ngiven features well.\\n4.4\\nMS COCO Object Detection\\nWe conduct object detection on the Microsoft COCO dataset [3]. This dataset\\ninvolves 80k training images (“2014 train”) and 40k validation images (“2014\\nval”). The average mAP over diﬀerent IoU thresholds from 0.5 to 0.95 is used\\nfor evaluation. According to [38, 39], we trained our model using all the training\\nimages as well as a subset of validation images, holding out 5,000 examples for\\nvalidation. Our training code is based on [40] and we train the network for 490K\\niterations for fast performance validation. We adopt Faster-RCNN [41] as our\\ndetection method and ImageNet pre-trained ResNet50 and ResNet101 [5] as our\\nbaseline networks. Here we are interested in performance improvement by plug-\\nging CBAM to the baseline networks. Since we use the same detection method in\\nall the models, the gains can only be attributed to the enhanced representation\\npower, given by our module CBAM. As shown in the Table 6, we observe signiﬁ-\\ncant improvements from the baseline, demonstrating generalization performance\\nof CBAM on other recognition tasks.\\n14\\nWoo, Park, Lee, Kweon\\n4.5\\nVOC 2007 Object Detection\\nWe further perform experiments on the PASCAL VOC 2007 test set. In this ex-\\nperiment, we apply CBAM to the detectors, while the previous experiments (Ta-\\nble 6) apply our module to the base networks. We adopt the StairNet [30] frame-\\nwork, which is one of the strongest multi-scale method based on the SSD [39].\\nFor the experiment, we reproduce SSD and StairNet in our PyTorch platform\\nin order to estimate performance improvement of CBAM accurately and achieve\\n77.8% and 78.9% mAP@.5 respectively, which are higher than the original accu-\\nracy reported in the original papers. We then place SE [28] and CBAM right be-\\nfore every classiﬁer, reﬁning the ﬁnal features which are composed of up-sampled\\nglobal features and corresponding local features before the prediction, enforcing\\nmodel to adaptively select only the meaningful features. We train all the models\\non the union set of VOC 2007 trainval and VOC 2012 trainval (“07+12”), and\\nevaluate on the VOC 2007 test set. The total number of training epochs is 250.\\nWe use a weight decay of 0.0005 and a momentum of 0.9. In all the experiments,\\nthe size of the input image is ﬁxed to 300 for the simplicity.\\nThe experimental results are summarized in Table 7. We can clearly see\\nthat CBAM improves the accuracy of all strong baselines with two backbone\\nnetworks. Note that accuracy improvement of CBAM comes with a negligible\\nparameter overhead, indicating that enhancement is not due to a naive capacity-\\nincrement but because of our eﬀective feature reﬁnement. In addition, the result\\nusing the light-weight backbone network [34] again shows that CBAM can be\\nan interesting method to low-end devices.\\n5\\nConclusion\\nWe have presented the convolutional bottleneck attention module (CBAM), a\\nnew approach to improve representation power of CNN networks. We apply\\nattention-based feature reﬁnement with two distinctive modules, channel and\\nspatial, and achieve considerable performance improvement while keeping the\\noverhead small. For the channel attention, we suggest to use the max-pooled\\nfeatures along with the average-pooled features, leading to produce ﬁner atten-\\ntion than SE [28]. We further push the performance by exploiting the spatial\\nattention. Our ﬁnal module (CBAM) learns what and where to emphasize or\\nsuppress and reﬁnes intermediate features eﬀectively. To verify its eﬃcacy, we\\nconducted extensive experiments with various state-of-the-art models and con-\\nﬁrmed that CBAM outperforms all the baselines on three diﬀerent benchmark\\ndatasets: ImageNet-1K, MS COCO, and VOC 2007. In addition, we visualize\\nhow the module exactly infers given an input image. Interestingly, we observed\\nthat our module induces the network to focus on target object properly. We hope\\nCBAM become an important component of various network architectures.\\nConvolutional Block Attention Module\\n15\\nReferences\\n1. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale\\nhierarchical image database. In: Proc. of Computer Vision and Pattern Recognition\\n(CVPR). (2009)\\n2. Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images\\n3. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,\\nZitnick, C.L.: Microsoft coco: Common objects in context. In: Proc. of European\\nConf. on Computer Vision (ECCV). (2014)\\n4. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to\\ndocument recognition. Proceedings of the IEEE 86(11) (1998) 2278–2324\\n5. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\\nIn: Proc. of Computer Vision and Pattern Recognition (CVPR). (2016)\\n6. Zagoruyko, S., Komodakis, N.:\\nWide residual networks.\\narXiv preprint\\narXiv:1605.07146 (2016)\\n7. Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K.: Aggregated residual transformations\\nfor deep neural networks. arXiv preprint arXiv:1611.05431 (2016)\\n8. Szegedy, C., Ioﬀe, S., Vanhoucke, V., Alemi, A.A.: Inception-v4, inception-resnet\\nand the impact of residual connections on learning. In: Proc. of Association for\\nthe Advancement of Artiﬁcial Intelligence (AAAI). (2017)\\n9. Simonyan, K., Zisserman, A.:\\nVery deep convolutional networks for large-scale\\nimage recognition. arXiv preprint arXiv:1409.1556 (2014)\\n10. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,\\nVanhoucke, V., Rabinovich, A.:\\nGoing deeper with convolutions.\\nIn: Proc. of\\nComputer Vision and Pattern Recognition (CVPR). (2015)\\n11. Chollet, F.: Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 (2016)\\n12. Mnih, V., Heess, N., Graves, A., et al.: Recurrent models of visual attention.\"\\nadvances in neural information processing systems. In: Proc. of Neural Information\\nProcessing Systems (NIPS). (2014)\\n13. Ba, J., Mnih, V., Kavukcuoglu, K.: Multiple object recognition with visual atten-\\ntion. (2014)\\n14. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning\\nto align and translate. (2014)\\n15. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R.,\\nBengio, Y.: Show, attend and tell: Neural image caption generation with visual\\nattention. (2015)\\n16. Gregor, K., Danihelka, I., Graves, A., Rezende, D.J., Wierstra, D.: Draw: A recur-\\nrent neural network for image generation. (2015)\\n17. Jaderberg, M., Simonyan, K., Zisserman, A., et al.: Spatial transformer networks.\\nIn: Proc. of Neural Information Processing Systems (NIPS). (2015)\\n18. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-\\ncam: Visual explanations from deep networks via gradient-based localization. In:\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\\n(2017) 618–626\\n19. Krizhevsky, A., Sutskever, I., Hinton, G.E.:\\nImagenet classiﬁcation with deep\\nconvolutional neural networks. In: Proc. of Neural Information Processing Systems\\n(NIPS). (2012)\\n20. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proc. of Computer\\nVision and Pattern Recognition (CVPR). (2017)\\n16\\nWoo, Park, Lee, Kweon\\n21. Huang, G., Liu, Z., Weinberger, K.Q., van der Maaten, L.:\\nDensely connected\\nconvolutional networks. arXiv preprint arXiv:1608.06993 (2016)\\n22. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the incep-\\ntion architecture for computer vision. In: Proc. of Computer Vision and Pattern\\nRecognition (CVPR). (2016)\\n23. Itti, L., Koch, C., Niebur, E.: A model of saliency-based visual attention for rapid\\nscene analysis. In: IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI). (1998)\\n24. Rensink, R.A.: The dynamic representation of scenes. In: Visual cognition 7.1-3.\\n(2000)\\n25. Corbetta, M., Shulman, G.L.: Control of goal-directed and stimulus-driven atten-\\ntion in the brain. In: Nature reviews neuroscience 3.3. (2002)\\n26. Larochelle, H., Hinton, G.E.: Learning to combine foveal glimpses with a third-\\norder boltzmann machine.\\nIn: Proc. of Neural Information Processing Systems\\n(NIPS). (2010)\\n27. Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X.,\\nTang, X.:\\nResidual attention network for image classiﬁcation.\\narXiv preprint\\narXiv:1704.06904 (2017)\\n28. Hu, J., Shen, L., Sun, G.:\\nSqueeze-and-excitation networks.\\narXiv preprint\\narXiv:1709.01507 (2017)\\n29. Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Chua, T.S.: Sca-cnn: Spatial and\\nchannel-wise attention in convolutional networks for image captioning. In: Proc.\\nof Computer Vision and Pattern Recognition (CVPR). (2017)\\n30. Sanghyun, W., Soonmin, H., So, K.I.: Stairnet: Top-down semantic aggregation\\nfor accurate one shot detection. In: Proc. of Winter Conference on Applications of\\nComputer Vision (WACV). (2018)\\n31. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.\\nIn: Proc. of European Conf. on Computer Vision (ECCV). (2014)\\n32. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep fea-\\ntures for discriminative localization. In: Computer Vision and Pattern Recognition\\n(CVPR), 2016 IEEE Conference on, IEEE (2016) 2921–2929\\n33. Zagoruyko, S., Komodakis, N.: Paying more attention to attention: Improving the\\nperformance of convolutional neural networks via attention transfer. In: ICLR.\\n(2017)\\n34. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-\\ndreetto, M., Adam, H.:\\nMobilenets: Eﬃcient convolutional neural networks for\\nmobile vision applications. arXiv preprint arXiv:1704.04861 (2017)\\n35. : Pytorch. http://pytorch.org/ Accessed: 2017-11-08.\\n36. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.\\nIn: Proc. of European Conf. on Computer Vision (ECCV). (2016)\\n37. Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with\\nstochastic depth. In: Proc. of European Conf. on Computer Vision (ECCV). (2016)\\n38. Bell, S., Lawrence Zitnick, C., Bala, K., Girshick, R.: Inside-outside net: Detecting\\nobjects in context with skip pooling and recurrent neural networks. In: Proc. of\\nComputer Vision and Pattern Recognition (CVPR). (2016)\\n39. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.: Ssd:\\nSingle shot multibox detector. In: Proc. of European Conf. on Computer Vision\\n(ECCV). (2016)\\n40. Chen, X., Gupta, A.:\\nAn implementation of faster rcnn with study for region\\nsampling. arXiv preprint arXiv:1702.02138 (2017)\\nConvolutional Block Attention Module\\n17\\n41. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object de-\\ntection with region proposal networks. In: Proc. of Neural Information Processing\\nSystems (NIPS). (2015)\\n', metadata={'Published': '2018-07-18', 'Title': 'CBAM: Convolutional Block Attention Module', 'Authors': 'Sanghyun Woo, Jongchan Park, Joon-Young Lee, In So Kweon', 'Summary': 'We propose Convolutional Block Attention Module (CBAM), a simple yet\\neffective attention module for feed-forward convolutional neural networks.\\nGiven an intermediate feature map, our module sequentially infers attention\\nmaps along two separate dimensions, channel and spatial, then the attention\\nmaps are multiplied to the input feature map for adaptive feature refinement.\\nBecause CBAM is a lightweight and general module, it can be integrated into any\\nCNN architectures seamlessly with negligible overheads and is end-to-end\\ntrainable along with base CNNs. We validate our CBAM through extensive\\nexperiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets.\\nOur experiments show consistent improvements in classification and detection\\nperformances with various models, demonstrating the wide applicability of CBAM.\\nThe code and models will be publicly available.'}), Document(page_content='IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n1\\nConvolutional Neural Network with Convolutional\\nBlock Attention Module for Finger Vein\\nRecognition\\nZhongxia Zhang, Mingwen Wang\\nAbstract\\nConvolutional neural networks have become a popular research in the ﬁeld of ﬁnger vein recognition because of their powerful\\nimage feature representation. However, most researchers focus on improving the performance of the network by increasing the\\nCNN depth and width, which often requires high computational effort. Moreover, we can notice that not only the importance of\\npixels in different channels is different, but also the importance of pixels in different positions of the same channel is different. To\\nreduce the computational effort and to take into account the different importance of pixels, we propose a lightweight convolutional\\nneural network with a convolutional block attention module (CBAM) for ﬁnger vein recognition, which can achieve a more accurate\\ncapture of visual structures through an attention mechanism. First, image sequences are fed into a lightweight convolutional neural\\nnetwork we designed to improve visual features. Afterwards, it learns to assign feature weights in an adaptive manner with the\\nhelp of a convolutional block attention module. The experiments are carried out on two publicly available databases and the\\nresults demonstrate that the proposed method achieves a stable, highly accurate, and robust performance in multimodal ﬁnger\\nrecognition.\\nIndex Terms\\nFinger vein recognition, lightweight convolutional neural network, attention mechanism, convolutional block attention module.\\nI. INTRODUCTION\\nWith the development of society, people have higher and higher requirements for identity information security. Traditional\\nidentiﬁcation technology has been difﬁcult to meet people’s needs, so it is necessary to develop biometric technology with\\nhigher security. Finger veins are hidden under the skin of the ﬁngers [1],[2]. Compared with other biometric features, its\\nstructure is complex and not available under visible light. It has high stability, concealment and anti-counterfeiting properties\\nand has a broad application prospect.\\nGenerally speaking, the ﬁnger vein recognition process includes the following four steps: image acquisition, pre-processing,\\nfeature extraction and matching [3]. Among them, feature extraction plays a crucial role. According to the different feature\\nextraction methods, the existing ﬁnger vein feature extraction methods can be roughly divided into two groups: vein pattern-\\nbased methods and local binary-based methods. These methods perform well in most cases.\\nRelative to the above-mentioned custom features, deep features learned from convolutional deep neural networks have been\\nshown to have better generalization and representation [4],[5]. Recent studies have shown that deep convolutional neural\\nnetworks (CNNs) have outstanding performance in the ﬁeld of image understanding and recognition [6],[7], which motivated\\nus to employ deep CNNs for feature learning and selection of ﬁnger vein images [8]. For example, Hong et al. [9] applied a\\npre-trained CNN model of VGG-Net-16 for ﬁnger vein validation. Yin et al. [10] ﬁne-tuned VGG-Net-16 and achieved good\\nrecognition accuracy on two publicly available databases of ﬁnger veins. Besides, there are many studies [11],[12],[13] devoted\\nto improve the results by increasing the depth or width of neural networks, although all of them achieved good results, but\\nthe network is too deep or too wide, which is a great test for the computer’s computational power.\\nZ. Zhang and M. Wang are with School of Mathematics, Southwest Jiaotong University, Chengdu, 610031, China. E-mail: 1065947699@qq.com,\\nwangmw@swjtu.edu.cn.\\nThis work was partially supported by the Fundamental Research Funds for the Central Universities under Grant 2682021ZTPY100, in part by the Science\\nand Technology Support Project of Sichuan Province under Grant 2020YFG0045 and 2020YFG0238.\\nCorresponding author: M. W. Wang.\\narXiv:2202.06673v1  [cs.CV]  14 Feb 2022\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n2\\nIt is known that an important feature of the human visual system is that it does not try to process the whole scene\\nimmediately, but selectively focuses on salient parts in order to better capture the visual structure. Attention can be directed to\\nfocus, and expressivity can be improved by using attentional mechanisms, i.e., focusing on important features and suppressing\\nunnecessary ones. The convolutional block attention module (CBAM) [14] is a simple and effective attention module for\\nfeedforward convolutional neural networks that can attend to important information in channels and spaces separately, which\\nnot only saves parameters and computational power, but also ensures its integration into existing network architectures as a\\nplug-and-play module.\\nMotivated by the success of CNN and CBAM, we propose a novel and effective feature representation of lightweight\\nCNN based on CBAM blocks for the ﬁnger recognition to save computational power. By embedding CBAM in the designed\\nlightweight CNN architecture, the accuracy is improved while ensuring small computational power. Speciﬁcally, the image is\\nfed into a lightweight base CNN architecture, and the initial features of the ﬁnger vein are extracted using the powerful feature\\nrepresentation capability of CNN. Then, CBAM blocks are embedded in the original network to infer the attention mapping\\naccording to two independent dimensions - channel and spatial order, and the attention mapping is multiplied into the input\\nfeature mapping with adaptive feature reﬁnement. Finally, the output features are classiﬁed using the softmax function, and the\\nmethod proposed in this paper has signiﬁcant improvements in two different databases. The main contributions are as follows:\\n(1) To our knowledge, this paper is the ﬁrst to successfully apply lightweight CBAM blocks into CNN for ﬁnger vein\\nrecognition.\\n(2) We combine a convolutional block attention module (CBAM) with a lightweight CNN to simulate visual attention\\nmechanisms and enhance the ﬂow of information in channels and spaces.\\n(3) Our network structure is simple and guarantees the smallest possible computation without sacriﬁcing network performance.\\nExperimental results show that the scheme in this paper has competitive potential in ﬁnger vein recognition systems.\\nThe rest of the paper is organized as follows. Section II is to describe the related work. Section III presents the theoretical\\nbackground of CNN and CBAM. Proposed approach is discussed in Section IV. In Section V, we present experimental results.\\nFinally, Section VI concludes the paper.\\nII. RELATED WORK\\nUndoubtedly, a proper feature extraction method in ﬁnger vein recognition systems can be of great beneﬁt in improving the\\nperformance [15],[16]. How to effectively extract discriminative features remains a major challenge for ﬁnger recognition. The\\nexisting ﬁnger feature extraction methods can be broadly classiﬁed into two categories [17]: traditional recognition methods\\nand deep learning methods.\\nTraditional recognition methods generally include subspace learning-based methods, vein pattern-based, detail point matching-\\nbased, and local feature-based methods.\\n(1) Subspace learning approaches are based on machine learning methods to reduce the dimensionality of global features\\nand ﬁlter noise at the same time, such as principal component analysis (PCA) [18], linear discriminant analysis (LDA)\\n[19] and sparse representation (SR) [20], all of which transform texture images into different subspaces and generate\\nfeature vectors from individual coefﬁcients of the subspace to accomplish texture recognition. Wang et al. [18] combined\\nthe traditional 2D-PCA and 2D-FLD (Fisher linear discriminant) techniques, Wu and Liu [19] implemented ﬁnger vein\\nclassiﬁcation using PCA and LDA, and Xin et al. [20] successfully applied SR to the ﬁnger vein recognition task. These\\nPCA, LDA and SR-based methods can reduce the preprocessing steps and have a smaller space occupation of the feature\\nvector. However, they extract features from a global perspective and do not provide sufﬁcient description of local feature\\ninformation.\\n(2) Vein-based methods segment the vein pattern from the ﬁnger vein image and match it by geometry or topology. Typical\\nmethods include the mean curvature method [21], the maximum curvature (MC) point method [22], the repetitive line\\ntracing method (RLT) [23], and morphological operations combined with the mean Gabor method [1],[24],[25], etc. The\\nRLT [23] method extracts the vein network by calculating the difference between the central pixel value and the pixel value\\nin the corresponding range, which has high time complexity and does not take into account the symmetry and continuity of\\nthe vein pattern. Miura et al. [22] modiﬁed the RLT method by calculating the maximum curvature information during vein\\ntracing. Subsequently, both literature [21] and [26] also used the curvature method to extract ﬁnger vein features. Kumar\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n3\\net al. [1] used Gabor ﬁlter to extract vein patterns. Although Gabor ﬁlter is powerful in image texture analysis, it causes\\ninformation loss for low quality vein images. Recently, Yang et al. [27] proposed a ﬁnger vein code indexing method and\\ncombined it with ﬁnger vein pattern matching method into an integrated framework. And the ﬁnal experimental results\\nshow that the integrated framework does improve the recognition accuracy. However, the method, like other vein-based\\nmethods, requires segmentation of ﬁnger vein images, which is greatly affected by the quality of ﬁnger images.\\n(3) Detail point matching-based methods extract stable structures such as intersections and endpoints of vessels as feature\\npoints to calculate the similarity of two matched images, including the improved Hausdorff distance matching method\\n[28], the singular value decomposition-based detail matching method [29]. Besides, the scale-invariant feature transform\\n(SIFT)-based method [30],[31] can automatically extract feature points from ﬁnger vein images for matching, and is\\nusually regarded as a method based on minutiae points. Recently, Meng et al. [32] combined detail point matching with\\nthe traditional region of interest (ROI)-based method to select details of reasonable neighborhoods for matching, which\\navoids mismatch to some extent and is more stable in detail matching. Use of above features for vein matching usually\\nshows poor performance for low-quality images due to the presence of spurious minutiae features. In addition, employing\\nminutiae features are sensitive to changes in ﬁnger pose [33].\\n(4) Local feature-based methods, such as local binary patterns (LBP) [34],[35], local line binary patterns (LLBP) [36], local line\\ndirectional patterns (LLDP) [37], and local graph structures (LGS) [38],[39], have been widely used in ﬁnger biometrics.\\nFor example, Dong et al. [39] proposed a multi-directional weighted symmetric local graph structure (MoW-SLGS)\\noperator for ﬁnger vein recognition. Unfortunately, the operator assigns different weights to symmetric pixels, which leads\\nto an unbalanced feature representation between the left and right sides. Liu et al. [40] stated a multi-directional local\\nline binary pattern (PLLBP) method that makes full use of the discriminative power of the LLBP histogram in different\\ndirections. And the method can be used to extract vein line patterns in any direction. Recently, Al-Nima et al. [41] proposed\\nthe multi-scale Sobel angular local binary pattern (MSALBP) for the feature extraction algorithm of ﬁnger texture images.\\nThis method combines the Sobel operator with a multi-scale local binary pattern, which is computed statistically for each\\nimage block to form a texture vector as an input to an artiﬁcial neural network (ANN). In general, although these local\\nfeature-based methods can better capture the local information of the image, the methods only consider the relationship\\nbetween the target pixel and its surrounding pixels, ignoring the hidden relationship between surrounding neighboring\\npixels. In other words, the uniqueness of ﬁnger images will not be effectively expressed by relying only on hand-crafted\\nfeatures.\\nAlthough the traditional recognition method has achieved good results [42], but its recognition process is more complicated,\\nand the extracted features are the shallow features of the image, which are easily affected by the image quality. Deep learning\\nmethods are not easily affected by image quality, have powerful image processing without any prior knowledge, and perform\\nwell in noisy image processing and adaptive learning of feature representations. In the ﬁeld of ﬁnger vein recognition, deep\\nlearning method has been successfully applied in recent years [43]. For example, Huang et al. [44] designed a new ﬁnger vein\\nveriﬁcation method with deep convolutional neural network, which works well for ﬁnger vein pattern matching. In [45], Das et\\nal. developed a CNN-based ﬁnger-vein identiﬁcation system, which performed an effective identiﬁcation, but did not consider\\nfactors such as training time and model size. Beisides, Ahmad Radzi et al. [46] used a four-layer CNN to recognize ﬁnger\\nveins. The recognition accuracy reached 100%, but the results were obtained in their own database, which is not universal. And\\nLi et al. [47] used improved GCNs to recognize ﬁnger veins and obtained better recognition accuracy, but image preprocessing\\nwas needed to construct a weighted map of ﬁnger veins. He et al. [48] enhanced the feature extraction ability of the network\\nby adding convolution layer, which improved the recognition accuracy with less training samples, and also increased the\\ncomplexity of the network.\\nIII. METHODOLOGY\\nCNN has been successfully applied in the ﬁeld of computer vision, demonstrating its powerful ability to represent features.\\nIt uses multiple ﬁlters that share different parameters to extract image features. Recently, the attention mechanism has received\\nincreasing attention for their focus on important details and their ability to better capture visual structure. In our study, a CNN\\nwith a convolutional attention module (CBAM) is proposed. It uses a basic CNN to extract the overall features of all input data.\\nThen, the CBAM processes the feature map and adaptively assigns weights to the channel dimensions and spatial dimensions.\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n4\\nConv\\nReLU\\nBN\\nMaxPool\\nConv 1\\nConv 2\\nBasic CNN\\nFlatten\\nDatabase\\nCAM\\nSAM\\nCBAM\\nSoftmax\\nFig. 1: General framework of the network for the proposed approach\\nBoth the basic CNN architecture and CBAM are indispensable because the basic network architecture focuses on the global\\ninformation, while CBAM highlights its features. These two parts complement each other and complete the neural network.\\nFig. 1 shows the complete framework of the proposed network, and we explain the components of the framework (i.e., the\\nbasic CNN, CBAM and output layer) in detail below.\\nA. The Basic CNN\\nIn our work, considering the problem of overﬁtting when training with small data in a too deep network, we designed\\na lightweight CNN with the network parameters set as shown in Table I. The lightweight CNN consists of a total of two\\nconvolutional modules, each with a convolutional layer, a normalization layer, an activation function layer, and a pooling layer.\\nTABLE I: Details of the proposed network framework\\nLayer Type\\nNumber of\\nﬁlters\\nSize\\nOutput size\\nInput\\n−\\n−\\n1 × 81 × 333\\nConv 1\\n16\\n5 × 5\\n16 × 81 × 333\\nBN\\n1\\n−\\n16 × 81 × 333\\nReLU\\n1\\n−\\n16 × 81 × 333\\nPool 1\\n1\\n3 × 3\\n16 × 27 × 111\\nConv 2\\n32\\n5 × 5\\n32 × 27 × 111\\nBN\\n1\\n−\\n32 × 27 × 111\\nReLU\\n1\\n−\\n32 × 27 × 111\\nPool 2\\n1\\n3 × 3\\n32 × 9 × 37\\nFor convolutional layer, features are extracted by performing a two-dimensional convolution of the input graph and the\\nconvolutional kernel, which can be speciﬁcally expressed by Eq. (1) as:\\ny1 = σ(b +\\nk−1\\nX\\nl=0\\nk−1\\nX\\nm=0\\nwl,mai+l,j+m)\\n(1)\\nwhere σ represents the activation function, such as sigmoid function, etc.; b is the offset value; w is the k × k size shared\\nweight matrix. We use matrix a to represent the input layer neurons, and ax,y to denote the neuron in the (x + 1)-th row and\\n(y + 1)-st column (note that the subscripts here are counted from 0 by default, a0,0 represents the neuron in the ﬁrst row and\\nﬁrst column), so we get Eq. (1) by adding the offset value after the matrix w linear mapping.\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n5\\nThe batch normalization (BN) layer normalizes the feature maps generated by the convolutional layers and sends these feature\\nmaps to activation function to speed up the training process. As for the activation function, rectiﬁed linear units (ReLU) were\\nchosen in Eq. (1). ReLU is introduced as the activation function of the hidden layer instead of the traditional sigmoid unit\\nbecause ReLU is better at capturing patterns in natural images and improves the ability of the neural network to solve the\\nimage denoising problem.\\nThe pooling layer compresses the information in the original feature layer so that the input representation can be more\\ncompact. In general, there are two operations: maximum pooling and average pooling. Max-pooling simply reduces the\\ndimensionality of the data by taking only the largest data, while average-pooling works in a similar way by taking the\\naverage of the inputs instead of the maximum. Based on the conceptual differences between these two methods, max-pooling\\nis sensitive to the texture information of the image, while the average pooling method retains more background information\\nof the image. Therefore, max-pooling is more beneﬁcial for extracting the feature information of the image. In this paper,\\nmax-pooling is used after calculating the ReLU output.\\nB. Convolutional Block Attention Module (CBAM)\\nThe above basic CNN network transforms the image data X into a feature map F as the input to CBAM. The CBAM\\ncontains two independent sub-modules, channel attention module (CAM) and spatial attention module (SAM), which can save\\nparameters and computational power by performing attention mechanism on channel and space respectively, as shown in Fig.\\n2.\\nCAM\\nSAM\\nFeature\\nRefined feature\\n\\uf0c4\\n\\uf0c4\\nFig. 2: Convolutional block attention module\\nCBAM inferred the attention maps one at a time along two independent dimensions (channel and space) and then multiplied\\nthe attention maps by the input feature maps for adaptive feature reﬁnement. The process of the input feature map F being\\nprocessed in CBAM can be summarized as:\\nF′ =Mc(F) ⊗F\\nF′′ =Ms(F′) ⊗F′\\n(2)\\nwhere ⊗denotes element multiplication, F′ denotes the result of multiplying the feature map with the channel attention map,\\nand F′′ is the ﬁnal reﬁned output.\\nBelow, we will detail how the two separate dimensions, CAM and SAM, work.\\n1) CAM: To accomplish feature extraction and reduce data loss, the channel attention module compresses the feature maps\\non the spatial dimension using the global average pooling layer and the global maximum pooling layer. Fig. 3 shows the\\nchannel attention module. The global average pooling layer obtains the overall information, while the global maximum pooling\\nlayer obtains the feature variance information. The combination of these two layers is better than any layer.\\nFeature\\nAvgpool\\nMaxpool\\nShared MLP\\nChannel Attention\\nFig. 3: Channel attention module\\nThe compressed FC\\navg and FC\\nmax descriptors are then sent to a shared network to generate our channel attention map\\nMc ∈RC×1×1. The shared network consists of a multi-layer perceptron (MLP) with a hidden layer. To reduce the parameter\\noverhead, the hidden activation size is set to RC/r×1×1, where r is the reduction rate. After the shared network is applied to\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n6\\neach descriptor, we use element summation to merge the output feature vectors. In short, the channel attention is calculated\\nas Eq. (3):\\nMc(F) = σ(MLP(AvgPool(F)) + MLP(MaxPool(F)))\\n= σ(W1(W0(FC\\navg)) + W1(W0(FC\\nmax)))\\n(3)\\nwhere σ denotes the sigmoid function, W0 ∈RC/r×C, and W1 ∈RC×C/r. Note that the MLP weights W0 and W1 are\\nshared between the two inputs, and the ReLU activation function is followed by W0.\\n2) SAM: The feature map F′ output from CAM is used as the input feature map of this module. First, we perform global\\nmax pooling and global average pooling based on channels to obtain two H × W × 1 feature maps, and then we do concat\\noperation (channel splicing) on these two feature maps based on channels. Next, after a 7 × 7 convolution (7 × 7 is better than\\n3×3 ) operation, it is reduced to one channel, i.e., H ×W ×1. After that, the spatial attention feature is generated by sigmoid\\nfunction. Finally, this feature is multiplied with the input feature of the module to get the ﬁnal generated feature. Speciﬁcally,\\nas shown in Fig. 4, the calculation process is as follows Eq. (4):\\nMs(F) = σ(f 7×7([AvgPool(F); MaxPool(F)]))\\n= σ(f 7×7[Fs\\navg; Fs\\nmax])\\n(4)\\nwhere σ is a sigmoid function and f 7×7 denotes a convolution operation with a ﬁlter size of 7 × 7.\\nChannel refined \\nfeature\\n[MaxPool,AvgPool]\\nSpatial attention\\nFig. 4: Spatial attention module\\nCAM utilizes the inter-channel relationships of features to generate a channel attention map. As each channel of a feature\\nmap is considered as a feature detector, channel attention focuses on ‘what’ is meaningful given an input. SAM uses spatial\\nrelationships between features to generate spatial attention maps. Unlike CAM, spatial attention focuses on the ‘where’ is an\\ninformative part, which is complementary to CAM.\\nC. Output layer\\nThe feature map F is reﬁned into F′′ after going through the CAM and SAM modules. F′′ is the ﬁnal feature obtained\\nfrom our proposed network framework. Additional dense layers are needed in our network to recreate the data structure used\\nfor classiﬁcation. Softmax is used as the ﬁnal activation because it is a sigmoid equivalent, however with traditionally better\\nresults and a normalized output essential for classiﬁcation problems with multiple classes.\\nThe deﬁnition of the Softmax function (with the i-th node output as an example) is given below:\\nSoftmax(zi) =\\nezi\\nPC\\nc=1 ezc\\n(5)\\nwhere zi is the output value of the i-th node and C is the number of output nodes, i.e., the number of categories of the\\nclassiﬁcation. The Softmax function allows converting the output values of multiple categories into a probability distribution\\nranging from [0, 1] and summing to 1. In classiﬁcation problems, we want the model to assign probabilities close to 1 to\\nthe correct category and 0 to the others, which is difﬁcult to achieve if we use linear normalization methods. Softmax has a\\n‘two-step’ strategy of discretization followed by normalization, so it has a signiﬁcant advantage in classiﬁcation problems.\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n7\\nIV. EXPERIMENTAL RESULTS AND ANALYSIS\\nA. Database\\nIn this section, we will discuss our experiments to conﬁrm the effectiveness of our proposed method and show the observed\\nresults. In our experiment, we used vein images from two public ﬁnger vein databases: the Hong Kong Polytechnic University\\ndatabase (HKPU) [1] and the University Sains Malaysia database (USM) [49]. The characteristics of the databases used in our\\nexperiment are shown in Table II.The details of these databases are given below:\\nTABLE II: The details of databases\\nDatabase\\nFinger\\nnumber\\nImage number\\nper ﬁnger\\nSize of raw\\nimage (pixels)\\nROI image\\nHKPU\\n312\\n6\\n513 × 256\\nExtract using\\nthe method of [1]\\nUSM\\n492\\n12\\n640 × 480\\nDatabase includes\\nROI images\\n(1) HKPU Database: The images of the database were collected from 156 volunteers. Every volunteer only collected the\\nindex ﬁnger and middle ﬁnger of his left hand, totaling 312 ﬁngers. Each ﬁnger collected 6 images. Each of ﬁrst 210 ﬁngers\\nhas 12 images, captured in two separate sessions, and others each has 6 images, captured in one session. All of the images\\nare 8-bit gray level BMP ﬁles with a resolution of 513 × 256 pixels. In our experiments, we use 6 samples each ﬁngers and\\na total of 312 ﬁnger types were obtained. We used the method in the literature [1] to perform ROI operations on the original\\nimages and obtained an image size of 333 × 81.\\n(2) USM Database: The database of Universiti Sains Malaysia (USM) consists of 492 ﬁngers, and every ﬁnger provided 12\\nimages. The spatial resolution of the captured ﬁnger images were 640 × 480. This database also provides the extracted ROI\\nimages with a size of 300 × 100 for ﬁnger vein recognition using their proposed algorithm described in [49].\\nB. Parameter Settings and Experiments\\nIn our method, we adopt accuracy as a measure of model performance, the formula is as follows:\\nAccuracy = Total number of correct identiﬁcations\\nTotal number of samples\\n(6)\\nWe implement the proposed method using Pytorch framework and conduct the experiments on a common desktop PC with\\ni7 3.60 GHz CPU. In the proposed model, Adam’s method is chosen as the optimizer considering the performance and training\\ntime because of its good performance in improving traditional gradient descent and facilitating hyperparameter dynamic tuning.\\nThe learning rate of the model was set to 0.0001 and the batch size was 36. In the next experiments, the number of epochs was\\ndeﬁned as 20. For training purposes, 70% of the ﬁnger vein images were randomly selected as the original training samples and\\nthe remaining 30% were considered as the test samples. As a rough estimate because the proposed networks are lightweight\\nand easily trained, it takes less than half an hour to train.\\nC. Experiments\\nIn this subsection, we will ﬁrst show our training process and the results obtained. Fig. 5 shows our loss curve for training\\non two publicly available data, and Fig. 6 shows the accuracy curve. The higher number in the number of epochs usually\\nallows the network to be well trained so that the weights of the different layers can be updated accurately. Only 20 epochs\\nare considered for all our experiments, but we can still observe encouraging results Fig. 6 that our validation set accuracy can\\nreach 100%, which shows the effectiveness of our method.\\nIn addition, both the proposed network and the basic network were tested on the HKPU database. Table III shows the\\nrecognition accuracy of the single network (basic network) and our method on HKPU. The basic network uses CNN as the\\nonly method to extract the overall features of ﬁnger veins. The accuracy of this method is 98.11%. In comparison, the proposed\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n8\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\nepoch\\n0.000000\\n0.015050\\n0.030100\\n0.045150\\n0.060200\\n0.075250\\n0.090300\\n0.105350\\n0.120400\\n0.135450\\n0.150500\\n0.165550\\nLoss\\nTrain loss\\nVal loss\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\nepoch\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nacc\\nTrain acc\\nVal acc\\n(a)\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\nepoch\\n0.000000\\n0.015050\\n0.030100\\n0.045150\\n0.060200\\n0.075250\\n0.090300\\n0.105350\\n0.120400\\n0.135450\\n0.150500\\n0.165550\\nLoss\\nTrain loss\\nVal loss\\n0\\n2\\n4\\n6\\n8\\n1\\nep\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nacc\\n(b)\\nFig. 5: Loss curves on two public databases: (a) HKPU, (b) USM.\\n10\\n12\\n14\\n16\\n18\\n20\\noch\\nTrain loss\\nVal loss\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\nepoch\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nacc\\nTrain acc\\nVal acc\\n(a)\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\nepoch\\n0.000000\\n0.015050\\n0.030100\\n0.045150\\n0.060200\\n0.075250\\n0.090300\\n0.105350\\n0.120400\\n0.135450\\n0.150500\\n0.165550\\nLoss\\nTrain loss\\nVal loss\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\nepoch\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nacc\\nTrain acc\\nVal acc\\n(b)\\nFig. 6: Accuracy curves on two public databases: (a) HKPU, (b) USM.\\nnetwork achieved 100% recognition rate. The time in the table is the total time spent for training 20 epochs plus validation,\\nand the difference in time between the two methods is 13 seconds, indicating the lightweight nature of the proposed model.\\nOverall, the network with the convolutional block attention module in the basic CNN framework improves the recognition\\nperformance.\\nTABLE III: Comparison results with basic CNN\\nArchitecture\\nAccuracy\\nTime(Training + veriﬁcation)\\nBasic CNN\\n98.11%\\n670s\\nOur approach\\n100%\\n683s\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n9\\nD. Comparison with the Existing Methods\\nFinger vein recognition technology has been developed for more than a decade and several advanced algorithms have been\\nproposed. In this experiment, we have divided the existing methods into traditional methods and deep learning methods and\\ncompared them with our proposed method.\\nTable. IV shows the results of the comparison with some classical traditional ﬁnger vein recognition algorithms. The results\\nshow that our scheme signiﬁcantly outperforms some algorithms, e.g., literature [1],[18],[20],[22],[23],[34], while there is almost\\nno difference in recognition accuracy compared to scheme [15],[16], but our method does not require additional processing of\\nﬁnger vein images, which greatly reduces the time cost.\\nTABLE IV: Comparison results with traditional methods\\nDatabase\\nMethod\\nAccuracy\\nHKPU\\nLBP [34]\\n83%\\nGabor [1]\\n90.08%\\nMC [22]\\n85.24%\\nRLT [23]\\n78.28%\\nWeighted vein indexing [15]\\n99.68%\\nThis Paper\\n100%\\nUSM\\nWLBP [16]\\n93.8%\\nPCA [18]\\n94.7%\\nSR [20]\\n95.1%\\nWeighted vein indexing [15]\\n99.93%\\nCPBFL-BCL [2]\\n99.98%\\nThis Paper\\n100%\\nTABLE V: Comparison results with neural network methods\\nMethod\\nAccuracy\\nNumber of\\nconvolution\\nNumber of\\nﬁlters\\nKernel size\\nHKPU\\nUSM\\nCNN\\nHong et al. [9]\\n−\\n95.83%\\n13\\n64, 128,\\n256, 512\\n3 × 3\\nCNN with\\noriginal images [45]\\n95.3%\\n97.53%\\n5\\n153, 512,\\n768, 1024\\n5 × 5, 4 × 15,\\n1 × 1\\nCNN with CLAHE\\nenhanced images [45]\\n94.37%\\n97.05%\\nImproved CNN [8]\\n−\\n98%\\n3\\n8, 16, 32\\n5 × 5, 3 × 3, 3 × 3\\nTwo-stream\\nCNN [13]\\n−\\n95.45%\\n3, 3\\nNet1: 20, 50, 200\\nNet2: 20, 50, 200\\nNet1: 9 × 9, 10 × 26\\nNet2: 7 × 7, 4 × 7\\nBasic CNN\\n98.11%\\n−\\n2\\n16, 32\\n5 × 5\\nOther NN\\nDeep Belief\\nNetwork CGN\\n−\\n97.4%\\n−\\n−\\n−\\nAlexNet\\n−\\n92.28%\\nResNet-18\\n−\\n96.04%\\nCAE+CNN [5]\\n−\\n99.49%\\nOur method\\n100%\\n100%\\n2\\n16, 32\\n5 × 5\\nCompared with classical networks, such as AlexNet ResNet-18, Basic CNN, the reason for the high accuracy of our scheme\\nis that the weights are assigned adaptively through the attention mechanism, which highlights the important detail information\\nof the image and the extracted features are more distinguishable. The network layer setup in literature [8] is similar to our\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n10\\nmodel structure, but it needs to extract curvature by Gaussian template and use certain methods to extract feature images as\\ninput to CNN, which is time costly and computationally not small. Other literature, such as [9],[13],[45], shows from the\\nnetwork structures listed in Table V that the structures of these schemes are not as simple as those of the proposed scheme.\\nCombining Table III and Table V, it is obvious that our scheme is less computationally intensive and takes less time to train.\\nFrom the experimental data, the ideas and methods of this paper are completely correct and effective.\\nV. CONCLUSION\\nAccording to the characteristics of ﬁnger veins, this paper adds CBAM to the basic CNN framework for ﬁnger vein\\nrecognition.The CBAM block further reﬁnes the features extracted by CNN to make the features more distinguishable. Compared\\nwith the classical network model, our model extracts features that better reﬂect image information with higher accuracy, which\\nhas obvious advantages. In the model, we use a simple network structure with lightweight features, less computation and\\nshorter training time. To make our model more practical, in future work, we will consider larger datasets, i.e., in fusing\\nmultiple publicly available datasets into one dataset for learning, which makes the model more robust due to the difference in\\nimage quality.\\nREFERENCES\\n[1] A. Kumar and Y. Zhou, “Human identiﬁcation using ﬁnger images,” IEEE Trans. Image Process, vol. 21, no. 4, pp. 2228–2244, 2012.\\n[2] H. Liu, G. Yang and Y. Yin, “Category-preserving binary feature learning and binary codebook learning for ﬁnger vein recognition,” Int. J. Mach. Learn.\\nCybern., vol. 11, no. 11, pp. 2573–2586, 2020.\\n[3] J. D. Wu and S. H. Ye, “Driver identiﬁcation using ﬁnger-vein patterns with Radon transform and neural network,” Expert Syst. Appl., vol. 36, no. 3,\\npp. 5793–5799, 2009.\\n[4] J. D. Wu and C. T. Liu, “Finger vein pattern identiﬁcation using SVM and neural network technique,” Expert Syst. Appl., vol. 38, no. 11, pp. 14284–14289,\\n2011.\\n[5] B. Hou and R. Yan, “Convolutional auto-encoder based deep feature learning for ﬁnger-vein veriﬁcation,” in Proc. IEEE Int. Symp. Med. Meas. Appl,\\n2018, pp. 1-5.\\n[6] C. Xie and A. Kumar, “Finger vein identiﬁcation using convolutional neural network and supervised discrete hashing,” Pattern Recognit. Lett., vol. 119,\\npp. 148–156, 2019.\\n[7] Z. M. Fang and Z. M. Lu, “Deep belief network based ﬁnger vein recognition using histograms of uniform local binary patterns of curvature gray\\nimages,” Int. J. Innov. Comput. Inf. Control., vol. 15, no. 5, pp. 1701–1715, 2019.\\n[8] J. Y. Zhao, J. Gong, S. T. Ma and Z. Ming Lu2, “Curvature Gray feature decomposition based ﬁnger vein recognition with an improved convolutional\\nneural network,” Int. J. Innov. Comput. Inf. Control., 16, no. 1, pp. 77–90, 2020.\\n[9] H. G. Hong, M. B. Lee and K. R. Park, “Convolutional neural network-based ﬁnger-vein recognition using NIR image sensors,” Sensors, vol. 17, no.\\n6, pp. 1297. 2017.\\n[10] Y. Yin, L. Liu and X. Sun, “ SDUMLA-HMT: a multimodal biometric database,” in Proc. The International Conference on Advanced Computer Control,\\n2011, pp. 260–268.\\n[11] F. J. Xu, V. N. Boddeti and M. Savvide, “ Local binary convolutional neural networks,” in Proc. Int. Conf. Computer Vision Pattern Recognition, pp.\\n1–10, 2017.\\n[12] S. Y. Li, B Zhang, S. P. Zhao and J. F. Yang, “Local discriminant coding based convolutional feature representation for multimodal ﬁnger recognition,”\\nInf. Sci., vol. 547, pp. 1170–1181, 2021.\\n[13] Y. Fang, Q. Wu and W. Kang, “A novel ﬁnger vein veriﬁcation system based on two-stream convolutional network learning,” Neurocomputing, vol. 290,\\npp.100–107, 2018.\\n[14] S. Woo, J. Park, J.Y. Lee and I. S. Kweon, “CBAM: convolutional block attention module,” Computer Vision-ECCV 2018, vol 11211, pp. 3–19, 2018.\\n[15] L. Yang, G. Yang, X. Xi, K. Su, Q. Chen and Y. Yin, “Finger vein code: from indexing to matching,” IEEE Trans Inf Forensics Secur, vol. 14, no. 5,\\npp. 1210–1223, 2018.\\n[16] H. C. Lee, B. J. Kang, E. C. Lee and K. J. Park, “Finger vein recognition using weighted local binary pattern code based on a support vector machine,”\\nJ. Zhejiang Univ. Sc. A, vol. 11, no. 7, pp. 514–524, 2010.\\n[17] K. X. Wang, G. H. Chen and H. J. Chu, “Finger vein recognition based on multi-receptive ﬁeld bilinear convolutional neural network,” IEEE Singal\\nProces. Lett., vol. 28, pp.1590–1594, 2021.\\n[18] J. Wang, H. Li, G. Wang, M. Li and D. Li, “Vein recognition based on 2D 2FPCA,” Internat. J. Signal Proces., Image Proces. and Pattern Recogn.,\\nvol. 6, no. 4, pp. 323–332, 2013.\\n[19] J. Wu and C. Liu, “Finger vein pattern identiﬁcation using SVM and neural network technique,” Expert Syst. Appl., vol. 38, no. 11, pp. 14284–14289,\\n2011.\\n[20] Y. Xin, Z. Liu, H. Zhang and H. Zhang, “Finger vein veriﬁcation system based on sparse representation,” Applied Optics, vol. 51, no. 25, pp. 6252–6258,\\n2012.\\n[21] W. Song, T. Kim and H. C. Kim, “A ﬁnger-vein veriﬁcation system using mean curvature,” Pattern Recognit. Lett., vol. 32, no. 8, pp. 1541–1547, 2011.\\n[22] N. Miura, A. Nagasaka and T. Miyatake, “Extraction of ﬁnger-vein patterns using maximum curvature points in image proﬁles,” IEICE Trans. Inf. Syst.,\\nvol. E90-D, no. 8, pp. 1185–1194, 2007.\\n[23] N. Miura, A. Nagasaka and T. Miyatake, “Feature extraction of ﬁnger-vein patterns based on repeated line tracking and its application to personal\\nidentiﬁcation,” Mach. Vis. Appl., vol. 15, no. 4, pp. 194–203, 2004.\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n11\\n[24] J. Yang and X. Zhang, “Feature-level fusion of ﬁngerprint and ﬁnger-vein for personal identiﬁcation,” Pattern Recognit. Lett., vol. 33, no. 5, pp. 623–628,\\n2012.\\n[25] W. Y. Han and J. C. Lee, “Palm vein recognition using adaptive Gabor ﬁlter,” Expert Syst. Appl., vol. 39, no. 18, pp. 13225-13234, 2012.\\n[26] H. Qin, X. He, X. Yao and H. Li, “Finger-vein veriﬁcation based on the curvature in radon space,” Expert Syst. Appl, vol. 82, pp. 151–161, 2017.\\n[27] L. Yang, G. Yang, X. Xi, K. Su, Q. Chen and L. Yin, “Finger vein code: from indexing to matching,” IEEE Trans. Inf. Forensics Secur., vol. 14, no. 5,\\npp. 1210–1223, 2019.\\n[28] C. Yu, H. Qin, L. Zhang, Y. Cui, “Finger-vein image recognition combining modiﬁed hausdorff distance with minutiae feature matching,” J. Biomed.\\nSci. Engineer., vol. 1, no. 3, pp. 280–289, 2009.\\n[29] F. Liu, G. Yang, Y. Yin and S. Wang, “Singular value decomposition based minutiae matching method for ﬁnger vein recognition,” Neurocomputing,\\nvol. 145, pp. 75–89, 2014.\\n[30] H. Kim, E. Lee and G. Yoon, “Illumination normalization for SIFT based ﬁnger vein authentication,” in Proc. Int. Symp. Vis. Comput., pp. 21–30, 2012.\\n[31] S. Pang, Y. Yin, G. Yang and Y. Li, “Rotation invatiant ﬁnger vein recognition,” Chinese Conference on Biometric Recognition, (CCBR), pp. 151–156,\\n2012.\\n[32] X. Meng, J. Meng, X. Xi, Q. Zhang, and Y. Zhang, “Finger vein recognition based on zone-based minutia matching,” Neurocomputing, vol. 423, pp.\\n110–123, 2021.\\n[33] J. Peng, A. El-Latif, Q. Li and X. Niu, “Multimodal biometric authentication based on score level fusion of ﬁnger biometrics,” International Journal\\nfor Light and Electron Opticsz, vol. 125, no. 23, pp. 6891–6897, 2014.\\n[34] L. Yang, G. Yang, Y. Yin and X. Xi, “Exploring soft biometric trait with ﬁnger vein recognition,” Neurocomputing, vol. 135, pp. 218–228, 2014.\\n[35] C. Liu and Y.-H. Kim, “An efﬁcient ﬁnger-vein extraction algorithm based on random forest regression with efﬁcient local binary patterns,” In Proc.\\nInt. Conf. Image Process, USA, pp. 3141–3145, 2016.\\n[36] B. A. Rosdi, C. W. Shing and S. A. Suandi, “Finger vein recognition using local line binary pattern,” Sensors, vol. 11, no. 12, pp. 11357–11371, 2011.\\n[37] Y. T. Luo, L. Y. Zhao and B. Zhang, “Local line directional pattern for palmprint recognition,” Pattern Recogn., vol. 50, 26–44, 2016.\\n[38] M. F. Abdullah, M. S. Sayeed and K. S. Muthu, “Face recognition with Symmetric Local Graph Structure (SLGS),” Expert Syst. Appl, vol. 41, no. 14,\\npp. 6131–6137, 2014.\\n[39] S. Dong, J. C. Yang and Y. Chen, “Finger vein recognition based on multi-orientation weighted symmetric local graph structure,” Ksii Trans. Internet\\nInf. Syst, vol. 9, no. 10, pp. 4126–4142, 2015.\\n[40] Y. Lu, S. J. Xie, S. Yoon and D. S. Park, “Finger vein identiﬁcation using poly directional local line binary pattern,” In Proc. International Conference\\non ICT Convergence, pp. 61–65, 2013.\\n[41] A, Nima, R. Abdullaha, M. A. Kaltakchi, M. Dlay and L. Woo, “Finger texture biometric veriﬁcation exploiting multi-scale sobel angles local binary\\npattern features and score-based fusion,” Digital Signal Proces., vol. 70, pp. 178–189, 2017.\\n[42] S. Li, B. Zhang, L. Fei and S. Zhao, “Joint discriminative feature learning for multimodal ﬁnger recognition,” Pattern Recognit., vol. 111. no. 107704,\\n2021.\\n[43] F.J. Xu, V. N. Boddeti and M. Savvide, “Local binary convolutional neural networks,” in Proc. Int. Conf. Computer Vision Pattern Recognition, pp.\\n1–10, 2017.\\n[44] H. Huang, S. Liu, H. Zheng, L. Ni, Y. Zhang and W. Li, “DeepVein: Novel ﬁnger vein veriﬁcation methods based on deep convolutional neural networks,”\\nin Proc. IEEE Int. Conf. Identity, Secur. Behav. Anal, pp. 1–8, 2017.\\n[45] R. Das, E. Piciucco, E. Maiorana and P. Campisi, “Convolutional neural network for ﬁnger-vein-based biometric identiﬁcation,” IEEE Trans. Inf. Forensics\\nSecur., vol. 14, no. 2, pp. 360–373, 2019.\\n[46] S. A. Radzi, M. Khalil-Hani and R. Bakhteri, “Finger-vein biometric identiﬁcation using convolutional neural network,” Turkish J. Elect. Eng. Comput.\\nSci, vol. 24, pp. 1863–1878, 2016.\\n[47] R. Li, Z. G. Su, H. G. Zhang and J. F. Yang, “Application of improved GCNs in feature representation of ﬁnger-vein,” J. Signal Process., vol. 36, pp.\\n550–561, 2020.\\n[48] X. He and X. Chen, “Finger vein recognition based on improved convolution neural network,” Comput. Eng. Design, vol. 40, pp. 562–566, 2019.\\n[49] M. S. M. Asaari, S. A. Suandi and B. A. Rosdi, “Fusion of band limited phase only correlation and width centroid contour distance for ﬁnger based\\nbiometrics,” Expert Syst. Appl., vol. 41, no. 7, pp. 3367-3382, 2014.\\n', metadata={'Published': '2022-02-14', 'Title': 'Convolutional Neural Network with Convolutional Block Attention Module for Finger Vein Recognition', 'Authors': 'Zhongxia Zhang, Mingwen Wang', 'Summary': 'Convolutional neural networks have become a popular research in the field of\\nfinger vein recognition because of their powerful image feature representation.\\nHowever, most researchers focus on improving the performance of the network by\\nincreasing the CNN depth and width, which often requires high computational\\neffort. Moreover, we can notice that not only the importance of pixels in\\ndifferent channels is different, but also the importance of pixels in different\\npositions of the same channel is different. To reduce the computational effort\\nand to take into account the different importance of pixels, we propose a\\nlightweight convolutional neural network with a convolutional block attention\\nmodule (CBAM) for finger vein recognition, which can achieve a more accurate\\ncapture of visual structures through an attention mechanism. First, image\\nsequences are fed into a lightweight convolutional neural network we designed\\nto improve visual features. Afterwards, it learns to assign feature weights in\\nan adaptive manner with the help of a convolutional block attention module. The\\nexperiments are carried out on two publicly available databases and the results\\ndemonstrate that the proposed method achieves a stable, highly accurate, and\\nrobust performance in multimodal finger recognition.'}), Document(page_content='An Enhanced Convolutional Neural Network in\\nSide-Channel Attacks and Its Visualization\\nMinhui Jin, Mengce Zheng, Honggang Hu, Nenghai Yu\\nKey Laboratory of Electromagnetic Space Information, CAS\\nUniversity of Science and Technology of China, Hefei 230026, China\\n{jmh123,mczheng}@mail.ustc.edu.cn\\nAbstract. In recent years, the convolutional neural networks (CNNs) have received\\na lot of interest in the side-channel community. The previous work has shown that\\nCNNs have the potential of breaking the cryptographic algorithm protected with\\nmasking or desynchronization. Before, several CNN models have been exploited,\\nreaching the same or even better level of performance compared to the traditional\\nside-channel attack (SCA). In this paper, we investigate the architecture of Residual\\nNetwork and build a new CNN model called attention network. To enhance the\\npower of the attention network, we introduce an attention mechanism - Convolutional\\nBlock Attention Module (CBAM) and incorporate CBAM into the CNN architecture.\\nCBAM points out the informative points of the input traces and makes the attention\\nnetwork focus on the relevant leakages of the measurements. It is able to improve\\nthe performance of the CNNs. Because the irrelevant points will introduce the extra\\nnoises and cause a worse performance of attacks. We compare our attention network\\nwith the one designed for the masking AES implementation called ASCAD network\\nin this paper. We show that the attention network has a better performance than\\nthe ASCAD network. Finally, a new visualization method, named Class Gradient\\nVisualization (CGV) is proposed to recognize which points of the input traces have a\\npositive inﬂuence on the predicted result of the neural networks. In another aspect, it\\ncan explain why the attention network is superior to the ASCAD network. We validate\\nthe attention network through extensive experiments on four public datasets and\\ndemonstrate that the attention network is eﬃcient in diﬀerent AES implementations.\\nKeywords: Side-Channel Attack · Convolutional Neural Network · Attention Mecha-\\nnism · Visualization Method\\n1\\nIntroduction\\nSide-channel attack (SCA) was ﬁrst proposed on several cryptographic systems by Paul\\nKocher in 1996 [Koc96]. Since then, it has aroused great interest in the security community.\\nSCA is a class of cryptanalytic attacks but diﬀerent from traditional cryptanalysis. It\\nexploits the physical properties such as timing, power consumption [KJJ99], electromagnetic\\n(EM) emanation [AARR02] and even sound [GST17]. In the side-channel community, SCA\\nis divided into two categories: proﬁled attack and non-proﬁled attack. In the proﬁled\\nattack, attackers have a copy of the target device. They can fully control of the input\\nplaintexts and secret key of the replicated device and gather suﬃcient measurements\\n(or traces). Then they build a model to describe the physical characteristics. Attackers\\nsubsequently utilize the measurements captured from the target device and perform the\\nkey-recovery process. The proﬁled attack includes Template Attacks (TA) [CRR02] and\\nStochastic models [SLP05]. For the non-proﬁled attack, there is a weaker assumption.\\narXiv:2009.08898v1  [cs.CR]  18 Sep 2020\\n2\\nAn Enhanced Convolutional Neural Network in Side-Channel Attacks\\nAttackers only have a target device, and they do not know the secret key about the\\nencryption algorithm except the plaintexts and ciphertexts. They extract abundant traces\\nfrom the target device and then exploit the statistical analysis technique to recover the\\nsecret key. The non-proﬁled attack includes Diﬀerential Power Analysis (DPA), Correlation\\nPower Analysis (CPA) [BCO04] and Mutual Information Analysis (MIA) [GBTP08]. The\\nattack methods mentioned above are called traditional SCA in this paper. In practical\\nattacks, pre-processing is required in traditional SCA like reducing the noise of the traces\\nor selecting Points of Interests (PoIs) before recovering the secret key. Meanwhile, with the\\ndevelopment of countermeasures such as desynchronization [CDP17] and masking [MPP16],\\ntraditional SCA becomes more diﬃcult.\\nConvolutional neural networks (CNNs) have recently been introduced as a new al-\\nternative method to SCA. Researchers found that the proﬁled attack is similar to the\\nclassiﬁcation problem in CNNs. For the proﬁled attack, attackers build a model to describe\\nthe features in the proﬁling stage which corresponds to the modeling stage in CNNs. Then\\nattackers recover the secret key in the attacking stage which corresponds to the classifying\\nstage in CNNs. Thereby, one aspect of SCA is to exploit CNNs to recover the secret key of\\nembedded devices. Before, CNNs have led to an impressive performance in many diﬀerent\\nﬁelds where massive data are available, such as visual recognition tasks, natural language\\nprocessing, medical data analysis, etc. In the ﬁeld of SCA, CNNs have become one of the\\nmost powerful attack methods. In certain situations, they are even better than traditional\\nSCA. For instance, some recent papers have implied that CNNs are at least as eﬃcient\\nas TA and under some circumstance, even better than TA [ZBHV20, KPH+19], while\\nTA is considered to be the most powerful attack from an information-theoretic point of\\nview. Compared to the traditional SCA, due to the translation invariance and some other\\nnatural characteristics, CNNs are robust to the most common countermeasures like desyn-\\nchronization or masking [CDP17,ZBHV20,KPH+19,BPS+18]. Another advantage is that\\nCNNs are eﬃcient to deal with the multi-classiﬁcation problems to implement end-to-end\\nclassiﬁcations. Moreover, CNNs can automatically extract features, so pre-processing is\\nnot needed.\\nIn SCA, there exist the environment noises in the measurements. Traditional SCA and\\nCNNs are all expected to focus on the informative points of the traces as far as possible.\\nBecause the irrelevant points will introduce extra noises and cause a worse performance of\\nattacks. For the traditional SCA, it uses Principal Components Analysis (PCA) or Kernel\\nDiscriminant Analysis (KDA) to select the PoIs where most information is contained. But\\nfor the proﬁled SCA based on CNNs, it has not been considered yet. So in this paper,\\nwe propose a new CNN model. In the model, we introduce an attention mechanism -\\nConvolutional Block Attention Module. CBAM can make the model focus on the important\\npoints and suppressing unnecessary points. Then we proposed a visualization technique\\ncalled Class Gradient Visualization to verify the eﬀectiveness of the new CNN model.\\n1.1\\nRelated Work\\nWe introduce the application of neural networks in SCA in previous work. One of the\\nﬁrst applications of neural networks in SCA was presented in [YZLC11]. Subsequently,\\nthe applications of multiple-layer perceptron (MLP) in SCA have been proposed in\\n[MHM13, MZVT15]. In [CDP17], Cagli et al. proposed a data augmentation method\\nand noted that CNN can deal with the traces misalignment. [KPH+19] addressed how\\nto improve the performance of the neural network by adding the artiﬁcial noise to input\\ntraces. [ZS19] showed that the synchronization method is still useful to CNNs. [YLMZ18]\\nexploited the time-frequency patterns of side-channel traces and perform a successful\\nattack based on CNNs. In [HGG18], Hettwer et al. used the domain knowledge to recover\\nthe secret key without any assumption about the leakage.\\nAnother topic is to explain how the trained CNNs work and use the trained models\\nMinhui Jin et al.\\n3\\nto estimate the informative points of the input trace. [MDP19] proposed a gradient\\nvisualization method to localize PoIs based on a successful trained neural network. It\\ndepends on the gradient of the loss function concerning the components of the input\\ntrace. In [Tim19], Timon et al. introduced a Sensitivity Analysis to reveal the secret\\nkey and PoIs of the input trace. The method is similar to the gradient visualization\\nin [MDP19]. In [ZBHV20], Zaid et al. explained the role of hyperparameters using some\\nspeciﬁc visualization techniques including Weight Visualization, Gradient Visualization,\\nand Heatmap. [HGG19] investigated three attribution methods especially the Layer-wise\\nRelevance Propagation (LRP) to reveal the PoIs of the input traces. [PEC19] evaluated\\nthe neural network using a backward propagation path method. It can verify what the\\nneural network learned from the side-channel traces.\\n1.2\\nOur Contributions\\nIn this paper, our main contributions are summarized as follows.\\n1. We investigate the architecture of Residual Network and propose a new CNN model.\\nTo reduce the noises introduced by irrelevant points, we enhance the attention\\nnetwork by introducing an attention mechanism — Convolutional Block Attention\\nModule. CBAM can make the CNN models focus on the informative part.\\n2. We propose a new visualization method called Class Gradient Visualization (CGV)\\nto recognize what the networks focus on in the training process and verify the\\neﬀectiveness of the attention network.\\n3. We validate the attention network through extensive experiments on four publicly\\navailable datasets, and the results show that the new CNN model is eﬃcient in\\ndiﬀerent AES implementations.\\n1.3\\nOrganizations\\nThe rest of the paper is organized as follows. In Section 2, we introduce the notations of\\nSCA. Then introduce the proﬁled attack and evaluation metrics. At last, we introduce\\nan attention mechanism. Section 3 introduces the architecture of the new CNN model\\nand the application of attention mechanism in the new network model. We perform the\\nexperiments to evaluate the performance of the new CNN model in Section 4. In Section 5,\\nwe proposed a new visualization method and implement the visualization of network\\nmodels. Finally, a conclusion is present in Section 6.\\n2\\nPreliminaries\\nThe ﬁrst part of this section provides notations of the side-channel attack and the subsequent\\nparts introduce the proﬁled attack and the metric which is used to evaluate attacking\\nresult. At last, we brieﬂy introduce an attention mechanism — Attention Module in\\nAttention Network.\\n2.1\\nNotations\\nIn this paper, we use the calligraphic letter X to denote sets and the corresponding upper-\\ncase letter X to denote random variable. In SCA, the random variable is constructed\\nas X ∈R1×D, where D denotes the dimension of the traces. The lower-case letter ⃗\\nx\\ndenotes the realization of X. The i-th entry of a vector ⃗\\nx is denoted by ⃗\\nx[i] and the i-th\\nobservation of a random variable X is denoted by ⃗\\nxi. For the encryption algorithm, there\\nis Z = f(P, K), where f denotes the cryptographic primitive and Z is the target sensitive\\n4\\nAn Enhanced Convolutional Neural Network in Side-Channel Attacks\\nvariable. P denotes the public variable (e.g. plaintext or ciphertext) and K is the secret\\nkey that the attacker aim to retrieve. We denote k∗as the secret key of the cryptographic\\nalgorithm and k is any possible key hypothesis.\\n2.2\\nProﬁled Attack\\nFor the proﬁled attack, there are two phases: proﬁling phase and attacking phase. During\\nthe proﬁling phase, attackers have a copy of the target device. They set the plaintexts and\\nthe secret key of the cryptographic algorithm in the replicated device and collect suﬃcient\\nmeasurements X. For each measurement ⃗\\nx, attackers compute the sensitive variable z\\nwith the known information of the cryptographic algorithm. Attackers aim to estimate the\\nprobability\\nPr[X = ⃗\\nx|Z = z]\\n(1)\\nby generating a model F : RD →R|Z| from the proﬁling set {(⃗\\nxi, zi)}i=1,...,Np, where Np\\ndenotes the number of measurements. Then in the attacking phase, attackers randomly\\nchoose plaintext pi of the target device and collect some measurements. The attacking\\nset is {(⃗\\nxi, pi)}i=1,...,Na, where Na is the number of measurements. The secret key k∗is\\nunknown but ﬁxed. Aimed to recover the secret key, attackers utilize the attacking set\\nand the model F(·) built in the proﬁling phase and compute the score vector F(X) of\\nthe sensitive variable Z. Then they use the Maximum Likelihood strategy to compute the\\npredicted probability of each key candidate. The key that has the maximum probability is\\nconsidered as the recovered key k. when the secret key k∗is exactly the recovered key k,\\nthe proﬁled attack is successful.\\n2.3\\nEvaluation Metric\\nIn SCA, the common metrics to assess the performance of models are the Success Rate\\n(SR) and Rank [BPS+18]. In this paper, we use the Rank to evaluate the performance of\\nmodels. Given Na attacking measurements, the key guess vector is ⃗\\ng = {g1, g2, . . . , g|K|}\\nin descending order of the predicted probability, where |K| denotes the size of the keyspace.\\ng1 is considered as the recovered key of the models. the key guess vector is calculated by\\nthe log form of maximum likelihood strategy, i.e.,\\ngi =\\nNa\\nX\\nj=1\\nlog(ˆ\\npij),\\n(2)\\nwhere ˆ\\npij denotes the predicted probability of i-th key candidates in the j-th attacking\\nmeasurement. Rank denotes the average position of k∗in ⃗\\ng. When Rank is equivalent to\\n0, it represents that k∗is equivalent to g1 and implies this attack is successful. The aim of\\nour attack is to use the minimum number of attacking traces to achieve a successful attack.\\n2.4\\nConvolutional Block Attention Module\\nAttention mechanisms are inspired by the human visual system. One does not attempt to\\nconcentrate on the whole scene immediately. Instead, humans selectively focus on some\\nimportant parts and recognize the scene [ZF14]. In [WPLSK18], Woo et al. proposed\\na Convolutional Block Attention Module (CBAM) for feed-forward convolutional neural\\nnetworks. CBAM separately underlines features along two dimensions: channel and spatial.\\nChannel attention focuses on the meaningful intermediate features, and spatial attention\\nstress where is the informative part.\\nFor channel attention module, the module independently uses a Global Average Pooling\\n(GAP) operation and a Global Max Pooling (GMP) operation to extract the average\\nMinhui Jin et al.\\n5\\nfeatures and max features of the input features. Then both features are sent to a shared\\nlayer composed of MLP which has a hidden layer and produces the channel attention\\nmaps. The maps are considered to be the weight of each feature. Finally, an element-wise\\nmultiplication is computed between the input feature and the channel attention maps. In\\nshort, channel attention can be characterized by the following formula.\\nMc(F) = σ(MLP(GAP(F)) + MLP(GMP(F))) ⊗F\\n(3)\\nwhere σ denotes the sigmoid function and ⊗denotes the element-wise multiplications. Mc\\nand F denote the input and output of the channel attention module.\\nFor spatial attention module, the module ﬁrst applies an average-pooling operation\\n(AP) and a max-pooling (MP) operation along the channel axis of input features and\\ngenerate an average samples and a max samples. Then it concatenate these two samples\\nand apply a convolutional layer to generate the spatial attention maps. This maps can\\nbe viewed as the weight of each time points. Finally, an element-wise multiplication is\\ncomputed between the input features and the spatial attention maps. Spatial attention\\nmodule is computed as:\\nMs(F) = σ(γ([AP(F); MP(F)])) ⊗F\\n(4)\\nwhere σ denotes the sigmoid function and r denotes the convolutional layer. Ms denotes\\nthe output of the spatial attention module.\\n3\\nCNN Architecture\\nIn this section, we introduce the architecture of the new CNN model and the application of\\nCBAM in the new network. CBAM makes the attention network focus on the informative\\npoints and suppresses irrelative points to improve the performance of CNNs.\\n3.1\\nBasic Architecture of the Enhanced Network\\nWith the development of CNNs, some classical architectures have been proposed such\\nas VGG [SZ14] and Residual Networks (ResNets) [HZRS16].\\nSome researchers have\\ninvestigated the application of VGG architecture in SCA [BPS+18,KPH+19] and indicated\\nthat the architecture performs well. In this work, we investigate another architecture\\n- ResNets in SCA and propose a new CNN model called attention network. The basic\\nstructure of the attention network is shown in Figure 1. It consists of several blocks\\nwith similar architecture. These blocks are called residual blocks. Each residual block is\\ncomposed of several layers and a shortcut connection. In the attention network, these\\nseveral layers are convolutional layers γ, pooling layers δ, and activation functions σ. Each\\nresidual block has two basic blocks. The basic block is composed of convolutional layers\\nand activation functions. The shortcut connection connects the input and output of the\\nbasic block. It can deal with the degradation problem. After stacking several residual\\nblocks, the ﬂatten layer and fully-connected layers λ are adopted. Finally, a softmax layer\\ns is applied to generate the predicted probability of each class.\\nThe structure of attention network can be characterized by the following formula:\\ns ◦[λ]n1 ◦[δ ◦σ ◦[γ(x) ◦σ]n2 ⊕f(x)]n3\\n(5)\\nwhere n1, n2, n3 represent the number of fully-connection layers, basic blocks, and residual\\nblocks. In attention network, n1 and n2 are all set to 2, while n3 is up to the datasets.\\nShortcut connection is denoted as ⊕. Here x is the input of the current basic block and\\nf(x) is the function of x. The funtion f is to make the input and output of basic block\\nhave the same dimensions and is a convolutional operation in attention network. The basic\\nhyperparameters in the attention network are set as follows:\\n6\\nAn Enhanced Convolutional Neural Network in Side-Channel Attacks\\n• For the convolutional layers, the size of the ﬁlter is set to 11 and the stride is set to\\n1. The number of the ﬁlter is various in diﬀerent convolutional layers.\\n• For the pooling layers, we use average-pooling. The pooling stride is set to 2 and the\\npooling window is the same.\\n• The activation functions are all set to ’ReLU’.\\nThe other hyperparameters are up to the experimental datasets and they will be shown\\nlater in the paper. The choices of hyperparameters are motivated by the fact that they\\nprovide good results based on our datasets.\\nInput  trace\\nConv\\nConv\\nPooling\\nFlatten\\nFC\\nOutput\\nRelu\\nRelu\\nConv\\nConv\\nPooling\\nRelu\\nRelu\\nf\\nRelu\\nRelu\\nf\\nConv\\nConv\\nPooling\\nf\\nFigure 1: The basic architecture of the attention network. As the architecture is shown,\\nthe input of the attention network is the whole traces. The output layer has 256 neurons\\nwhich correspond to the 256 classes. We use 3 residual blocks and each residual block\\nconsists of 2 basic blocks. The function f is a convolutional operation applied to the inputs\\nof the basic blocks.\\n3.2\\nEnhanced CNN with Attention Module\\nFor CNNs, it can automatically extract the diﬀerent features of the whole trace by diﬀerent\\nﬁlters of the convolutional operation. But the unnecessary points of the input traces not\\nonly do not provide useful information, but also will introduce extra noises. So some\\nfeatures extracted by ﬁlters are still useless and worsen the performance of CNNs. To\\nmake the attention network focus on the informative points, we introduce an attention\\nmechanism - Convolutional Block Attention Module and apply it to the attention network.\\nCBAM can strengthen the representation of the features in the attention network. First,\\nthe channel attention module is used to assign weights to diﬀerent features. By this\\nmodule, attention network can ﬁnd the important features. Besides, spatial attention is\\nused to point out the locations of the features. So we use the spatial attention module to\\nlocate the important features after the channel attention module. As is shown in Figure 2,\\nthe attention module is inserted in the ﬁrst residual block and sequentially uses channel\\nattention module and spatial attention module. By introducing the CBAM, the attention\\nnetwork can focus on the informative points of the traces and reduce the noises introduced\\nby the unnecessary points.\\n4\\nPerformance of the Enhanced Network\\nIn this section, we apply our attention network on diﬀerent publicly available datasets.\\nThese datasets will be introduced in the Subsection 4.1 and the results will show in the\\nSubsection 4.2. Our experiments are performed by the NVIDIA Graphics Processing Units\\nMinhui Jin et al.\\n7\\nInput\\nConv\\nConv\\nChannel attention Spatial attention\\nNext block\\nFirst residual block + attention mechanism\\nPooling\\nFigure 2: Attention mechanism in the attention network.\\n(GPUs) and we implement the experiment with the open-source deep-learning library\\nKeras and TensorFlow backend.\\n4.1\\nDatasets\\nIn our experiments, we use four public datasets. All of them are the implementations\\nof the Advanced Encryption Standard (AES) algorithm.\\nThey are divided into two\\ntypes: unprotected implementations and protected implementations. The unprotected\\nimplementations have two types, for instance, software implementation and hardware\\nimplementation. The countermeasures of protected implementations also have two types\\nsuch as desynchronization and masking. In the following, we will introduce these four\\ndatasets.\\n4.1.1\\nDPAcontest v4\\nThe cryptographic algorithm of DPAcontest v4 is a masking software implementation of\\nAES-256, which is called AES-256 RSM (Rotating Sbox Masking) on an Atmel ATMega-\\n163 smart-card. DPAcontest v4 is electromagnetic measurements where it uses the same\\nsecret key with diﬀerent plaintexts. [MGH14] has veriﬁed that the masking implementation\\nexists the ﬁrst-order leakage, so we turn the masking protected implementation into the\\nunprotected scenario. The corresponding leakage model is changed to:\\nY (k∗) = Sbox[Pi ⊕k∗] ⊕M\\n(6)\\nwhere M denotes the mask of each cryptographic operation and the value is known. P\\ncorresponds to the plaintexts. We choose the ﬁrst Sbox operation in the ﬁrst round of\\ncryptographic operation that i is set to 1.\\nFor DPAcontest v4, the measurements consist of 10 000 traces, each trace with 1000\\nsamples. We divide the dataset into two subsets such that 5000 traces for the training set\\nand 5000 traces for the attacking set. For the network settings of DPAcontest v4, there are\\n3 residual blocks and the number of the ﬁlters of the convolutional layers in each residual\\nblock is set to {128, 256, 512}. The epoch is set to 60 and the batch-size is set to 200. The\\nnumber of neurons of the hidden fully-connected layers is set to 1024. This public dataset\\nis available at http://www.dpacontest.org/v4/.\\n4.1.2\\nAES_RD\\nThe cryptographic algorithm in AES_RD is a protected software implementation of AES. It\\nis one of the typical classes used in reality. The target device is a smartcard of 8-bit Atmel\\nAVR microcontroller. The countermeasure of the algorithm applies random delay which is\\nproposed in [CK09]. Adding random delay in the algorithm causes the misalignment of\\nthe time samples and make the attack more diﬃcult. For this dataset, the leakage model\\n8\\nAn Enhanced Convolutional Neural Network in Side-Channel Attacks\\nis deﬁned as follows:\\nY (k∗) = Sbox[Pi ⊕k∗]\\n(7)\\nAs same as DPAcontest v4, we choose the ﬁrst Sbox operation in the ﬁrst round of\\ncryptographic operation that is i = 1.\\nFor AES_RD, the measurements consist of 50 000 traces and each trace has 3500\\ntime samples. In the experiment, we divide the dataset into two subsets such that 40 000\\ntraces for training and 10 000 traces for attacking. For the network settings of AES_RD,\\nthere are 5 residual blocks and the number of the ﬁlters of the convolutional layers in\\neach residual block is set to {64, 64, 128, 128, 256}. The number of neurons of hidden\\nfully-connected layers is set to 1024 and we add two dropout layers whose rates are all set\\nto 0.2. In the training process, the epoch is set to 101 and batch-size is set to 256. This\\ndataset is available at https://github.com/ikizhvatov/randomdelays-traces.\\n4.1.3\\nAES_HD\\nThe cryptographic algorithm in AES_HD is a typical class of the hardware implementations\\nof AES-128. The hardware implementation was written in VHDL and applied the parallel\\noperation that it takes 11 clock cycles for each encryption. The hardware design was\\nimplemented on Xilinx Virtex-5 FPGA of a SASEBA GII evaluation board. AES_HD is\\nelectromagnetic measurements that were measured using a high sensitivity near-ﬁeld EM\\nprobe. The suitable and common leakage model of unprotected hardware implementations\\nis to exploit the register writing in the last round. The leakage model is deﬁned as follows:\\nY (Ci1, Ci2, k∗) = Sbox−1[Ci1 ⊕k∗] ⊕Ci2\\n(8)\\nwhere Ci1 and Ci2 denote two ciphertexts, and the relation between i1 and i2 is given\\nthrough the inverse ShiftRows operation of AES. we choose i1 = 12 and i2 = 8.\\nThe measurements of AES_HD is composed of 500 000 traces with 1250 samples of\\neach trace. In our experiment, we only use 75 000 traces that 50 000 traces are used for\\ntraining and 25 000 traces are used for attacking. The network settings of AES_HD are\\nthe same as DPAcontest v4, except that the epoch is set to 75. AES_HD is publicly\\navailable at https://github.com/AESHD/AES_HD_Dataset.\\n4.1.4\\nASCAD\\nThe ﬁnal dataset is ASCAD and the cryptographic algorithm is a masking software\\nprotected implementation of AES-128 [BPS+18]. The implementation is running over an\\n8-bit AVR architecture that is ATMega8515. ASCAD possesses ﬁrst-order security that\\nis robust to the ﬁrst-order SCA. Measurements are measured using an electromagnetic\\nprobe and stored with the current version 5 of the Hierarchical Data Format (HDF5). The\\nleakage model is deﬁned as follows:\\nY (k∗) = Sbox[Pi ⊕k∗]\\n(9)\\nThe ﬁrst and the second Sbox operations in the ﬁrst round of the encryption process are\\nunprotected, so we choose the third Sbox operation that i = 3.\\nThis dataset is composed of 60 000 traces with 700 features of each trace. There are\\n50 000 in the training dataset group and 10 000 in the test dataset group. So we use the\\ntraining dataset group to train the CNN model and use the test dataset group to perform\\nattack. The network settings for ASCAD are the same as DPAcontest v4, except that the\\nepoch is set to 75 and the number of neurons of the hidden fully-connected layers is set to\\n4096. ASCAD dataset is available at https://github.com/ANSSI-FR/ASCAD.\\nMinhui Jin et al.\\n9\\n4.2\\nExperimental Results\\nWe will show the performance of the attention network on four public datasets and compare\\nit with the ASCAD network proposed in [BPS+18] and TA performed in [CRR02]. For our\\nexperiments, we compare these two networks with the Average Rank which repeats the\\nattacking process 300 times. The test traces are randomly selected from the test dataset on\\neach attack. The aim of using Average Rank is to reduce the eﬀect of the sequence of test\\ntraces. We verify that the attention network have the same or even better performance\\ncompared to the ASCAD network and TA on diﬀerent AES implementations.\\n4.2.1\\nDPAcontest V4\\nThe ﬁrst dataset we try to attack is DPAcontest v4. In our experiments, DPAcontest v4 is\\nthe easiest dataset, because we apply the known masks to transform it into the unprotected\\nimplementation. The Figure 3 shows that the attention network has an excellent result\\nwith several traces to attack successfully. The required traces are displayed in Table 1\\nthat attention network demands 3 traces which is the same to the ASCAD network. TA\\nneeds 4 traces to recover the secret key. The result veriﬁed that when the AES is software\\nunprotected, both CNN models perform excellently and have comparable performance to\\nTA.\\n0\\n1\\n2\\n3\\n4\\n5\\nNumber of Traces\\n0\\n2\\n4\\n6\\n8\\n10\\nAverage Rank\\nFigure 3: Average Rank in DPAcontest v4.\\nTable 1: Comparison on DPAcontest v4\\nASCAD network\\n[BPS+18]\\nTemplate attacks\\n[KPH+19]\\nAttention network\\nRequired traces\\n3\\n4\\n3\\n4.2.2\\nAES_RD\\nThe algorithm of AES_RD applies the random delay and makes it hard to align the PoIs.\\nFigure 4 shows that we need more traces to implement a successful attack compared to\\nDPAcontest v4. The attention network needs hundreds of traces to make average rank\\nequal to 0. We also use the ASCAD network to attack the dataset and the result shows in\\nTable 2. ASCAD network needs 247 traces to make the average rank less than 1. But the\\nattention network only needs 171 traces. The result shows both CNN models are robust\\nto the desynchronization and attention network works better than the ASCAD network.\\n10\\nAn Enhanced Convolutional Neural Network in Side-Channel Attacks\\nFor TA, it cannot implement an eﬃcient attack even with 20 000 traces, which reveals\\nthat traditional SCA is hard to break the AES implementation with desynchronization.\\n0\\n200\\n400\\n600\\n800\\nNumber of Traces\\n0\\n20\\n40\\n60\\n80\\n100\\nAverage Rank\\nFigure 4: Average Rank in AES_RD.\\nTable 2: Comparison on AES_RD\\nASCAD network\\n[BPS+18]\\nTemplate attacks\\n[KPH+19]\\nAttention network\\nRequired traces\\n247\\n>20 000\\n171\\n4.2.3\\nAES_HD\\nAES_HD is an unprotected hardware AES implementation. It is diﬃcult to attack because\\nof a much higher level of environmental and algorithmic noise. Figure 5 shows that the\\nattention network has a dramatic performance. Table 3 represents the required traces of\\nthe three models. The attention network only needs around 2100 traces to implement an\\neﬃcient attack, while the ASCAD network and TA cannot recover the secret key even\\nusing the whole traces of the test dataset. It explicates that the attention network is much\\nbetter than the ASCAD network and TA. We think it is because the attention network\\ncan focus on the regions of leakage and ignore the unnecessary points by using CBAM. It\\nreduces the inﬂuence of the noises and improves performance.\\n0\\n2000\\n4000\\n6000\\n8000\\n10000\\nNumber of Traces\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nAverage Rank\\nFigure 5: Average Rank in AES_HD.\\nMinhui Jin et al.\\n11\\nTable 3: Comparison on AES_HD\\nASCAD network\\n[BPS+18]\\nTemplate attacks\\n[KPH+19]\\nAttention network\\nRequired traces\\n>25 000\\n>25 000\\n2100\\n4.2.4\\nASCAD\\nThe cryptographic algorithm of ASCAD applies masks to reduce the relevance between\\nthe measurements and sensitive values. Figure 6 shows that the attention network has\\neﬃcient performance in ASCAD. Table 4 represents the traces that are required to\\nimplement a successful attack. The attention network requires 550 traces, while the\\nASCAD network needs 770 traces. Though the ASCAD network is designed for the\\nmasking AES implementation, the attention network is still better than the ASCAD\\nnetwork. For traditional SCA, [KPH+19] performed the template attack, but it did not\\nretrieve the secret key. They used 500 traces and the Rank is still greater than 50. It\\nshows the attention network is also better than TA.\\n0\\n200\\n400\\n600\\n800\\n1000 1200 1400\\nNumber of Traces\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nAverage Rank\\nFigure 6: Average Rank in ASCAD.\\nTable 4: Comparison on ASCAD\\nASCAD network\\n[BPS+18]\\nTemplate attacks\\n[KPH+19]\\nAttention network\\nRequired traces\\n769\\n>500\\n552\\n5\\nNetwork Visualization\\nWe ﬁrst introduce the principle of a new visualization method — Class Gradient Visual-\\nization. Then we use the visualization method to generate the visualization weight of each\\npoint in the traces and evaluate the eﬀectiveness of the attention network and ASCAD\\nnetwork.\\n5.1\\nClass Gradient Visualization\\nThough CNNs have a signiﬁcant performance in visual tasks, natural language processing,\\nand even side-channel attack, it is still a ’black’ box operation. In the ﬁeld of computer\\n12\\nAn Enhanced Convolutional Neural Network in Side-Channel Attacks\\nvision, in order to understand how CNNs work, researchers have investigated the visual-\\nization technique. (GAP) [LCY13] is directly followed by the softmax layer. [SCD+17]\\nproposed the Gradient-weighted Class Activation Mapping (Grad-CAM). Grad-CAM uses\\ngradients to locate the important regions in the image. It computes the gradients of the\\nfeatures of the last convolutional operations to the predicted class. But Grad-CAM is not\\nsuited for the attention network. Because it is only suited to the network using Global\\nAverage Pooling (GAP) after the last convolutional layer. In our network, we exploit the\\naverage pooling and ﬂatten operation instead of GAP. Besides, Grad-CAM is used to\\nvisualize the regions of images which is 2-dimensions. While the measurements of SCA\\nare always 1-dimension. Therefore based on Grad-CAM, we proposed a new visualization\\nmethod in the side-channel context, called Class Gradient Visualization (CGV).\\nThe principle of the CGV is that the convolutional operation with the average pooling\\noperation of the CNNs is viewed as the process of extracting features. Thus the eﬀect of\\nthe feature maps of the last pooling operation on the predicted result can reﬂect which\\nareas of input traces are important and positive to the ﬁnal predictive class. In the CGV\\nmethod, we compute the gradient of the features after the ﬁnal pooling operation with\\nrespect to the class score.\\nFor the predicted class c, yc denotes the class score that is before the softmax function.\\nA represents the feature matrix after the ﬁnal pooling operation such that A ∈RD×V .\\nHere D denotes the number of samples and V denotes the number of features. xj\\ni is\\nan element of matrix A where i ∈RV and j ∈RD. We ﬁrst compute the gradient of\\nfeature matrix A with respect to the class score yc and obtain the weights matrix W where\\nW ∈RD×V . αj\\ni denotes the weight of the j-th sample of the i-th feature:\\nαj\\ni = ∂yc\\n∂xj\\ni\\n(10)\\nThen we compute a weight map W c\\nCGV for class c and each element of it is computed as\\nfollows:\\nW c\\nCGV = ReLU(\\nV\\nX\\ni\\nxi ⊙αi)\\n(11)\\nwhere xi ⊙αi = (x0\\ni α0\\ni , x1\\ni α1\\ni , . . . , xD\\ni αD\\ni ). The larger value of the weight map shows that\\nthe corresponding time sample has a more important inﬂuence on the predicted class. In\\nour experiment, we only explore the positive inﬂuence of the samples to the predicted class,\\nso we use the ’ReLU’ activation function to keep positive eﬀective samples and discard the\\nnegative eﬀective samples. Using the CGV visualization method, we get a coarse weight\\nvisualization of the same size as the pooling feature maps. Finally, we expand the size\\nof the coarse weight visualization to the original size of the input traces. So we roughly\\nestimate which regions of the input have more inﬂuence on the ﬁnal predicted results.\\n5.2\\nVisualization Results\\nWe visualize which components of the input traces have a positive inﬂuence on the ﬁnal\\nprediction by the CGV method in the trained networks. For the neural network, not all\\nthe points of the input traces are important. Irrelevant points will increase the noises and\\nhave negative eﬀects on the prediction. Thus the CNN models that take more attention to\\nthe signiﬁcant parts of the traces and take less attention to the unimportant parts will\\nhave a better performance. ASCAD network and attention network are the target models.\\nWe utilize plaintexts and the secret key of cryptographic algorithms to exploit CPA\\nanalysis and ﬁnd the leakages of the traces. Figure 7 shows the CPA analysis of DPAcontest\\nv4 and the extended weight visualizations of the ASCAD network and the attention network.\\nFor the attention network, the visualization weight implies that the part of the largest\\nMinhui Jin et al.\\n13\\nweight corresponds to the part of the highest correlation coeﬃcient in the CPA analysis.\\nwhile the ASCAD network not only attends to the informative part but also learns the\\nunimportant part around the informative area. For AES_RD, though it applies random\\n0\\n200\\n400\\n600\\n800\\n1000\\ntime samples\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\ncorrelation\\n(a)\\n0\\n200\\n400\\n600\\n800\\n1000\\ntime samples\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nweight value\\n(b)\\n0\\n200\\n400\\n600\\n800\\n1000\\ntime samples\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nweight value\\n(c)\\nFigure 7: (a) CPA analysis in DPAcontest v4; (b) Weight visualization of the ASCAD\\nnetwork in DPAcontest v4; (c) Weight visualization of the attention network in DPAcontest\\nv4.\\ndelay, CPA analysis still ﬁnds the region of leakages. The weight visualization shows that\\nthe ASCAD network mainly focuses on the region between 2000 and 3500 in the input\\ntraces. But CPA analysis shows the samples between 3000 and 3500 have no apparent\\nleakages. The region that the attention network focuses on is consistent in the region of\\nleakages in CPA analysis (see Figure 8). As for AES_HD, though measurements have\\na lot of noise, we still ﬁnd the main leakages in CPA analysis. The region of leakages\\nis between 980 and 1000. ASCAD network attends to the information of this area but\\nstill learns the other area which is the beginning of the traces. But the attention network\\nmainly focuses on the area of leakages in the CPA analysis(see Figure 9).\\nFor ASCAD, due to the existence of masks, the visualization weight of ASCAD is\\nnot apparent compared to the other three datasets. From the result of visualization (see\\nFigure 10), we ﬁnd that the ASCAD network almost learns the whole trace, while the\\nattention network mainly focuses on three regions. The CPA analysis of masks and outputs\\nof masked Sbox shows that the samples between 50 and 400 leak the information of masks\\ncorresponding to the ﬁrst region between 70 and 200 and the second region between 350\\nand 400. The third region around the 600 samples corresponds to the leakage information\\nof masked Sbox output. By the CGV visualization method, It implies that the attention\\nnetwork takes more attention to the leakages compared to the ASCAD network. Especially\\nfor AES_HD, attending to the leakages instead of other unnecessary points can improve\\nthe attacking performance a lot, because it reduces the eﬀect of the noises.\\n14\\nAn Enhanced Convolutional Neural Network in Side-Channel Attacks\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\n3500\\ntime samples\\n0.015\\n0.010\\n0.005\\n0.000\\n0.005\\n0.010\\n0.015\\n0.020\\n0.025\\ncorrelation\\n(a)\\n0\\n500\\n1000 1500 2000 2500 3000 3500\\ntime samples\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\nweight value\\n(b)\\n0\\n500\\n1000 1500 2000 2500 3000 3500\\ntime samples\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\nweight value\\n(c)\\nFigure 8: (a) CPA analysis in AES_RD; (b) Weight visualization of the ASCAD network\\nin AES_RD; (c) Weight visualization of the attention network in AES_RD.\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\ntime samples\\n0.10\\n0.08\\n0.06\\n0.04\\n0.02\\n0.00\\n0.02\\n0.04\\ncorrelation\\n(a)\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\ntime samples\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nweight value\\n(b)\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\ntime samples\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nweight value\\n(c)\\nFigure 9: (a) CPA analysis in AES_HD; (b) Weight visualization of the ASCAD network\\nin AES_HD; (c) Weight visualization of the attention network in AES_HD.\\nMinhui Jin et al.\\n15\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\ntime samples\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\ncorrelation\\n(a)\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\ntime samples\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\ncorrelation\\n(b)\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\ntime samples\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\nweight value\\n(c)\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\ntime samples\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nweight value\\n(d)\\nFigure 10: (a) CPA analysis of masks in ASCAD; (b) CPA analysis of outputs of masked\\nSbox in ASCAD; (c) Weight visualization of the ASCAD network in ASCAD; (d) Weight\\nvisualization of the attention network in ASCAD.\\n6\\nConclusions\\nIn this paper, we investigate the architecture of ResNets in SCA and build a new CNN\\nmodel. Besides, we introduce an attention mechanism — Convolutional Block Attention\\nModule (CBAM) and insert the module to the new CNN model. CBAM is to make the\\nnetwork attend to the informative part of the input traces and suppress the unnecessary\\npoints. Based on it, the new network can reduce the inﬂuence of the noise introduced by\\nthe irrelevant points and improve the performance. Then we use four publicly available\\ndatasets to analyze the performance of the attention network and compare it to the ASCAD\\nnetwork. The experiments show that the attention network has a better performance than\\nthe ASCAD network in diﬀerent AES implementations. We also compare the attention\\nnetwork to the traditional SCA such as TA, and the results show the performance of the\\nattention network is extremely better than TA.\\nBesides, we propose a new visualization method called Class Gradient Visualization.\\nFor a trained model, applying this method on the training traces can understand what the\\nmodel focuses on and observe which points of the input traces have a positive inﬂuence\\non the ﬁnal prediction.\\nWe use this visualization method to attention network and\\nASCAD network. The results show that the attention network takes more attention to the\\nimportant leakage compared to the ASCAD network. In other respects, it can explain\\nwhy the attention network works better than the ASCAD network. In this paper, we only\\nexploit the CBAM and in future work, we will explore other attention mechanisms and\\ncompare the eﬀectiveness of diﬀerent attention mechanisms.\\n16\\nAn Enhanced Convolutional Neural Network in Side-Channel Attacks\\nReferences\\n[AARR02]\\nDakshi Agrawal, Bruce Archambeault, Josyula R Rao, and Pankaj Rohatgi.\\nThe em side—channel (s).\\nIn International Workshop on Cryptographic\\nHardware and Embedded Systems, pages 29–45. Springer, 2002.\\n[BCO04]\\nEric Brier, Christophe Clavier, and Francis Olivier. Correlation power analysis\\nwith a leakage model. In International Workshop on Cryptographic Hardware\\nand Embedded Systems, pages 16–29. Springer, 2004.\\n[BPS+18]\\nRyad Benadjila, Emmanuel Prouﬀ, Rémi Strullu, Eleonora Cagli, and Cécile\\nDumas. Study of deep learning techniques for side-channel analysis and\\nintroduction to ascad database. ANSSI, France & CEA, LETI, MINATEC\\nCampus, France. Online verfügbar unter https://eprint. iacr. org/2018/053.\\npdf, zuletzt geprüft am, 22:2018, 2018.\\n[CDP17]\\nEleonora Cagli, Cécile Dumas, and Emmanuel Prouﬀ. Convolutional neural\\nnetworks with data augmentation against jitter-based countermeasures. In\\nInternational Conference on Cryptographic Hardware and Embedded Systems,\\npages 45–68. Springer, 2017.\\n[CK09]\\nJean-Sébastien Coron and Ilya Kizhvatov. An eﬃcient method for random\\ndelay generation in embedded software. In International Workshop on Cryp-\\ntographic Hardware and Embedded Systems, pages 156–170. Springer, 2009.\\n[CRR02]\\nSuresh Chari, Josyula R Rao, and Pankaj Rohatgi. Template attacks. In\\nInternational Workshop on Cryptographic Hardware and Embedded Systems,\\npages 13–28. Springer, 2002.\\n[GBTP08]\\nBenedikt Gierlichs, Lejla Batina, Pim Tuyls, and Bart Preneel. Mutual\\ninformation analysis. In International Workshop on Cryptographic Hardware\\nand Embedded Systems, pages 426–442. Springer, 2008.\\n[GST17]\\nDaniel Genkin, Adi Shamir, and Eran Tromer. Acoustic cryptanalysis. Journal\\nof Cryptology, 30(2):392–443, 2017.\\n[HGG18]\\nBenjamin Hettwer, Stefan Gehrer, and Tim Güneysu. Proﬁled power analysis\\nattacks using convolutional neural networks with domain knowledge. In\\nInternational Conference on Selected Areas in Cryptography, pages 479–498.\\nSpringer, 2018.\\n[HGG19]\\nBenjamin Hettwer, Stefan Gehrer, and Tim Güneysu. Deep neural network\\nattribution methods for leakage analysis and symmetric key recovery. IACR\\nCryptology ePrint Archive, 2019:143, 2019.\\n[HZRS16]\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual\\nlearning for image recognition. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 770–778, 2016.\\n[KJJ99]\\nPaul Kocher, Joshua Jaﬀe, and Benjamin Jun. Diﬀerential power analysis. In\\nAnnual International Cryptology Conference, pages 388–397. Springer, 1999.\\n[Koc96]\\nPaul C Kocher. Timing attacks on implementations of diﬃe-hellman, rsa, dss,\\nand other systems. In Annual International Cryptology Conference, pages\\n104–113. Springer, 1996.\\nMinhui Jin et al.\\n17\\n[KPH+19]\\nJaehun Kim, Stjepan Picek, Annelie Heuser, Shivam Bhasin, and Alan Han-\\njalic. Make some noise. unleashing the power of convolutional neural networks\\nfor proﬁled side-channel analysis.\\nIACR Transactions on Cryptographic\\nHardware and Embedded Systems, pages 148–179, 2019.\\n[LCY13]\\nMin Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv\\npreprint arXiv:1312.4400, 2013.\\n[MDP19]\\nLoïc Masure, Cécile Dumas, and Emmanuel Prouﬀ. Gradient visualization\\nfor general characterization in proﬁling attacks. In International Workshop\\non Constructive Side-Channel Analysis and Secure Design, pages 145–167.\\nSpringer, 2019.\\n[MGH14]\\nAmir Moradi, Sylvain Guilley, and Annelie Heuser. Detecting hidden leakages.\\nIn International Conference on Applied Cryptography and Network Security,\\npages 324–342. Springer, 2014.\\n[MHM13]\\nZdenek Martinasek, Jan Hajny, and Lukas Malina. Optimization of power\\nanalysis using neural network. In International Conference on Smart Card\\nResearch and Advanced Applications, pages 94–107. Springer, 2013.\\n[MPP16]\\nHoussem Maghrebi, Thibault Portigliatti, and Emmanuel Prouﬀ. Breaking\\ncryptographic implementations using deep learning techniques. In Interna-\\ntional Conference on Security, Privacy, and Applied Cryptography Engineering,\\npages 3–26. Springer, 2016.\\n[MZVT15]\\nZdenek Martinasek, Ondrej Zapletal, Kamil Vrba, and Krisztina Trasy. Power\\nanalysis attack based on the mlp in dpa contest v4. In 2015 38th International\\nConference on Telecommunications and Signal Processing (TSP), pages 154–\\n158. IEEE, 2015.\\n[PEC19]\\nGuilherme Perin, Baris Ege, and Lukasz Chmielewski. Neural network model\\nassessment for side-channel analysis. Technical report, Cryptology ePrint\\nArchive, Report 2019/722, 2019. https://eprint. iacr. org . . . , 2019.\\n[SCD+17]\\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna\\nVedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations\\nfrom deep networks via gradient-based localization. In Proceedings of the\\nIEEE International Conference on Computer Vision, pages 618–626, 2017.\\n[SLP05]\\nWerner Schindler, Kerstin Lemke, and Christof Paar. A stochastic model\\nfor diﬀerential side channel cryptanalysis. In International Workshop on\\nCryptographic Hardware and Embedded Systems, pages 30–46. Springer, 2005.\\n[SMY09]\\nFrançois-Xavier Standaert, Tal G Malkin, and Moti Yung. A uniﬁed framework\\nfor the analysis of side-channel key recovery attacks. In Annual international\\nconference on the theory and applications of cryptographic techniques, pages\\n443–461. Springer, 2009.\\n[SZ14]\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks\\nfor large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\\n[Tim19]\\nBenjamin Timon. Non-proﬁled deep learning-based side-channel attacks with\\nsensitivity analysis. IACR Transactions on Cryptographic Hardware and\\nEmbedded Systems, pages 107–131, 2019.\\n18\\nAn Enhanced Convolutional Neural Network in Side-Channel Attacks\\n[WPLSK18] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam:\\nConvolutional block attention module. In Proceedings of the European Con-\\nference on Computer Vision (ECCV), pages 3–19, 2018.\\n[YLMZ18]\\nGuang Yang, Huizhong Li, Jingdian Ming, and Yongbin Zhou. Convolutional\\nneural network based side-channel attacks in time-frequency representations.\\nIn International Conference on Smart Card Research and Advanced Applica-\\ntions, pages 1–17. Springer, 2018.\\n[YYD+16]\\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard\\nHovy. Hierarchical attention networks for document classiﬁcation. In Proceed-\\nings of the 2016 conference of the North American chapter of the association\\nfor computational linguistics: human language technologies, pages 1480–1489,\\n2016.\\n[YZLC11]\\nShuguo Yang, Yongbin Zhou, Jiye Liu, and Danyang Chen. Back propagation\\nneural network based leakage characterization for practical security analysis of\\ncryptographic implementations. In International Conference on Information\\nSecurity and Cryptology, pages 169–185. Springer, 2011.\\n[ZBHV20]\\nGabriel Zaid, Lilian Bossuet, Amaury Habrard, and Alexandre Venelli.\\nMethodology for eﬃcient cnn architectures in proﬁling attacks. IACR Trans-\\nactions on Cryptographic Hardware and Embedded Systems, pages 1–36, 2020.\\n[ZF14]\\nMatthew D Zeiler and Rob Fergus. Visualizing and understanding convolu-\\ntional networks. In European conference on computer vision, pages 818–833.\\nSpringer, 2014.\\n[ZK16]\\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv\\npreprint arXiv:1605.07146, 2016.\\n[ZKL+16]\\nBolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio\\nTorralba. Learning deep features for discriminative localization. In Proceedings\\nof the IEEE conference on computer vision and pattern recognition, pages\\n2921–2929, 2016.\\n[ZS19]\\nYuanyuan Zhou and François-Xavier Standaert. Deep learning mitigates but\\ndoes not annihilate the need of aligned traces and a generalized resnet model\\nfor side-channel attacks. Journal of Cryptographic Engineering, pages 1–11,\\n2019.\\n', metadata={'Published': '2020-09-18', 'Title': 'An Enhanced Convolutional Neural Network in Side-Channel Attacks and Its Visualization', 'Authors': 'Minhui Jin, Mengce Zheng, Honggang Hu, Nenghai Yu', 'Summary': 'In recent years, the convolutional neural networks (CNNs) have received a lot\\nof interest in the side-channel community. The previous work has shown that\\nCNNs have the potential of breaking the cryptographic algorithm protected with\\nmasking or desynchronization. Before, several CNN models have been exploited,\\nreaching the same or even better level of performance compared to the\\ntraditional side-channel attack (SCA). In this paper, we investigate the\\narchitecture of Residual Network and build a new CNN model called attention\\nnetwork. To enhance the power of the attention network, we introduce an\\nattention mechanism - Convolutional Block Attention Module (CBAM) and\\nincorporate CBAM into the CNN architecture. CBAM points out the informative\\npoints of the input traces and makes the attention network focus on the\\nrelevant leakages of the measurements. It is able to improve the performance of\\nthe CNNs. Because the irrelevant points will introduce the extra noises and\\ncause a worse performance of attacks. We compare our attention network with the\\none designed for the masking AES implementation called ASCAD network in this\\npaper. We show that the attention network has a better performance than the\\nASCAD network. Finally, a new visualization method, named Class Gradient\\nVisualization (CGV) is proposed to recognize which points of the input traces\\nhave a positive influence on the predicted result of the neural networks. In\\nanother aspect, it can explain why the attention network is superior to the\\nASCAD network. We validate the attention network through extensive experiments\\non four public datasets and demonstrate that the attention network is efficient\\nin different AES implementations.'}), Document(page_content=\" Fault diagnosis for PV arrays considering dust impact based on \\ntransformed graphical feature of characteristic curves and \\nconvolutional neural network with CBAM modules \\nJiaqi Qu, Lu Wei*, Qiang Sun, Hamidreza Zareipour, Zheng Qian* \\n \\nAuthor Names and Affiliations: \\nJiaqi Qu, Zheng Qian (Corresponding author): School of Instrumentation and Optoelectronic Engineering, \\nBeihang University, Beijing, China. \\nLu Wei (Corresponding author): School of Electronics and Information Engineering, Beihang University, \\nBeijing, China. \\nQiang Sun: State Key Laboratory of Control and Simulation of Power System and Generation Equipment, \\nDepartment of Electrical Engineering, Tsinghua University, Beijing, China. \\nHamidreza Zareipour: The Department of Electrical and Computer Engineering, University of Calgary, \\nCalgary, Canada. \\n \\n \\n*Corresponding Author: \\nLu Wei: School of Electronics and Information Engineering, Beihang University, Beijing, China. \\nE-mail: weilu@buaa.edu.cn \\nZheng Qian: School of Instrumentation and Optoelectronic Engineering, Beihang University, Beijing, China. \\nE-mail: qianzheng@buaa.edu.cn \\nTel: +86-010-82339267 \\nFax: +86-010-82339267 \\n \\nAddress:  \\nXueyuan Road No.37, Haidian District, Beijing, 100191, China. \\n \\n \\nFault diagnosis for PV arrays considering dust impact based on \\ntransformed graphical feature of characteristic curves and \\nconvolutional neural network with CBAM modules \\nAbstract: Various faults can occur during the operation of PV arrays, and both the dust-affected operating \\nconditions and various diode configurations make the faults more complicated. However, current methods \\nfor fault diagnosis based on I-V characteristic curves only utilize partial feature information and often rely \\non calibrating the field characteristic curves to standard test conditions (STC). It is difficult to apply it in \\npractice and to accurately identify multiple complex faults with similarities in different blocking diodes \\nconfigurations of PV arrays under the influence of dust. Therefore, a novel fault diagnosis method for PV \\narrays considering dust impact is proposed. In the preprocessing stage, the Isc-Voc normalized Gramian \\nangular difference field (GADF) method is presented, which normalizes and transforms the resampled PV \\narray characteristic curves from the field including I-V and P-V to obtain the transformed graphical feature \\nmatrices. Then, in the fault diagnosis stage, the model of convolutional neural network (CNN) with \\nconvolutional block attention modules (CBAM) is designed to extract fault differentiation information from \\nthe transformed graphical matrices containing full feature information and to classify faults. And different \\ngraphical feature transformation methods are compared through simulation cases, and different CNN-based \\nclassification methods are also analyzed. The results indicate that the developed method for PV arrays with \\ndifferent blocking diodes configurations under various operating conditions has high fault diagnosis accuracy \\nand reliability. \\nKeywords: Photovoltaic; Fault diagnosis; Dust impact; Graphical feature transformation; Characteristic \\ncurves \\n1. Introduction \\nSolar energy, with its advantages of cleanness, accessibility, and utilization, has been paid increasing \\nattention in the fight against global warming and fossil energy shortages. The International Energy Agency \\n(IEA) predicts that the global PV market will grow by 25% year-on-year to 197 GW of new installed capacity \\nin 2022, with cumulative installed capacity exceeding 1,000 GW. From 2022 to 2031, the global PV grid-\\nconnected installed capacity will grow at an average annual rate of 8% [1]. However, PV systems are often \\nsubject to abnormal failures due to disturbances in the operating environment, which can result in an \\nestimated annual energy loss of up to 18.9% [2]. \\nPV arrays, as the core component of PV systems, are prone to multiple faults such as short-circuit, open-\\ncircuit, abnormal degradation, partial shading, etc. in complex outdoor environments [3]. Currently, the \\ntraditional DC-side protection methods, such as over-current protection (OCPDs) and ground fault protection \\n(GFPDs), have certain difficulty in determining the type of faults [4, 5]. Furthermore, due to the non-linear \\noutput characteristics of the PV array, both low mismatch faults and faults at low irradiance may cause the \\nfailures of the protection [6]. These failures can not only affect power generation but also lead to serious \\nsafety issues. In some cases, faults may even increase the risk of fire and personal danger, if not detected and \\ncorrected in time [7]. In addition, the dusty environment has a significant impact on the operation \\nperformance of PV systems [8-10]. Dust from PV panels can reduce the power of PV systems [11], and more \\nimportantly, the long-term dust deposition operating conditions also complicate faults, forming compound \\nfaults that are more difficult to classify [12]. Therefore, effective fault detection and diagnosis of PV arrays \\nunder the influence of dust is essential for the safety and reliability of PV systems [13]. \\nAt present, there are two main categories of techniques for detecting and diagnosing faults in PV arrays. \\nOne is the offline diagnosis methods, which include earth capacitance measurement (ECM) [14] and time \\ndomain reflection (TDR) [15, 16], etc. These methods rely on specific external signal generators for testing \\nand require the offline operation of the PV array, which interferes with normal operation and makes it hard \\nto diagnose faults online in real-time. The other category is the online diagnostic methods that do not affect \\nthe normal operation of the PV system. According to the diagnostic indicators, these online methods are \\nfurther divided into those based on the measurement of current and voltage output on the DC side of each \\nsubstring of the array [4, 17, 18], those based on the measurement of current and voltage output on the AC \\nside of the array [6, 19, 20], and those based on the I-V characteristic curves [21-23]. Specifically, the DC \\nindicators-based method analyses faults through the output residuals of the current or voltage in each \\nsubstring relying on a large number of sensors, which is costly and has limited universality. The AC \\nindicators-based method does not need to install a large number of sensors and uses the residual thresholds \\nof current and voltage output on the AC side of the array. However, the types of faults that can be \\ndistinguished between different threshold intervals are inadequate, and it is hard to diagnose multiple \\ncomplex faults with complete accuracy. Since I-V characteristic curves usually contain rich information on \\nthe status of PV modules, diagnosis based on I-V curves is a hot topic [24]. Moreover, the I-V tracer currently \\nalready supports measurements for a single module or small-scale strings or arrays and has realized online \\nmeasurement without changing the operational state [25, 26]. In this sense, the diagnosis method based on \\nI–V curves can be applied to all common PV installations and is easier to implement in the field. \\nIn existing studies, according to forms of curve features applied, fault diagnosis methods using I-V \\ncharacteristic curves can generally be divided into: i) raw I-V curves relying on deep learning models for \\ndiagnosis and ii) extracted key features of I-V curves for diagnosis. Further, the former includes: (1) taking \\nI-V curves data as input directly. For example, Chen et al. [27] assembled I-V curves with irradiance (G) and \\nmodule temperature (T) into a 4-column matrix to classify 8 classes of PV array faults by an improved ResNet \\nmodel. Gao et al. [28] designed a fusion model of convolutional neural network (CNN) and residual-gated \\nrecurrent unit (Res-GRU) to diagnose hybrid faults using a combination matrix of I-V curves, irradiance and \\ntemperature as inputs. (2) taking the residual between the measured I-V curve and the theoretical curve as \\ninput. For example, Chine et al. [29] compared the difference between the measured and the simulated PV \\narray output power and identified faults by the attributes of differences in I-V curves. Liu et al. [21] proposed \\na fault diagnosis method based on stacked auto encoder (SAE) and clustering, which extracts features from \\nthe difference between the simulated and measured I-V curves to achieve classification. These methods have \\ninsufficient ability to process the full feature information contained in the original I-V curves completely, and \\nthe extraction of features mostly depends on classifiers with complex structures. In addition, they only enable \\nthe classification of a fixed degree of faults and have tiny diagnostic ability for the full fault levels of defects. \\nThe latter includes (3) identifying key features of array characteristics (e.g. VOC, ISC, VMPP, IMPP, FF, RS, and \\nRP) in the curves as input. For example, Fadhel et al. [30] adopt VMPP, IMPP, and PMPP as features to classify \\nfour different shading configurations. Liu et al. [31] extracted five key points from I-V curves as valid \\nfeatures as input to a fault diagnosis method based on variable prediction model. Besides, some similar \\napproaches are presented in [32-34]. (4) calculating shape features (e.g. derivative and curvature) of the \\ncurves as input. For example, Bressan et al. [35] proposed method based on the analysis of the first and \\nsecond derivative of the I-V curves, for detecting faults on series resistors and activation of bypass diodes. \\nMa et al. [36] analyzed the extraction of negative peaks on the derivative of the I-V curves, whereby single \\nfaults and compound faults at different levels of shading were diagnosed. However, it should be pointed out \\nthat these studies only used part information of characteristic curves. The diagnosis was completed by \\nanalyzing the current (IMPP), voltage (VMPP), power (PMPP) or curve shape features at the maximum power \\npoint (MPP). In some complex scenarios such as faults considering soiling, the feature extraction process is \\ncomplicated, and they may appear to have the same MPPs leading to wrong classifications. \\nRecently, several studies have proposed additional contributions to this field. Lin et al. [37] extracted \\nmulti-scale fault features using different scales of convolution horizons and identified fixed fault parameters \\nof faults effected by soiling, Huang et al. [38] investigated full-scale faults under the operating condition with \\nsoiling impact in PV arrays without blocking diodes, and Li et al. [39] used full characteristic information of \\nthe graphical features of I-V curves and machine learning techniques for PV arrays fault diagnosis. However, \\nseveral gaps still remain. First, to the best of the authors’ knowledge, there are no studies that provide a \\ncomprehensive analysis of full-scale faults with dynamic fault parameters in PV arrays with various blocking \\ndiode configurations considering the impact of soiling. Second, extracting features of array characteristics or \\ncurve shapes does not make effective use of full information contained in the I-V curves, leaving incomplete \\ntypes of PV array faults to be identified. Furthermore, most current diagnostic methods using characteristic \\ncurves rely on additional measured curves of fault samples to calibrate the I-V curves to the standard test \\nconditions, severely restricting their practical applicability. Therefore, this paper aims to develop a feature \\nprocessing method using full information of characteristic curves, which does not require additional \\nexperiments to calibrate curves under different environments. Furthermore, a full-range multi-faults \\nclassification method adapted to different PV array configurations considering soiling impact is designed. It \\nenables the complete diagnosis of complex faults for various operating conditions and configurations of PV \\narrays. The main contributions of the method proposed in this paper are: \\n(1) Various fault types of PV arrays in multiple scenarios are compared and analyzed, including PV \\narrays with and without blocking diode configurations and compound fault types with and without the \\ninfluence of soiling. It overcomes the challenge of uniqueness of array fault diagnosis methods for different \\nobjects and provides universal ability to diagnose different PV configurations with dynamic array faults. \\n(2) The graphical feature transformation method based on Isc-Voc normalized GADF is proposed to \\nextract common features of the same fault in different environments, and the transformed graphical features \\nof the characteristic curves are stacked into 2-channel 2D matrices as input features for the classification \\nmodel. It fills the gap of diagnosing complex faults affected by soiling using the complete characteristic \\ncurves information without calibration experiments, which greatly improves the practical application value. \\n(3) The classification model of convolutional neural network with CBAM module is designed for fault \\ndiagnosis in multiple scenarios, which can accurately classify and identify the full range of complicated faults. \\nIt extends the performance of diagnosis methods under the influence of dust, and improves the diagnosis \\naccuracy and robustness under multiple scenes. \\nThe rest of this paper is organized as follows: Section 2 formulates the preliminary analysis of the \\nproblem to be studied. Section 3 details the proposed fault diagnosis methodology. Section 4 presents \\nexperimental results and discussion. Finally, Section 5 concludes this work. \\n2. Preliminary analysis \\nIn this section, first, the fault behaviors of PV arrays with different blocking diode configurations are \\nanalyzed. Second, the characteristic curves under normal operating condition and condition considering the \\ninfluence of soiling are compared. Furthermore, the preprocessing techniques used for the current fault \\ndiagnosis are investigated, and the limitations of the existing preprocessing methods are identified. \\n \\nFig. 1. Faults of PV arrays under different operating conditions. \\n2.1 Faults analysis of PV arrays  \\nAs shown in Fig. 1a, two configurations (with and without blocking diodes, denoted as Configuration 1 \\nand Configuration 2, respectively) are designed to analyze each type of fault. In fact, during the operation of \\na PV array, various faults may occur, including short-circuits in substrings such as short-circuit in one or two \\nmodules, open-circuit of substrings, various degrees of shading such as shading of a single module or multiple \\nmodules, as well as series resistance degradation of array and parallel resistance degradation of array. \\nPV arrays with different blocking diode configurations differ in the manifestation of short-circuits [6], \\nas exemplified by the I-V characteristic curves shown in Fig. 2. Specifically, the open-circuit voltage of a \\nnormal substring (denoted VOCS) of a PV array is proportional to the number of serial modules (denoted NSM), \\nand when different numbers of PV modules in a particular substring are short-circuited (denoted NLL), then \\nthe OC voltage of the fault string is (NSM-NLL)/NSM⋅VOCS (denoted as VOCF). When substrings are equipped \\nwith blocking diodes, i.e. Configuration 1, the current of each string is only allowed to flow in one direction. \\nVOCS and VMPP of the faulty string are reduced, while IMPP remains unchanged. When VOCS > VOCF, the faulty \\nstring will be disconnected from array, which is expressed in the IV/PV characteristic curve as the presence \\nof a local minimum inflection point and the overall VOC of the array remaining stable, as shown in Fig. 2b. \\nOn the contrary, when the substrings are not equipped with blocking diodes, i.e. Configuration 2, the current \\nfrom the normal string is reversed into the faulty string when VOCS > VOCF. As a result, VOC of the faulty array \\nwith short-circuits is lower than that of the normal array. It is worth noting that when VOCS < VOCF, the \\ncharacteristic curves of the PV array are the same for both configurations in this interval. \\n \\nFig. 2. I-V curve of PV arrays with different blocking diode configurations. \\nIn addition, during outdoor operation, all modules in the PV array can be affected by various types of \\nshading, including shading from buildings, tree shade, etc., as well as shielding from dust adhering to the \\nsurface. The essential effect of both is manifested in the reduction of irradiance intensity of the affected \\nmodules. Specifically, the former generally results in a larger reduction in irradiance typically of 20% or \\nmore, whereas dust accumulation is considered a specific form of shading that occurs across all PV modules \\nof an array and produces a relatively small reduction in irradiance usually below 20% [40, 41]. \\nThe enlarged structure of a PV module in Fig. 1a shows that every N cells are inversely connected in \\nparallel with one bypass diode. For a single module, when the shading intensity and area are fixed, the output \\nI-V characteristic curve of the module shows a single peak, as shown in Fig. 3a. Further, when a module in a \\nsubstring containing multiple modules is shaded, the I-V curve splits into two parts [30]. When the voltage \\nis lower than the substring voltage (denoted VS), the shaded cells of the shaded module are bypassed by the \\nbypass diode, and the I-V characteristic in this interval is approximately equivalent to the I-V characteristic \\nof the remaining unshaded modules in series. When the voltage is larger than VS, the bypass diode turns off \\nresulting in the decrease of the total output current of the string [42]. The I-V curve in this interval is mainly \\ndetermined by the shaded module. Therefore, no matter what degree of shading, there is a turning peak on \\nthe I-V/P-V characteristic curve. The shadow degree can influence the inflection point of current, as shown \\nin Fig. 3a. The current at inflection point decreases with the increase of the shading degree and the number \\nof shading modules. As the degree of shading increases, the voltage at inflection point decreases \\nproportionally, as shown in Fig. 3b. It also can be seen that in Fig. 2 and Fig. 3b, the characteristic curves of \\narrays under certain shading conditions are very similar to the short-circuited arrays configured with blocking \\ndiodes. \\n \\nFig. 3. I-V curve of PV modules with different structures under the influence of different shading levels. \\nIn fact, the intensity of dust accumulation is uneven across all modules, representing a non-uniform \\nsoiling effect for the array [38, 40] as shown in Fig. 1b. Therefore, under the influence of non-uniform soiling, \\neach module of the array is equivalent to being affected by different degrees of shading, the characteristic \\ncurves of the array will also show a different number of peaks. Importantly, when other faults occur under \\nthe impact of dust, such as short-circuit and open-circuit, the characteristic curves become more complex, \\nshowing a superposition of multiple peaks and corresponding fault characteristics at the same time, as in Fig. \\n4b. However, most cleaning operations for dust or soiling are usually carried out on an annual basis or more \\nfrequently in areas heavily affected by dust, but other faults under the influence of non-uniform soiling can \\nstill occur during the cleaning interval [43, 44]. This makes the diagnosis of complex fault types more \\nchallenging and difficult to identify accurately in time.  \\n \\nFig. 4. I-V curves of faults under different operating conditions. \\nIt can be seen that various complex fault types of different array structures are rich and diverse, and \\nhave high similarities. Therefore, it is of universal significance to propose an effective method applicable to \\nfault diagnosis in multiple scenarios. \\n2.2 Preprocessing methods for fault diagnosis  \\nThe shape of PV characteristic curves is dependent on environmental conditions such as irradiance and \\ntemperature [45]. Therefore, when using characteristic curves for PV array fault diagnosis, the influence of \\ndifferent environmental conditions causing different characteristics manifestations of the same fault type \\nshould be excluded to reduce confusion of features caused by factors other than the fault types [39]. At present, \\nthere are three data preprocessing methods to minimize the impact of environmental factors on the \\ncharacteristic curves: \\n(1) The input feature is a two-dimensional matrix recombined by I-V curve and ambient variables, which \\nstems from the ability of the convolutional neural network to automatically extract features of two-\\ndimensional data. Specifically, irradiance and temperature are repeatedly supplemented into column vectors \\nwith the same points of I/V vector, and then stitched together with the I-V curves to form a final two-\\ndimensional matrix in Fig. 5, which is used as input for the deep learning network classification method. \\n \\nFig. 5. The recombined GTIV matrix. \\nThis method does not directly eliminate the influence of environmental variables on the characteristic \\ncurves, but only relies on the convolutional network to extract common feature information of the same fault \\ntype under different environmental conditions. It can be inferred that its ability to deal with complicated \\nfaults diagnosis is shallow. \\n(2) Converting key features of array characteristics in different environments to features under STC is \\noften applied to the diagnosis method using key features identified in characteristic curves. That is, the open-\\ncircuit voltage VOC, the short-circuit current ISC, the maximum power point voltage VMPP, the maximum power \\npoint current IMPP, and the equivalent series resistance RS are converted to corresponding values under STC. \\nThe approach to obtain the feature functions is based on the traditional approximation equations [46, 47], \\nwhere the unknown parameters are denoted by a, b, c and d. The functions of key features can be rewritten \\nas: \\n \\nOC\\nOC,STC\\nSTC\\nST\\n1\\n2\\n3\\nC\\nln G\\nG\\nV\\nV\\na\\na\\ndT\\na\\ndT\\nG\\nG\\n \\n(1) \\n \\nm\\nm,STC\\nSTC\\nST\\n1\\n3\\nC\\n2\\nln G\\nG\\nV\\nV\\nb\\nb\\ndT\\nb\\ndT\\nG\\nG\\n \\n(2) \\n \\n3\\nSTC\\nST\\n2\\nC\\nm\\n1\\nm,STC\\nG\\nG\\nI\\nc\\nI\\nc\\ndT\\nc\\ndT\\nG\\nG\\n \\n(3) \\n \\n1\\nS\\nS,STC\\nSTC\\nC\\n2\\n3\\nST\\nd\\nG\\nG\\nR\\nR\\nd\\ndT\\nd\\ndT\\nG\\nG\\n \\n(4) \\nThe parameters of this non-linear static model need to be identified by searching through multiple \\ncharacteristic curves. In addition, as the curves under STC still behave differently for various fault types, \\nseparate parameter identifications are required for various faults with different STC conditions, which \\ninvolves additional test experiments with fault samples. Moreover, the feature-dependent diagnosis method \\nis not effective in classifying complex fault types. \\n (3) Correcting all points of the entire I-V characteristic curve is a preprocessing method that relies on \\nthe use of full I-V information for diagnosis. The IEC 60891 [48] defines three standard procedures for the \\ncorrection of I-V curves, which are used to allow comparison of curves measured under different conditions, \\nthus enables health monitoring of PV panels. The following are named M1, M2 and M3 respectively, as well \\nas the correction method of M2new proposed in [49]: \\nM1： \\n \\n2\\n2\\n1\\nSC\\n2\\n1\\n1\\n1\\nG\\nI\\nI\\nI\\nT\\nT\\nG\\n\\uf061\\n \\n(5) \\n \\n2\\n1\\nS\\n2\\n1\\n2\\n2\\n1\\n2\\n1\\nV\\nV\\nR\\nI\\nI\\nI\\nT\\nT\\nT\\nT\\n\\uf06b\\n\\uf062\\n \\n(6) \\nM2： \\n \\n2\\n2\\n1\\nrel \\n2\\n1\\n1\\n1\\nG\\nI\\nI\\nT\\nT\\nG\\n\\uf061\\n \\n(7) \\n \\n2\\n2\\n1\\nOC1\\nrel \\n2\\n1\\n2\\n1\\n2\\n2\\n1\\n1\\nln\\nS\\nG\\nV\\nV\\nV\\nT\\nT\\na\\nR\\nI\\nI\\nI\\nT\\nT\\nG\\n\\uf062\\n\\uf06b\\n \\n(8) \\nM2new: \\nIt uses the same equation as M2 for current correction, but corrects for voltage by replacing the term \\n“\\nOC1\\nV\\n” in (8) with “\\nre\\nOC1\\nl\\n1\\n1\\n(25\\n)\\nV\\nT\\n\\uf062\\n”. \\nM3： \\n \\n3\\n1\\n2\\n1\\nI\\nI\\nI\\nI\\n\\uf067\\n \\n(9) \\n \\n3\\n1\\n2\\n1\\nV\\nV\\nV\\nV\\n\\uf067\\n \\n(10) \\n \\n3\\n1\\n2\\n1\\nG\\nG\\nG\\nG\\n\\uf067\\n \\n(11) \\n \\n3\\n1\\n2\\n1\\nT\\nT\\nT\\nT\\n\\uf067\\n \\n(12) \\nIn fact, M1, M2, and NewM2 are correction methods based on one single curve, that requires the setting \\nof corresponding correction factors such as \\uf06b  and RS. That is, after the correction parameters are \\ndetermined, they can be directly corrected from any test conditions to the STC. However, it is still difficult \\nto determine the correction factors of PV panels on site due to the rigorous experimental conditions required \\nin the IEC 60891 procedure. In addition, research shows that due to differences in irradiance, module \\ntemperature, and severity of faults, all these methods introduce significant errors, making it difficult to \\nperform well under all fault conditions [49]. Moreover, the distortion of curve shape in the IEC method \\nusually leads to a relative error of 13.8% and the estimation error of fault features extracted from the \\ncorrection curve also occurs frequently. It can be seen that if these features are used as defect features, it may \\naffect the diagnosis of faults. \\nAlternatively, M3 does not contain correction factors, but the interpolation constant \\uf067 needs to be set. \\nThis method is to apply linear interpolation method on multiple I-V characteristic testing curves to obtain the \\nI-V characteristic under the STC. Although the multiple curves-based method (M3) generally offers higher \\nperformance than the above methods based on one single curve, the conversion of a certain curve with both \\nG and T requirements relies on multiple testing curves, and the measurement and calculation are relatively \\ncomplex and inefficient, so it is not suitable for rapid field diagnosis. \\nIt is undeniable that the use of characteristic curves for PV health monitoring and fault diagnosis is a \\npromising method. Considering the limitations of the practical application of the preprocessing methods \\ndiscussed above, it will be beneficial to explore solutions based on field measurement data while reducing \\nthe dependence on the calibration process. \\n3. The proposed fault diagnosis methodology \\nIn this section, firstly, the accurate modeling approach of the real PV array is presented, and the \\nsimulation method of faults in different configurations of PV arrays under different operating conditions is \\ndescribed. Then, a data processing method that does not depend on the calibration of the characteristic curves \\nto STC is proposed to obtain transformed graphical feature matrices with full information of characteristic \\ncurves. Finally, the fault diagnosis model based on the convolutional neural network with CBAM module is \\nintroduced. \\n3.1 Configuration of the simulated PV modeling \\nTo explore the complex faults of PV arrays under different operating conditions, the single-diode PV \\ncell model proposed in [50] is used in this paper, as shown in Fig. 6. And based on this model a PV module \\nmodel is built on the PSCAD/EMTDC platform with the configuration parameters from the Shell Solar SP-\\n70 datasheet in Table 1. We compare the simulated I-V curves of this PV module model under different \\nradiation intensities at 25°C with the manual curves supplied by manufacturer. There is a high correlation \\nbetween the two, which verifies the consistency of the equivalence between the model and the actual PV \\nmodule. Based on this PV module model, the PV array model in this study is further designed for a structure \\nwith three modules in series and two substrings in parallel. Among them, the substrings containing blocking \\ndiodes or not results in the two configuration types of PV arrays, whose structure is shown in Fig. 1. In order \\nto characterize the faults under the operating environmental conditions of the real PV array as much as \\npossible, we take one year of outdoor irradiance and temperature records measured from the PV plant as the \\nenvironmental control variables for the PV array model. \\n \\n \\nFig. 6. The single-diode PV cell model. \\nTable 1 Electrical characteristics of solar module Shell SP-70 at nominal condition (25°C and 1000 W/m2). \\nParameter \\nValue \\nISC (A) \\n4.7 \\nVOC (V) \\n21.4 \\nIMPP (A) \\n4.25 \\nVMPP  (V) \\n16.5 \\nKV (mV/℃) \\n-76 \\nKi (mA/℃) \\n2 \\nNS \\n36 \\nRS (Ω) \\n0.41 \\nRP (Ω) \\n141 \\nTo fully demonstrate the generalizability of the proposed fault diagnosis method, the faults of PV arrays \\nwith two types of blocking diode configurations under different operating conditions are validated, including \\ncontamination-prone operating condition and ideal normal operating condition. The two operating conditions \\nconsist of 14 and 9 types of faults, respectively, as follows. \\nCase1：PV arrays under operating condition with non-uniform soiling impact \\n1) Two types of line-to-line short-circuit (LL): one or two modules in one string are shorted (noted as \\nLL1 and LL2 respectively).  \\n2) Open-circuit (OC): one string is open. \\n3) Two types of shading (Shade): 1 or 2 modules are shaded to different degrees (noted as Shade1 and \\nShade2, respectively). \\n4) Series resistance degradation of the array (Sdegration): an increase in the equivalent series resistance \\nof array. \\n5) Parallel resistance degradation of the array (Adegration): a decrease in the equivalent parallel \\nresistance of array. \\n6) Non-uniform soiling (Soiling): the accumulation of soiling with varying degrees on the surface of \\neach PV module in the presence of contamination. \\n7) Line-to-line short-circuit under the impact of non-uniform soiling (soiling_LL): 1 or 2 modules in \\none string are shorted under varying soiling accumulation (denoted as soiling_LL1 and soiling_LL2, \\nrespectively). \\n8) Open-circuit under the impact of non-uniform soiling (soiling_OC): one string is open under varying \\nsoiling accumulation. \\n9) Series resistance degradation of the array under the impact of non-uniform soiling \\n(soiling_Sdegartion): an increase in the equivalent series resistance of array under varying soiling \\naccumulation. \\n10) Parallel resistance degradation of the array under the impact of non-uniform soiling \\n(soiling_Adegartion): a decrease in the equivalent series resistance of array under varying soiling \\naccumulation. \\n Case2：PV arrays under operating condition without non-uniform soiling impact \\nThe types of faults include 1) - 6) above under the ideal operating condition without the influence of \\nnon-uniform soiling. \\nAs an example, the characteristic curves for faults of the PV array with the two blocking diode \\nconfigurations in Case1, including I-V and P-V are shown in Fig. 7 and Fig. 8. \\n \\nFig. 7. Faults characteristic curves of PV array with blocking diodes. \\n \\nFig. 8. Faults characteristic curves of PV array without blocking diodes. \\nImportantly, what is different from [18, 21, 37] is that the simulations of faults in this study for Shade, \\nADegration, SDegration, and Soiling, as well as the severity of these faults, are not simply set as constants. \\nThis is due to the fact that these faults change dynamically with time in service, therefore the simulations \\nshould contain the full range of fault parameters. In this study, the environmental conditions of a real PV \\nplant are used as inputs for the temperature and irradiance of the PV array model, and the fault parameter of \\neach fault type is set randomly within its corresponding full-scale parameter range to obtain characteristic \\ncurves. Specifically, for shading, the irradiance gain is set from 20% (low shading) to 100% (full shading) \\nfor one or two modules; for soiling, a special form of shading, the irradiance gain is set randomly within 10% \\nfor all modules to simulate non-uniform soiling accumulation; for abnormal aging of ADegration and \\nSDegration, the aging resistance values are set randomly for degradation in [20Ω, 200Ω] and [1Ω, 15Ω] \\nrespectively, adjusting for different levels of fault severity. Considering the dust-influenced operating \\ncondition, faults are simulated by superimposing other faults alongside the non-uniform soling, where the \\ndegree of soiling and dynamic fault parameters are set randomly when it comes to dynamically growing fault \\ntypes. The representation of different faults with full-scale dynamic fault parameter is shown in Fig. 9, where \\nthe characteristic curves show different shapes of distortions. \\n \\nFig. 9. I-V curves of different faults with full-scale dynamic fault parameter. \\n3.2 Graphical feature transformation method of Isc-Voc normalized GADF \\nThe graphical feature transformation method using the Gramian angular difference field (GADF) [51] \\nenables the extraction of complete information from the resampled characteristic curves. Typically, GADF is \\napplied to the transformation of time series signals, which mainly includes the following steps: \\nStep 1: Scale the data to [0, 1] according to the following equation. \\n \\nmin(\\n)\\nmax(\\n)\\nmin(\\n)\\ni\\ni\\nx\\nX\\nx\\nX\\nX  \\n(13) \\nwhere, the time series data is\\n1\\n2\\n{ ,\\n,...,\\n}\\nN\\nx x\\nX\\nx\\n\\uf03d\\nand its value at each timestamp is \\ni\\nx , \\ni\\nx  is noted as \\nthe normalized value. \\nStep 2: Convert the rescaled sequence to the polar coordinate system, i.e. the values of the time series \\nare treated as the cosine of the angle. The formulas for converting to polar coordinates are: \\n \\narccos\\n,0\\n1,\\n,\\ni\\ni\\ni\\ni\\ni\\ni\\ni\\nx\\nx\\nx\\nX\\nt\\nr\\nt\\nN\\nN\\n\\uf066\\n \\n(14) \\nwhere, \\ni\\nt  is the time stamp of point \\ni\\nx , N is the number of all the time points contained in the time \\nseries, and X represents the rescaled time series. \\nStep 3: Obtain the angle difference of each pair, and then take the sine value for difference to form the \\nGADF matrix: \\n \\n2\\n2\\nsin\\ni\\nj\\nGADF\\nI\\nX\\nX\\nX\\nI\\nX\\n\\uf066\\n\\uf066\\n \\n(15) \\nwhere, I is the unit row vector [1, 1, ..., 1], \\ni\\nr  is the value of the polar axis, and \\ni\\n\\uf066 the value of the \\npolar angle. \\nIt is worth noting that when applied to time series, the scaling of GADF is using the full-time axis data, \\nand thus when applied to the scaling of the I-V/P-V characteristic curves, the V-axis should be analogous to \\nthe time axis. If GADF with normal normalization following steps described above is used to convert the \\ncharacteristic curves, i.e. the conversion areas are A, B, C and D respectively in Fig. 10, in essence, the \\ncurrent VOC and ISC of each fault state are taken as their respective normalized maximum values, which would \\nblur the differences in VOC and ISC between the different faults. We can observe from the GADF \\ntransformation results in Fig. 11 that when GADF transformation is performed on the I-axis or P-axis with \\nthe normal normalization strategy, the same fault types under different irradiance and temperature conditions \\nare relatively consistent in the shape of the converted graphical features, which can avoid misidentification \\nof the same faults due to different environments. However, similarities in shapes also exist between the \\ncharacteristic curves of certain different fault types, such as health, LL1, LL2, and OC in Fig. 11. In fact, \\nthese curves differ in absolute values of VOC and ISC. Therefore, it is necessary to propose a new GADF \\ntransformation method that retains consistency in the characteristics of the transformation features for the \\nsame fault in different environmental conditions, while having the ability to distinguish the similarity in the \\nshapes of the characteristic curves for different fault types. \\n \\nFig. 10. Normal normalization area of similar faults under the same ambient condition. \\nThrough the analysis of the single-diode equivalent model of the PV module in Fig. 6, the calculation \\nequations for the short-circuit current ISC and the open-circuit voltage VOC are obtained as follows: \\nWhen the external circuit is short-circuited, i.e. the load is 0, the short-circuit current ISC is: \\n \\nSC\\nS\\nSC\\nS\\nSC\\n0\\nP\\nph\\n1\\nI\\nR\\nnVT\\nI\\nR\\nI\\nI\\nI e\\nR\\n \\n(16) \\nThe dark current \\nSC\\nS\\n0\\nd\\n1\\nI\\nR\\nnVT\\nI\\nI e\\n \\n flowing through the diode is very small and can be ignored. \\nConsidering that RP >> RS, \\nSC\\nS\\nP\\nI\\nR\\nR\\n can also be ignored [47], then: \\n \\np\\nSC\\nh\\nI\\nI\\n \\n(17) \\n \\nph\\nSC,n\\ni (\\n298) 1000\\nG\\nI\\nI\\nk\\nT\\n \\n(18) \\nwhere, T is the temperature in Kelvin, G is the irradiance, and ki is the temperature coefficient of short-\\ncircuit current. \\nWhen the external circuit is open, i.e. when the load is close to ∞ and I = 0, the open-circuit voltage VOC \\nis [52]: \\n \\nSC\\nS\\nSC\\nph\\n0\\nS\\nP\\n1\\nI\\nR\\nnVT\\nI\\nR\\nI\\nI\\nI e\\nR\\n \\n(19) \\n \\nOC\\nph\\nOC\\nP\\n0\\n0\\n1\\nV\\nnVT\\nV\\nI\\nI e\\nR\\n \\n(20) \\n \\nph\\n0\\nOC\\nln\\n1\\nI\\nnkT\\nV\\nq\\nI\\n \\n(21) \\nwhere, the diode saturation current \\n0\\nI  is expressed as [53, 54]:  \\n \\n3\\ng\\nn\\n0\\n0,n\\nn\\n1\\n1\\nexp qE\\nT\\nI\\nI\\nT\\nnk\\nT\\nT\\n \\n(22) \\n \\nSC,n\\n0,n\\nOC,n\\nt,n\\nexp\\n/\\n1\\nI\\nI\\nV\\nnV\\n \\n(23) \\n \\nn\\nt,n\\nS\\nN kT\\nV\\nq\\n \\n(24) \\nwhere the band gap energy of the semiconductor is represented by Eg and the nominal saturation current \\nat the nominal temperature Tn, i.e. 25°C, is represented by I0,n via (23), the nominal thermal voltage \\nrepresented by Vt,n is shown in (24), ISC,n is the short-circuit current at nominal temperature, and VOC,n is the \\nopen-circuit voltage at nominal temperature. In addition, n is the ideal factor of the diode, NS is the number \\nof series cells forming the module, k is the Boltzmann constant and q is the charge quality. Thus, it can be \\nseen from (22) that I0 is only related to the temperature T. \\nIt can be further concluded from (18) and (21) that the short-circuit current ISC and open-circuit voltage \\nVOC of PV modules depend only on irradiance G and ambient temperature T, except for the constant factors \\nwith fixed values, which means that the ideal ISC and VOC are the same for different fault types under the \\nsame environmental conditions. Therefore, we propose a GADF method based on the normalization of ISC \\nand VOC under environmental conditions, which can ensure that the transformed graphical features of the \\nsame fault types are the same in different environments, while having distinguishability between different \\nfaults with similar forms. Specifically, the main implementation process of the Isc-Voc normalized GADF \\nmethod is: \\n(1) Calculate the ideal ISC and VOC (denoted as ISC_ideal and VOC_ideal) of the ambient conditions \\ncorresponding to each characteristic curve, expand the maximum value of V-axis to the ideal VOC, and \\ncomplement the current and power of the characteristic curve by 0 over the range of the measured VOC and \\nthe ideal VOC. \\n(2) Resample the complemented characteristic curves using the bilinear difference method to provide \\nuniformly distributed characteristic curves with a small amount of data. Specifically, the V-axis of I-V and \\nP-V curves is within the range of [0, VOC_ideal] at a uniform voltage interval and the I-axis and P-axis are \\nresampled at the same uniform interval, reducing data of each curve from the original 200 points to 50 points. \\n(3) Convert the resampled characteristic curves to graphical features according to Isc-Voc normalized \\nGADF. That is, the V-axis is normalized according to the range of [0, VOC_ideal], the I-axis is normalized \\naccording to the range of [0, ISC_ideal], and the P-axis is normalized according to the range of [0, VOC_ideal * \\nISC_ideal]. \\n(4) Then, calculate the inner product according to the differences of normalized angles, preserving the \\ntime dependence of the V-axis, and generate a GADF matrix with size of 502. The I-V or P-V characteristic \\ncurve corresponding to each environmental condition is transformed into a matrix, and the I-V and P-V \\ntransformation matrices are stacked to form a 2-channel 2D matrix as the input to the fault diagnosis model. \\nThis method makes it possible to select a unified VOC under the same environmental conditions, which \\ncorresponds to the V-axis having the same time scale. In this way, all changes in characteristic curves can be \\nreflected in the transformation matrices. When the slope of the characteristic curve changes, the diagonal \\nregions in the matrix shrink in different directions. Further, the normalization strategy of selecting the \\nmaximum value of VOC and the maximum value of ISC in all characteristic curves, without distinguishing \\nenvironmental conditions, is also compared and referred to as the global normalized GADF. More detailed \\ndiscussion and analysis of the three current universal applied methods (direct I-V, RP, and GADF) as well as \\nthe proposed normalization strategies are provided in Chapter 4. \\n \\nFig. 11. Graphical matrices transformed by GADF with different normalization strategies for similar faults \\nunder different environmental conditions (the matrices are colored for only visualization). \\n3.3 Classification model of convolutional neural network with CBAM module  \\nTo improve the ability of diagnosing complicated faults, we design a PV array fault diagnosis model of \\nconvolutional neural network with CBAM module, referred to as CNN-CBAM, with the transformed \\ngraphical feature matrices containing full information of the characteristic curves as the input features.  \\n \\nFig. 12. The structure of proposed classification model of convolutional neural network with CBAM module. \\nThe structure of the model is shown in Fig. 12, which is mainly composed of convolution modules and \\nCBAM attention modules. \\nThe Convolution module can reduce the dimensions in time and space and lower the number of free \\nparameters required for training [55], due to the benefits of local receptive field and weight sharing. Therefore, \\nperformance can be enhanced. Specifically, the Conv2d layer slides each filter in the input feature matrix \\nthrough the local receptive field, calculates the sum of dot products on the local field, and automatically \\nextracts the effective features from the inputs. Then, the Pooling layer divides the input area and calculates \\nthe average value of each area to complete the down sampling of the feature matrix. \\nThe CBAM module focuses on the important features and suppresses the unimportant features in the \\nnetwork, which can effectively improve the performance of CNN. And the CBAM is composed of the channel \\nattention module (CAM) and the spatial attention module (SAM) [56]. Their detailed structures are shown in \\nFig. 13 and Fig. 14, respectively. Among them, the CAM emphasizes that the network should concentrate on \\nuseful channel features while ignoring other aspects by using the maximum pool and the average pool to \\ncompress the spatial dimension of the feature matrix. The SAM highlights that the network should focus on \\nthe local area of interest by applying the average pool and maximum pool along the channel dimension to \\nretain the background information of the feature matrix. Specifically, first, the input feature F is multiplied \\nby the feature matrix MC(F) generated by CAM compression along the spatial dimension to obtain F'. Then, \\nSAM compresses F' along the channel dimension to generate the spatial feature matrix MS(F'). Finally, the \\noptimized feature matrix F'' is obtained by multiplying MS(F') and F'. The whole process can be represented \\nby: \\n \\n'\\n( )\\nC\\nF\\nM\\nF\\nF  \\n(25) \\n \\n''\\n'\\n'\\n(\\n)\\nS\\nF\\nM\\nF\\nF  \\n(26) \\nwhere,\\nrepresents multiplication between elements \\n \\nFig. 13. The structure of channel attention module (CAM). \\n \\nFig. 14. The structure of spatial attention module (SAM). \\nThe CAM module processes and aggregates the input features with the maximum pool and average pool, \\nand then inputs them into a weight-sharing multi-layer perceptron (MLP) network for summation, which is \\nactivated by sigmoid to generate the final channel attention feature MC(F). The calculation program of CAM \\ncan be expressed as:  \\n \\n( )= (\\n(\\n( ))\\n(\\n( )))\\nC\\nM\\nF\\nMLP AvgPool F\\nMLP MaxPool F\\n\\uf073\\n \\n(27) \\nThe SAM module performs maximum pool and average pool operations on the input feature F' along \\nthe channel dimension, generates a two-layer feature matrix, and cascades them together. Then, the \\nconvolution kernel with size of 7×7 is used to reduce the dimension of features, and the sigmoid is applied \\nto generate the spatial attention feature MS(F'). The calculation program of SAM can be expressed as: \\n \\n'\\n'\\n'\\n(\\n)= ( ([\\n(\\n);\\n(\\n)]))\\nS\\nM\\nF\\nf\\nAvgPool F\\nMaxPool F\\n\\uf073\\n \\n(28) \\nwhere, σ denotes the sigmoid function and f denotes the convolution operation with filter. \\nTable 2 shows the specific structural configuration of the proposed PV array fault diagnosis model based \\non CNN and CBAM. The Convolution blocks with different sizes of convolutional kernels are stacked \\ntogether along the depth direction to extract features at different levels. This structure can automatically \\nextract more effective feature information directly from the original input feature matrix. In this study, two \\nCBAM modules are respectively embedded into the convolution modules, and the classification accuracy of \\nthe faults diagnosis model is improved through the processing capability of focused feature information. \\nTable 2 Detailed configuration of the CNN-CBAM. \\nLayer \\nOutput shape \\nDetailed structure \\nInput Layer \\n(50, 50, 2) \\n \\nConvolution 1 \\n(46, 46, 32) \\nk = 3×3, filter = 8,  \\nstride = 1×1, padding = 1 \\nk = 3×3, filter = 32,  \\nstride = 1×1, padding = 1 \\nCBAM 1 \\n(46, 46, 32) \\n \\nConvolution 2 \\n(44, 44, 64) \\nk = 3×3, filter = 64,  \\nstride = 1×1, padding = 1 \\nCBAM 2 \\n(44, 44, 64) \\n \\nGlobal Avgpool \\n64 \\n \\nFully connected \\n16 \\n \\nOutput Layer \\nclasses \\n \\n4. Results and discussion \\n4.1 Experimental setup \\nThe data used in this study are based on the configurations described in Section 3.1. Specifically, the \\nfaults of PV arrays with two blocking diode configurations under various operating conditions are analyzed, \\nincluding 14 faults under contamination-prone operating condition and 9 faults under ideal operating \\ncondition. To make the data fully reflect the real operating conditions, we take the collected annual ambient \\nrecords of the actual power station as the environmental control input for each fault type to obtain the \\ncorresponding characteristic curves. The data is divided into training set and testing set, accounting for 80% \\nand 20% respectively. And 90% of the training set is the training data and 10% is the validation data. The \\ndata volume and proportion of each data set applied to different operating conditions are shown in Table 3. \\nTable 3 The specific information of the different dataset. \\n \\n \\nPV array with blocking diodes  \\nPV array without blocking diodes \\n \\n80% \\n20% \\n80% \\n20% \\n \\nTraining \\ndata \\n(90%) \\nValidation \\ndata \\n(10%) \\nTesting \\ndata \\nTraining \\ndata \\n(90%) \\nValidation \\ndata \\n(10%) \\nTesting \\ndata \\nConsidering soiling \\nimpact \\nNumber of classes \\n14 \\n14 \\n14 \\n14 \\n14 \\n14 \\nNumber of data \\n43192 \\n4800 \\n11998 \\n43192 \\n4800 \\n11998 \\nWithout considering \\nsoiling impact \\nNumber of classes \\n9 \\n9 \\n9 \\n9 \\n9 \\n9 \\nNumber of data \\n27766 \\n3086 \\n7713 \\n27766 \\n3086 \\n7713 \\n4.2 Faults diagnosis evaluation indexes \\nThe widely used Precision, Recall, F1-score, and Accuracy are selected as the indicators of the \\neffectiveness of the classification algorithm [57, 58], which can be calculated as: \\n \\nPrecision\\nTP\\nTP\\nFP\\n\\uf03d\\n\\uf02b\\n \\n(29) \\n \\nRecall\\nTP\\nTP\\nFN\\n\\uf03d\\n\\uf02b\\n \\n(30) \\n \\n2 Recall Precision\\nF1_score=\\nRecall\\nPrecision\\n\\uf0d7\\n\\uf0d7\\n\\uf02b\\n \\n(31) \\n \\nAccuracy=\\nTP\\nTN\\nTP\\nFP\\nFN\\nTN\\n\\uf02b\\n\\uf02b\\n\\uf02b\\n\\uf02b\\n \\n(32) \\nwhere, the number of samples that belong to the positive category and are predicted to be in the positive \\ncategory are referred to as true positives (TP), the number of samples that belong to the positive category and \\nare predicted to be in the positive category are referred to as false negatives (FN), the number of samples that \\nbelong to the negative category and are predicted to be in the positive category are referred to as false \\npositives (FP), and the number of samples that belong to the negative category and are predicted to be in the \\nnegative category are referred to as true negatives (TN). \\n4.3 Case 1. PV arrays under operating condition with non-uniform soiling impact  \\nThis part analyzes the effectiveness of different methods to distinguish the 14 fault types of PV arrays \\nunder Case 1 operating condition with non-uniform soiling impact, including the accuracy and performance \\nof different graphical feature transformation methods and classification algorithms for fault diagnosis. \\n4.3.1 Analysis of graphical feature transformation methods  \\nFirst, we compare data preprocessing methods for fault diagnosis that do not rely on STC correction, as \\npresented in Section 2.2, including methods that combine I-V curves and environmental variables to form a \\ntwo-dimensional data matrix (GTIV) and methods that utilize the complete feature information of the \\ncharacteristic curves. In addition, the diagnosis results of different graphical feature transformation methods \\nsuch as GADF and recurrent plot (RP) using the complete information of the characteristic curves are \\nanalyzed, and three strategies of using the I-V graphical feature matrix, P-V graphical feature matrix and IV-\\nPV stacked graphical feature matrix obtained from transformation as input are further compared. \\nTable 4 The fault diagnosis results of applying different data preprocessing methods. \\n \\n \\n \\nPrecision \\nRecall \\nF1-score \\nAccuracy \\nCircumstance 1: \\nWith blocking \\ndiodes \\nGADF \\nI-V \\n98.40% \\n98.31% \\n98.35% \\n98.46% \\nP-V \\n95.27% \\n95.16% \\n95.21% \\n95.33% \\nIV-PV \\n98.53% \\n98.48% \\n98.50% \\n98.58% \\nRecurrent \\nPlot \\nI-V \\n80.97% \\n80.03% \\n80.50% \\n81.77% \\nP-V \\n86.87% \\n86.27% \\n86.57% \\n86.98% \\nIV-PV \\n94.43% \\n94.23% \\n94.33% \\n95.12% \\nDirect IV \\nGTIV \\n95.86% \\n95.79% \\n95.82% \\n95.98% \\nCircumstance 2: \\nWithout \\nblocking diodes \\nGADF \\nI-V \\n98.38% \\n98.25% \\n98.31% \\n98.50% \\nP-V \\n91.56% \\n91.32% \\n91.44% \\n92.06% \\nIV-PV \\n98.38% \\n98.28% \\n98.33% \\n98.48% \\nRecurrent \\nPlot \\nI-V \\n67.04% \\n65.53% \\n66.28% \\n65.94% \\nP-V \\n79.32% \\n78.25% \\n78.78% \\n78.69% \\nIV-PV \\n90.08% \\n89.86% \\n89.97% \\n90.85% \\nDirect IV \\nGTIV \\n95.61% \\n95.52% \\n95.56% \\n95.65% \\nTable 4 shows the results of applying different data preprocessing methods to the characteristic curves \\nfor fault diagnosis, all of which use the multilayer-CNN classifier. It can be seen that when the graphical \\nfeature matrix transformed by single curve is used as input, i.e. the I-V or the P-V characteristic curve, the \\neffectiveness of the information contained in the transformation matrix in distinguishing fault types is related \\nto the feature transformation methods. Sp \\necifically, the I-V graphical feature of the GADF transformation is more effective in classification than \\nthe P-V graphical feature, whereas the opposite is true for the RP transformation. In fact, the classification \\naccuracy of the IV-PV stacked graphical feature matrix is usually higher than that of the single characteristic \\ncurve transformed matrix. Additionally, for the faults classification of PV array considering the impact of \\ndust, the accuracies of the graphical feature matrix transformed by GADF are 98.58% and 98.48% \\nrespectively in the configurations of with and without blocking diodes, which is better than the RP \\ntransformation and GTIV matrix methods. Fig. 15 shows the classification accuracy of the optimal feature \\nstrategy for the three preprocessing methods, where the stacked IV-PV matrix is used for graphical feature \\ntransformation method. Among them, the accuracy of RP transformation method for arrays with blocking \\ndiodes is 95.12%, which is significantly higher than 90.85% without blocking diodes. This may be related to \\nthe similarity between the two types of short-circuit and the health state without blocking diode configuration \\nwhen RP transformation is applied, while the GADF transformation is applicable to different PV array \\nconfigurations, with high accuracy of 98.58% and 98.48% respectively.  \\n \\nFig. 15. Comparison of the optimal feature strategies for three preprocessing methods. \\nThe above comparative analysis confirms the effectiveness of the GADF graphical feature \\ntransformation method for fault diagnosis of PV arrays with different blocking diode configurations. Further, \\nTable 5 compares different GADF normalization approaches. The application of GADF with good diagnostic \\naccuracy first presented in [39] was based on the ANN method, while more types of faults and higher \\ncomplexity of faults are involved in this study, therefore the classification effects of multilayer-CNN and \\nANN are compared. \\nTable 5 The fault diagnosis results of applying different GADF normalization methods. \\n \\n \\nGADF \\nTesting accuracy \\n \\n \\nNormal \\nnormalization \\nGlobal  \\nnormalization \\nIsc-Voc \\nnormalization \\nCircumstance 1: \\nWith blocking \\ndiodes \\nANN \\nIV \\n84.52% \\n86.81% \\n95.66% \\nPV \\n80.44% \\n80.36% \\n92.97% \\nIV-PV \\n91.42% \\n87.40% \\n96.11% \\nMultilayer\\nCNN  \\nIV \\n98.46% \\n98.25% \\n98.85% \\nPV \\n95.33% \\n91.54% \\n97.83% \\nIV-PV \\n98.58% \\n98.19% \\n99.10% \\nCircumstance 2: \\nWithout \\nblocking diodes \\nANN \\nIV \\n72.30% \\n83.66% \\n94.97% \\nPV \\n65.54% \\n75.40% \\n91.58% \\nIV-PV \\n87.92% \\n82.96% \\n95.22% \\nMultilayer\\nCNN \\nIV \\n98.50% \\n98.04% \\n98.42% \\nPV \\n92.06% \\n91.23% \\n97.25% \\nIV-PV \\n98.48% \\n97.79% \\n98.65% \\nThe results in Fig. 16 show that for both blocking diode configurations, the graphical feature \\ntransformation preprocessing method using Isc-Voc normalized GADF is significantly more accurate in \\nclassifying faults than either the normal normalization or global normalization strategies. In particular, when \\nclassifying with relatively simple methods such as ANN, the classification effect mainly depends on the \\neffectiveness of the features in distinguishing faults. In the case of array with blocking diodes, for example, \\nthe classification accuracy of the Isc-Voc normalized GADF transformation is as high as 96.11%, while the \\nhighest classification accuracy of other normalization strategies is only 81.42%. This also fully demonstrates \\nthat the Isc-Voc normalization strategy can obtain features that make the GADF transformation matrix of \\ndifferent fault types better distinguishable compared to other normalization strategies. Moreover, the \\nclassification accuracy of the CNN is significantly improved over ANN when using the feature matrix of I-\\nV or P-V obtained by normal normalization or global normalization strategies as input, with the diagnostic \\naccuracy of the array without blocking diodes improved by 10.56% and 14.83% respectively. Further, when \\nthe I-V/P-V matrix or stacked IV-PV matrix processed by Isc-Voc normalized GADF is used as the input \\nfeatures for the CNN model, they both have higher diagnostic accuracy, which is related to the stronger \\nlearning capability of the multilayer CNN. \\n \\nFig. 16. Comparison of three GADF normalization methods with different input matrices and classifiers. \\nThe loss and accuracy of training and validation in Fig. 17 show that using the IV-PV stacked feature \\nmatrix can quickly achieve higher classification accuracy, indicating that the stacked matrix has the advantage \\nof more significant differentiation than the others. Similarly, when the features processed by the Isc-Voc \\nnormalization strategy are used as the input of the classification model, the training accuracy of the model in \\nFig. 18 is rapidly improved, and it takes less time to train to the highest accuracy than the features processed \\nby other normalization strategies. In other words, the proposed Isc-Voc normalized GADF graphical feature \\nmatrix is more efficient and more accurate when applied to fault diagnosis. \\n \\nFig. 17. The loss and accuracy for three input matrices. \\n \\nFig. 18. The loss and accuracy for three GADF normalization methods. \\n4.3.2 Analysis of convolutional neural network classification models \\nAlthough the classification accuracy of multilayer CNN in the previous section is relatively considerable, \\nthe structure of this model is simple and there is still room for improvement. In this section, we further explore \\nthe improvement of classification accuracy of CNN-based models with different structures. Therefore, we \\ndesign and compare the basic multilayer convolutional neural network, the multi-scale convolutional neural \\nnetwork presented in [37], and the proposed convolutional neural network with CBAM module, whose \\nstructures are shown in Fig. 19. \\n \\nFig. 19. The structures of compared three CNN-based classification models. \\n The results from Table 6 show that the IV-PV stacked feature matrix obtained from the Isc-Voc \\nnormalized GADF transformation has the highest classification accuracy as input to the CNN-based \\nclassification models for all three structures, regardless of the blocking diode configuration of the PV array. \\nIn addition, the multi-scale CNN performs better than the multilayer CNN when using the transformed \\nfeatures of all three GADF normalization strategies as input, mainly because the multi-scale CNN extracts \\nfeatures at different scales. When the CBAM module is embedded in the multilayer CNN, the introduction \\nof channel attention mechanism and spatial attention mechanism enables the network to extract features of \\ninterest in the local area while focusing on the channel features, which is more selective in focus than the \\nmulti-scale CNN with different scales of convolutional kernels for feature extraction. Overall, the CNN with \\nCBAM module has the highest classification accuracy, with the stacked IV-PV graphical feature matrix \\ntransformed by the proposed Isc-Voc normalized GADF as the input. The diagnosis accuracies applied to PV \\narrays with two diode configurations are 99.62% and 99.40% respectively. \\nTable 6 The fault diagnosis results of applying different CNN-based classification models. \\n \\n \\nGADF \\nTesting accuracy \\n \\n \\nNormal \\nnormalization \\nGlobal  \\nnormalization \\nIsc-Voc \\nnormalization \\nCircumstance 1: \\nWith blocking \\ndiodes \\nMultilayer \\nCNN \\nIV \\n98.46% \\n98.25% \\n98.85% \\nPV \\n95.33% \\n91.54% \\n97.83% \\nIV-PV \\n98.58% \\n98.19% \\n99.10% \\nMulti-scale \\nCNN \\nIV \\n98.96% \\n98.75% \\n99.21% \\nPV \\n96.75% \\n91.44% \\n98.81% \\nIV-PV \\n99.10% \\n98.65% \\n99.31% \\nProposed \\nCBAM-CNN \\nIV \\n99.10% \\n99.02% \\n99.58% \\nPV \\n98.35% \\n95.83% \\n99.08% \\nIV-PV \\n99.42% \\n98.98% \\n99.62% \\nCircumstance 2: \\nWithout \\nblocking diodes \\nMultilayer \\nCNN \\nIV \\n98.50% \\n98.04% \\n98.42% \\nPV \\n92.06% \\n91.23% \\n97.25% \\nIV-PV \\n98.48% \\n97.79% \\n98.65% \\nMulti-scale \\nCNN \\nIV \\n98.60% \\n98.96% \\n98.73% \\nPV \\n94.21% \\n92.92% \\n98.33% \\nIV-PV \\n98.85% \\n98.54% \\n99.06% \\nProposed \\nCBAM-CNN \\nIV \\n98.92% \\n99.02% \\n99.32% \\nPV \\n96.06% \\n94.10% \\n98.81% \\nIV-PV \\n99.15% \\n98.77% \\n99.40% \\n \\nFig. 20. The confusion matrices of PV arrays with blocking diodes under soiling condition. \\nFurthermore, the preprocessing methods of Isc-Voc normalized GADF and normal normalized GADF, \\nwhich are both highly accurate in using the CNN-CBAM as classifier, are compared. Fig. 20 and Fig. 21 are \\nthe confusion matrices of fault diagnosis for PV arrays with and without blocking diodes, respectively. For \\narrays with blocking diodes, the two normalization strategies are similar in the discrimination ability of most \\nfault types. The specific difference is that the Isc-Voc normalized GADF performs slightly better than the \\nnormal normalized GADF in terms of OC, Adegration, and Adegation under soiling, due to the similarity of \\nthese characteristic curves under some environmental conditions and levels of faults. The former reduces the \\nproportion of OC misdiagnosed as Adegeration by 59% (13 samples) and does not produce the 11 samples \\nfor which the latter diagnoses OC as Adegration under soiling. In addition, for arrays without blocking diodes, \\nthe characteristic curves of LL2 generated by current backflow are highly similar to those of Adegeration and \\nOC, etc. The Isc-Voc normalized GADF has higher discrimination than the normal normalized GADF in this \\ncase, and their overall recall and F1-score both are 99.28% and 98.82, respectively. Specifically, the Isc-Voc \\nnormalized GADF can avoid 11 samples of LL2 misclassified as Adegration, 15 samples of OC misidentified \\nas Adegration under soiling, and more 11 samples of LL1 and OC under soiling that are not correctly \\ndistinguished. Moreover, the Isc-Voc normalized GADF is 27% more accurate than the normal normalized \\nGADF in distinguishing between Adegration under soiling and LL2 under soiling and improved the \\ndiagnostic accuracy of complex faults affected by contamination by 41.94%. \\n \\nFig. 21. The confusion matrices of PV arrays without blocking diodes under soiling condition. \\nGenerally, the complexity of faults increases due to the impact of dust, there will be individual errors \\nin the classification of all fault types. And the proposed Isc-Voc normalized GADF method has higher \\naccuracy in differentiating complex faults and is able to converge quickly during training, and the processed \\nfeatures are beneficial for the rapid learning of parameters, which provide the best classification performance \\nwhen applied to the proposed CNN-CBAM, as shown in Fig. 22. \\n \\nFig. 22. The loss and accuracy for typical CNN-based classifiers with three GADF normalization methods. \\n4.4 Case 2. PV arrays under operating condition without non-uniform soiling impact \\nTo further illustrate the universality of the proposed method, the performance of fault diagnosis under \\nnormal operating condition without dust impact is analyzed, which contains 9 fault states. Similarly, we \\nanalyze the influence of transformed graphical features processed by various GADF normalization methods \\nas input on the accuracy of classification algorithms. Specifically, as shown in Table 7, for simple classifiers \\nsuch as ANN, using the matrix of I-V or P-V curve converted by Isc-Voc normalized GADF as input results \\nin a significant classification accuracy improvement of 97.57% compared to 93.26% for normal normalized \\nGADF and 93.20% for global normalized GADF. And the accuracy of the proposed normalized GADF is up \\nto 98.26% when the IV-PV stacked transformation matrices are used as the input features. Moreover, the \\nCBAM-CNN exhibits general merit when the features processed by the optimal GADF strategy, i.e. stacked \\nIV-PV transformed by Isc-Voc normalization, are applied to the CNN-based models. This is related to the \\nfact that the types of faults to be distinguished in operation are not diverse and their complexity is general. \\nIn other words, for the faults not affected by dust, the graphical feature matrices transformed by Isc-Voc \\nnormalized GADF applied to a relatively simple structured CNN can obtain satisfactory results, but it is \\nundeniable that the CNN embedded with CBAM module still has a slight accuracy advantage. \\nTable 7 The fault diagnosis results of applying different classifiers and GADF methods. \\n \\n \\nGADF \\nTesting accuracy \\n \\n \\nNormal \\nnormalization \\nGlobal  \\nnormalization \\nIsc-Voc \\nnormalization \\nCircumstance 1: \\nWith blocking \\ndiodes \\nANN \\nIV \\n87.59% \\n93.20% \\n97.57% \\nPV \\n93.26% \\n91.02% \\n97.57% \\nIV-PV \\n98.09% \\n93.68% \\n98.26% \\nMultilayer  \\nCNN \\nIV \\n99.22% \\n98.28% \\n99.58% \\nPV \\n98.41% \\n96.99% \\n99.38% \\nIV-PV \\n99.42% \\n98.54% \\n99.58% \\nProposed \\nCBAM-CNN \\nIV \\n99.13% \\n98.61% \\n99.80% \\nPV \\n98.83% \\n97.57% \\n99.51% \\nIV-PV \\n99.18% \\n98.35% \\n99.84% \\nCircumstance 2: \\nWithout \\nblocking diodes \\nANN \\nIV \\n75.28% \\n94.44% \\n97.81% \\nPV \\n82.74% \\n91.34% \\n97.89% \\nIV-PV \\n94.44% \\n94.83% \\n98.22% \\nMultilayer  \\nCNN \\nIV \\n99.22% \\n98.70% \\n99.25% \\nPV \\n98.39% \\n96.95% \\n99.35% \\nIV-PV \\n99.22% \\n98.61% \\n99.42% \\nProposed \\nCBAM-CNN \\nIV \\n99.19% \\n99.13% \\n99.77% \\nPV \\n98.51% \\n97.54% \\n99.45% \\nIV-PV \\n99.25% \\n98.38% \\n99.81% \\nFig. 23 and Fig. 24 show the fault classification results of Isc-Voc normalized GADF and normal \\nnormalized GADF for PV array with the two blocking diode configurations. It can be seen that the confusion \\nmatrix of the former does not contain many sporadic misclassifications of the latter and is expressed in the \\noverall recall and F1-score of 99.81% and 99.18%, respectively. In the fault diagnosis of PV array with \\nblocking diode configuration, the classification error between SDegration and Non-uniform soiling is 84% \\nless in the former than in the latter, with 8 and 50 samples respectively, and the proportion is 80.43% in the \\narray without blocking diode. This may be due to the low fault levels of these two fault types with dynamic \\nfault parameters, i.e. the low fault differentiation between some SDegartions with low fault levels and low \\ndegrees of dust shielding. In fact, the correct classification accuracy for both SDegration and Non-uniformed \\nsoiling, which may be misclassified, still reaches over 99% for either diode configuration of array. Overall, \\nthe proposed CBAM-CNN model based on Isc-Voc normalized GADF transformation method is able to \\nachieve high accuracy in faults classification, which is also applicable to ideal operating condition without \\ndust impact. \\n \\nFig. 23. The confusion matrix of PV arrays with blocking diodes. \\n \\nFig. 24. The confusion matrix of PV arrays without blocking diodes. \\nThe training and validation process of two blocking diode configurations are shown in Fig. 25 and Fig. \\n26. When the inputs are graphical features processed by Isc-Voc normalized GADF, the loss of model \\ndecreases the fastest and the accuracy rises rapidly compared to other normalization strategies, whether for \\nANN, CNN, or CNN-CBAM as the classifiers. It is consistent with the original intention that the graphical \\nfeature transformation method proposed can achieve high discrimination between similar faults in different \\nenvironments. In particular, for arrays without blocking diode configurations, the model that classifies similar \\nfault types using the transformed graphical features by normal normalization strategy requires a relatively \\nlong stage of learning, and the model is not as stable as models using other strategies. This is owing to the \\nfact that the classification accuracy relies heavily on the feature learning capability of the classifiers when \\nthe discriminative ability of features is constrained. Moreover, in terms of the classification ability of models \\nwith different levels of complexity, simple models such as ANN require longer training time than CNN-based \\nmodels, and the proposed Isc-Voc normalized GADF can significantly improve the diagnosis performance \\nwhen applied to category of simple classifiers. For faults not affected by dust, the CNN-CBAM still has \\nslightly better accuracy and training performance than multilayer CNN, although their differences are not \\nsignificant due to the excellent strength of the CNN-based models in dealing with less complicated tasks. \\n \\nFig. 25. The loss and accuracy of PV arrays with blocking diodes. \\n \\nFig. 26. The loss and accuracy of PV arrays without blocking diodes. \\n4.5 Discussion \\nCompared with the most advanced research in the existing literature, our approach enables the accurate \\nidentification of dynamic fault types for arrays with multiple scenarios and configurations. Among the \\ncategories using GADF transformed graphical features as classification features, the IV-PV stacked matrix \\nby the proposed Isc-Voc normalized GADF preprocessing method outperforms the transformed feature of \\nsingle characteristic curve in [39], both in terms of classification accuracy and performance of model training. \\nThis implies that the stacked GADF graphical features can highlight the distinguishability of different fault \\nclasses in preprocessing methods without employing correction of field characteristic curves, and that the \\nGADF based on Isc-Voc normalization has a more reliable discriminatory capability. The main reasons for \\nthis are that stacked graphical features contain richer fault information than features of single curve, and that \\nthe Isc-Voc normalized GADF overcomes the challenge of inconsistent feature characterization of the same \\nfaults in different environments. In the comparative analysis of different classifiers, the proposed CNN-\\nCBAM has a certain accuracy advantage over other CNN-based methods, and is obviously superior to simple \\nmodels such as ANN, especially under dust influenced operating condition containing multiple mixed and \\nhigh-complexity dynamic faults. This is mainly because the characteristic curves jointly affected by dynamic \\nfault parameters and changing environmental factors are more complex, and the learning ability of simple \\nclassifiers is limited. Indeed, it is worth noting that input features have a greater impact on classification \\nperformance, both in terms of accuracy and robustness, than the type of classifiers. The notion has been fully \\nproven in terms of the significantly better diagnostic performance of Isc-Voc normalized GADF features \\ncompared to other GADF features using simple model ANN as the classifier. Furthermore, the benefits of the \\nproposed Isc-Voc normalized GADF are still notable and effective in the condition of not considering the \\nimpact of dust, and CNN-CBAM has slight privileges over the basic CNN models due to the general \\ncomplexity of faults in this scenario. \\nThese results prove that features are extremely important, that is, the original data converted into \\neffective features can improve the discriminatory quality of the input features. It can simplify the tuning \\nprocess of the classifier and improve the diagnostic performance. Moreover, the CNN structure with the \\nintroduction of the CBAM attention module, based on high-quality effective features, offers more significant \\nadvantages in dealing with the diagnosis of complex fault types. \\n5. Conclusion \\nIn this paper, we propose a new fault diagnosis method for identifying and evaluating faults in PV arrays, \\nwhich is able to cope with PV arrays with different blocking diode configurations under both operating \\nconditions considering and not considering the dust impact. It is summarized that the salient aspects of our \\napproach are as follows. To facilitate the extraction of the full information in the characteristic curves by the \\nclassifier, we transform both the I-V and P-V curves into a graphical feature matrix of GADF and stack them \\ntogether. Then, for reliable and easy-to-implement practical applications, which require that features of the \\nsame faults in different environments are effectively uniform, we adopt calibration-independent field \\ncharacteristic curves and normalize them by GADF using the ideal Isc and Voc under their ambient conditions. \\nFurthermore, to overcome the challenge of classifiers for multiple dynamic faults in complex operating \\nconditions, we employ the convolutional neural network embedded with CBAM modules to identify and \\nclassify the graphical features of complicated faults. \\nThe performance of the proposed technique is validated in terms of accuracy and stability in various \\nscenarios. Specifically, the Isc-Voc normalized GADF transformed graphical features have higher fault \\ndiscrimination than the features transformed by the other strategies, significantly simplifying the processing \\nof the classifier and obtaining magnificent diagnostic accuracy for both conditions with or without dust \\neffects. And it is verified that CNN-CBAM is effective for the identification of normal operating condition \\ncontaining relatively simple faults or dust operating condition containing complex faults. In general, the \\nproposed method based on GADF-transformed graphical features of characteristic curves and convolutional \\nneural network with CBAM modules is applicable to PV arrays with or without blocking diodes and is also \\neffective for fault diagnosis of different operating conditions, including concurrent faults affected by dust, \\nwhich has economic benefits. And the proposed Isc-Voc normalized GADF transformation provides a new \\nscheme for using full characteristic curve information with discarding STC correction and relieves the \\nexperimental dependence on correction factors, which greatly expands the universal application of fault \\ndiagnosis. Based on this research, there are still challenges that need to be addressed in future work, such as \\nthe further differentiation of different dynamic fault levels and the location of strings occurring faults while \\ndiagnosing, which will play an important role in studying the improvement of O&M efficiency and return-\\non-investment. \\nAcknowledgement \\nThis work is supported by the National Natural Science Foundation of China (No. 61573046) and \\nProgram for Changjiang Scholars and Innovative Research Team in University (No. IRT1203).  \\nReference \\n[1] Citaristi I. International Energy Agency—IEA. The Europa Directory of International Organizations 2022. Routledge2022. \\npp. 701-2. \\n[2] Zhao Y, De Palma J-F, Mosesian J, Lyons R, Lehman B. Line–line fault analysis and protection challenges in solar \\nphotovoltaic arrays. IEEE Transactions on Industrial Electronics 2012;60:3784-95. \\n[3] Livera A, Theristis M, Makrides G, Georghiou G E. Recent advances in failure diagnosis techniques based on performance \\ndata analysis for grid-connected photovoltaic systems. Renewable Energy 2019;133:126-43. \\n[4] Momeni H, Sadoogi N, Farrokhifar M, Gharibeh H F. Fault diagnosis in photovoltaic arrays using GBSSL method and \\nproposing a fault correction system. IEEE Transactions on Industrial Informatics 2019;16:5300-8. \\n[5] Pillai D S, Blaabjerg F, Rajasekar N. A comparative evaluation of advanced fault detection approaches for PV systems. \\nIEEE Journal of Photovoltaics 2019;9:513-27. \\n[6] Li C, Yang Y, Zhang K, Zhu C, Wei H. A fast MPPT-based anomaly detection and accurate fault diagnosis technique for PV \\narrays. Energy Conversion and Management 2021;234:113950. \\n[7] Pillai D S, Rajasekar N. A comprehensive review on protection challenges and fault diagnosis in PV systems. Renewable \\nand Sustainable Energy Reviews 2018;91:18-40. \\n[8] Dida M, Boughali S, Bechki D, Bouguettaia H. Output power loss of crystalline silicon photovoltaic modules due to dust \\naccumulation in Saharan environment. Renewable and Sustainable Energy Reviews 2020;124:109787. \\n[9] Kazem H A, Chaichan M T, Al-Waeli A H, Sopian K. A review of dust accumulation and cleaning methods for solar \\nphotovoltaic systems. Journal of Cleaner Production 2020;276:123187. \\n[10] Song Z, Liu J, Yang H. Air pollution and soiling implications for solar photovoltaic power generation: A comprehensive \\nreview. Applied Energy 2021;298:117247. \\n[11] Salamah T, Ramahi A, Alamara K, Juaidi A, Abdallah R, Abdelkareem M A, et al. Effect of dust and methods of cleaning \\non the performance of solar PV module for different climate regions: Comprehensive review. Science of The Total Environment \\n2022;154050. \\n[12] Younis A, Alhorr Y. Modeling of dust soiling effects on solar photovoltaic performance: A review. Solar Energy \\n2021;220:1074-88. \\n[13] Van Gompel J, Spina D, Develder C. Satellite based fault diagnosis of photovoltaic systems using recurrent neural networks. \\nApplied Energy 2022;305:117874. \\n[14] Araneo R, Lammens S, Grossi M, Bertone S. EMC issues in high-power grid-connected photovoltaic plants. IEEE \\nTransactions on Electromagnetic Compatibility 2009;51:639-48. \\n[15] Takashima T, Yamaguchi J, Otani K, Oozeki T, Kato K, Ishida M. Experimental studies of fault location in PV module \\nstrings. Solar Energy Materials and Solar Cells 2009;93:1079-82. \\n[16] Roy S, Alam M K, Khan F, Johnson J, Flicker J. An irradiance-independent, robust ground-fault detection scheme for PV \\narrays based on spread spectrum time-domain reflectometry (SSTDR). IEEE Transactions on Power Electronics 2017;33:7046-\\n57. \\n[17] Chen S-Q, Yang G-J, Gao W, Guo M-F. Photovoltaic fault diagnosis via semisupervised ladder network with string voltage \\nand current measures. IEEE Journal of Photovoltaics 2020;11:219-31. \\n[18] Chen Z, Han F, Wu L, Yu J, Cheng S, Lin P, et al. Random forest based intelligent fault diagnosis for PV arrays using array \\nvoltage and string currents. Energy Conversion and Management 2018;178:250-64. \\n[19] Lu X, Lin P, Cheng S, Lin Y, Chen Z, Wu L, et al. Fault diagnosis for photovoltaic array based on convolutional neural \\nnetwork and electrical time series graph. Energy Conversion and Management 2019;196:950-65. \\n[20] Abd el-Ghany H A, Elgebaly A E, Taha I B M. A new monitoring technique for fault detection and classification in PV \\nsystems based on rate of change of voltage-current trajectory. International Journal of Electrical Power & Energy Systems \\n2021;133:107248. \\n[21] Liu Y, Ding K, Zhang J, Li Y, Yang Z, Zheng W, et al. Fault diagnosis approach for photovoltaic array based on the stacked \\nauto-encoder and clustering with I-V curves. Energy Conversion and Management 2021;245:114603. \\n[22] Pan T, Chen J, Xie J, Chang Y, Zhou Z. Intelligent fault identification for industrial automation system via multi-scale \\nconvolutional generative adversarial network with partially labeled samples. ISA Trans 2020;101:379-89. \\n[23] Ishaque K, Salam Z. An improved modeling method to determine the model parameters of photovoltaic (PV) modules \\nusing differential evolution (DE). Solar Energy 2011;85:2349-59. \\n[24] Mellit A, Tina G M, Kalogirou S A. Fault detection and diagnosis methods for photovoltaic systems: A review. Renewable \\nand Sustainable Energy Reviews 2018;91:1-17. \\n[25] Triki-Lahiani A, Abdelghani A B-B, Slama-Belkhodja I. Fault detection and monitoring systems for photovoltaic \\ninstallations: A review. Renewable and Sustainable Energy Reviews 2018;82:2680-92. \\n[26] Spataru S, Sera D, Kerekes T, Teodorescu R. Monitoring and fault detection in photovoltaic systems based on inverter \\nmeasured string IV curves. 31st European Photovoltaic Solar Energy Conference and Exhibition. WIP Wirtschaft und \\nInfrastruktur GmbH & Co Planungs KG2015. pp. 1667-74. \\n[27] Chen Z, Chen Y, Wu L, Cheng S, Lin P. Deep residual network based fault detection and diagnosis of photovoltaic arrays \\nusing current-voltage curves and ambient conditions. Energy Conversion and Management 2019;198:111793. \\n[28] Gao W, Wai R-J. A novel fault identification method for photovoltaic array via convolutional neural network and residual \\ngated recurrent unit. IEEE Access 2020;8:159493-510. \\n[29] Chine W, Mellit A, Lughi V, Malek A, Sulligoi G, Pavan A M. A novel fault diagnosis technique for photovoltaic systems \\nbased on artificial neural networks. Renewable Energy 2016;90:501-12. \\n[30] Fadhel S, Delpha C, Diallo D, Bahri I, Migan A, Trabelsi M, et al. PV shading fault detection and classification based on \\nIV curve using principal component analysis: Application to isolated PV system. Solar Energy 2019;179:1-10. \\n[31] Liu Y, Ding K, Zhang J, Lin Y, Yang Z, Chen X, et al. Intelligent fault diagnosis of photovoltaic array based on variable \\npredictive models and I–V curves. Solar Energy 2022;237:340-51. \\n[32] Singh R, Sharma M, Rawat R, Banerjee C. An assessment of series resistance estimation techniques for different silicon \\nbased SPV modules. Renewable and Sustainable Energy Reviews 2018;98:199-216. \\n[33] Li Y, Ding K, Zhang J, Chen F, Chen X, Wu J. A fault diagnosis method for photovoltaic arrays based on fault parameters \\nidentification. Renewable Energy 2019;143:52-63. \\n[34] Chen Z, Wu L, Cheng S, Lin P, Wu Y, Lin W. Intelligent fault diagnosis of photovoltaic arrays based on optimized kernel \\nextreme learning machine and I-V characteristics. Applied Energy 2017;204:912-31. \\n[35] Bressan M, El Basri Y, Galeano A G, Alonso C. A shadow fault detection method based on the standard error analysis of \\nIV curves. Renewable Energy 2016;99:1181-90. \\n[36] Ma M, Zhang Z, Xie Z, Yun P, Zhang X, Li F. Fault diagnosis of cracks in crystalline silicon photovoltaic modules through \\nIV curve. Microelectronics Reliability 2020;114:113848. \\n[37] Lin P, Qian Z, Lu X, Lin Y, Lai Y, Cheng S, et al. Compound fault diagnosis model for Photovoltaic array using multi-\\nscale SE-ResNet. Sustainable Energy Technologies and Assessments 2022;50:101785. \\n[38] Huang J-M, Wai R-J, Yang G-J. Design of hybrid artificial bee colony algorithm and semi-supervised extreme learning \\nmachine for PV fault diagnoses by considering dust impact. IEEE Transactions on Power Electronics 2019;35:7086-99. \\n[39] Li B, Delpha C, Migan-Dubois A, Diallo D. Fault diagnosis of photovoltaic panels using full I–V characteristics and \\nmachine learning techniques. Energy Conversion and Management 2021;248:114785. \\n[40] Schill C, Brachmann S, Koehl M. Impact of soiling on IV-curves and efficiency of PV-modules. Solar Energy \\n2015;112:259-62. \\n[41] Chanchangi Y N, Ghosh A, Sundaram S, Mallick T K. Dust and PV Performance in Nigeria: A review. Renewable and \\nSustainable Energy Reviews 2020;121:109704. \\n[42] Silvestre S, Boronat A, Chouder A. Study of bypass diodes configuration on PV modules. Applied Energy 2009;86:1632-\\n40. \\n[43] Heinrich M, Meunier S, Same A, Queval L, Darga A, Oukhellou L, et al. Detection of cleaning interventions on \\nphotovoltaic modules with machine learning. Applied Energy 2020;263:114642. \\n[44] Al-Addous M, Dalala Z, Alawneh F, Class C B. Modeling and quantifying dust accumulation impact on PV module \\nperformance. Solar Energy 2019;194:86-102. \\n[45] Hachicha A A, Al-Sawafta I, Said Z. Impact of dust on the performance of solar photovoltaic (PV) systems under United \\nArab Emirates weather conditions. Renewable Energy 2019;141:287-97. \\n[46] Soon J J, Low K-S. Photovoltaic model identification using particle swarm optimization with inverse barrier constraint. \\nIEEE Transactions on Power Electronics 2012;27:3975-83. \\n[47] Bastidas-Rodríguez J D, Franco E, Petrone G, Ramos-Paja C A, Spagnuolo G. Model-based degradation analysis of \\nphotovoltaic modules through series resistance estimation. IEEE Transactions on Industrial Electronics 2015;62:7256-65. \\n[48] Commission I E. IEC 60891, Photovoltaic Devices. Procedures for Temperature and Irradiance Corrections to Measure \\nIV Characteristics. International Electrotechnical Commission: Geneva, Switzerland 2007. \\n[49] Li B, Migan-Dubois A, Delpha C, Diallo D. Evaluation and improvement of IEC 60891 correction methods for I-V curves \\nof defective photovoltaic panels. Solar Energy 2021;216:225-37. \\n[50] Bauomy M F, Gamal H, Shaltout A A. Solar PV DC nanogrid dynamic modeling applying the polynomial computational \\nmethod for MPPT. Advances in Clean Energy Technologies. Elsevier2021. pp. 19-87. \\n[51] Wang Z, Oates T. Imaging time-series to improve classification and imputation. Twenty-Fourth International Joint \\nConference on Artificial Intelligence2015. \\n[52] Villalva M G, Gazoli J R, Ruppert Filho E. Comprehensive approach to modeling and simulation of photovoltaic arrays. \\nIEEE Transactions on Power Electronics 2009;24:1198-208. \\n[53] Villalva M G, Gazoli J R, Ruppert Filho E. Modeling and circuit-based simulation of photovoltaic arrays. 2009 Brazilian \\nPower Electronics Conference. IEEE2009. pp. 1244-54. \\n[54] De Soto W, Klein S A, Beckman W A. Improvement and validation of a model for photovoltaic array performance. Solar \\nEnergy 2006;80:78-88. \\n[55] Alzubaidi L, Zhang J, Humaidi A J, Al-Dujaili A, Duan Y, Al-Shamma O, et al. Review of deep learning: Concepts, CNN \\narchitectures, challenges, applications, future directions. Journal of Big Data 2021;8:1-74. \\n[56] Woo S, Park J, Lee J-Y, Kweon I S. Cbam: Convolutional block attention module. Proceedings of the European conference \\non computer vision (ECCV)2018. pp. 3-19. \\n[57] Wang Y, Li J. Credible intervals for precision and recall based on a K-fold cross-validated beta distribution. Neural \\nComputation 2016;28:1694-722. \\n[58] Hossin M, Sulaiman M N. A review on evaluation metrics for data classification evaluations. International journal of data \\nmining & knowledge management process 2015;5:1. \\n \\n\", metadata={'Published': '2023-03-24', 'Title': 'Fault diagnosis for PV arrays considering dust impact based on transformed graphical feature of characteristic curves and convolutional neural network with CBAM modules', 'Authors': 'Jiaqi Qu, Lu Wei, Qiang Sun, Hamidreza Zareipour, Zheng Qian', 'Summary': 'Various faults can occur during the operation of PV arrays, and both the\\ndust-affected operating conditions and various diode configurations make the\\nfaults more complicated. However, current methods for fault diagnosis based on\\nI-V characteristic curves only utilize partial feature information and often\\nrely on calibrating the field characteristic curves to standard test conditions\\n(STC). It is difficult to apply it in practice and to accurately identify\\nmultiple complex faults with similarities in different blocking diodes\\nconfigurations of PV arrays under the influence of dust. Therefore, a novel\\nfault diagnosis method for PV arrays considering dust impact is proposed. In\\nthe preprocessing stage, the Isc-Voc normalized Gramian angular difference\\nfield (GADF) method is presented, which normalizes and transforms the resampled\\nPV array characteristic curves from the field including I-V and P-V to obtain\\nthe transformed graphical feature matrices. Then, in the fault diagnosis stage,\\nthe model of convolutional neural network (CNN) with convolutional block\\nattention modules (CBAM) is designed to extract fault differentiation\\ninformation from the transformed graphical matrices containing full feature\\ninformation and to classify faults. And different graphical feature\\ntransformation methods are compared through simulation cases, and different\\nCNN-based classification methods are also analyzed. The results indicate that\\nthe developed method for PV arrays with different blocking diodes\\nconfigurations under various operating conditions has high fault diagnosis\\naccuracy and reliability.'}), Document(page_content='FREQUENCY AND TEMPORAL CONVOLUTIONAL ATTENTION FOR\\nTEXT-INDEPENDENT SPEAKER RECOGNITION\\nSarthak Yadav, Atul Rai\\nStaqu Technologies, India\\nABSTRACT\\nMajority of the recent approaches for text-independent speaker\\nrecognition apply attention or similar techniques for aggre-\\ngation of frame-level feature descriptors generated by a deep\\nneural network (DNN) front-end. In this paper, we propose\\nmethods of convolutional attention for independently mod-\\nelling temporal and frequency information in a convolutional\\nneural network (CNN) based front-end. Our system utilizes\\nconvolutional block attention modules (CBAMs) [1] appro-\\npriately modiﬁed to accommodate spectrogram inputs. The\\nproposed CNN front-end ﬁtted with the proposed convo-\\nlutional attention modules outperform the no-attention and\\nspatial-CBAM baselines by a signiﬁcant margin on the Vox-\\nCeleb [2, 3] speaker veriﬁcation benchmark. Our best model\\nachieves an equal error rate of 2.031% on the VoxCeleb1 test\\nset, which is a considerable improvement over comparable\\nstate of the art results. For a more thorough assessment of\\nthe effects of frequency and temporal attention in real-world\\nconditions, we conduct ablation experiments by randomly\\ndropping frequency bins and temporal frames from the input\\nspectrograms, concluding that instead of modelling either\\nof the entities, simultaneously modelling temporal and fre-\\nquency attention translates to better real-world performance.\\nIndex Terms— convolutional attention, speaker veriﬁca-\\ntion, speaker recognition, CNNs, deep learning\\n1. INTRODUCTION\\nMajority of the recent strides in the ﬁeld of text-independent\\nspeaker recognition can be ascribed to deep neural net-\\nwork (DNN) based speaker embeddings, which have far\\nsurpassed conventional state-of-the-art systems such as the\\ni-vector+PLDA framework.\\nEnd-to-end deep learning-based speaker recognition sys-\\ntems usually comprise of two components: (i) DNN front-end\\nfor extraction of frame-level features; and (ii) temporal aggre-\\ngation of these frame-level features to an utterance-level em-\\nbedding. Majority of the recent works utilize convolutional\\nneural network (CNN) based front-end models for extracting\\nframe-level feature descriptors from spectrogram inputs.\\nThis research is funded by Staqu Technologies, India.\\nAlthough sub-optimal since it does not differentiate be-\\ntween frames on the basis of content, temporal averaging is\\namongst the most frequently used techniques for aggregation\\nof frame-level features [2, 3, 4]. A number of recent works\\nhave proposed the use of statistical or dictionary based meth-\\nods for aggregation to mitigate this problem. [5] proposed\\nthe statistics pooling layer, which combines mean and stan-\\ndard deviation statistics for weighted aggregation of tempo-\\nral frames. More recently, [6] proposed time-distributed vot-\\ning (TDV) for aggregating features extracted by their UtterId-\\nNet front-end in short segment speaker veriﬁcation, especially\\nsub-second durations. [7] proposed the usage of dictionary-\\nbased NetVLAD or GhostVLAD [8] for aggregating temporal\\nfeatures, using a 34-layer ResNet based front-end for feature\\nextraction. Numerous recent works [9, 10, 11, 12] have pro-\\nposed attention based techniques for aggregation of frame-\\nlevel feature descriptors, to assign greater importance to the\\nmore discriminative frames.\\nA prominent attention mechanism in the domain of com-\\nputer vision is convolutional attention [1, 13], which facili-\\ntates modelling of spatial and channel attention throughout\\nthe entire CNN feature extraction network. In this paper, we\\npropose methods of convolutional attention based on Con-\\nvolutional block attention module (CBAM) [1] for speaker\\nveriﬁcation. The main contributions of this work are two-\\nfold: (i) We propose convolutional attention modules based\\non CBAM for modelling frequency and temporal attention,\\nviz. f-CBAM and t-CBAM, along with an equal-weighted\\ncomposite module for capturing both frequency and tempo-\\nral attention, called ft-CBAM; and, (ii) We conduct ablation\\nexperiments for a more thorough assessment of the proposed\\nattention modules as well as their performances under real-\\nworld conditions, concluding that instead of modelling either\\nof the entities, simultaneously modelling temporal and fre-\\nquency attention translates to better real-world performance.\\n2. RELATED WORKS\\nAttention mechanisms have led to signiﬁcant advances across\\ncomputer vision, spoken language understanding and natu-\\nral language processing, increasing the modelling capacity of\\ndeep neural networks by concentrating on crucial features and\\nsuppressing unimportant ones. For speaker recognition, [9,\\narXiv:1910.07364v2  [cs.SD]  19 Oct 2019\\n10] utilize self-attention for aggregating frame-level features.\\n[11] combined attention mechanism with statistics pooling [5]\\nto propose attentive-statistics pooling. Most recently, [12]\\nemploy the idea of multi-head attention [14] for feature ag-\\ngregation, outperforming an I-vector+PLDA baseline by 58%\\n(relative). However, by applying attention or similar tech-\\nniques only on the feature descriptors generated by the DNN\\nfront-end and not throughout the front-end model, majority\\nof the recent works are (i) not fully utilising the representa-\\ntion power of DNN front-end models; and (ii) implicitly mod-\\nelling temporal attention alone in the process. As opposed to\\nthe methods mentioned above, the proposed modules apply\\nattention in the feature extraction module, innately improving\\nthe representation capabilities of the model.\\nRecently, [15] proposed the usage of Gated Convolutional\\nNeural Networks (GCNN) for speaker recognition. Matched\\nwith a gated-attention pooling method for frame-level feature\\naggregation, they evaluate the performance of GCNN in an\\nx-vector [16] system on SRE16 and SRE18 datasets. In com-\\nparison, we propose add-on modules that explicitly model\\nfrequency and temporal attention. [17] proposed an encoder-\\ndecoder style attention module similar to [13], for extracting\\nspatial and channel attention for automatic speech recognition\\nin noisy conditions. In contrast, we propose convolutional at-\\ntention modules based on [1] that model frequency and tem-\\nporal attention along with channel attention, which drastically\\noutperform spatial attention baseline for speaker veriﬁcation.\\n2.1. CBAM: A brief overview\\nRecently, [1] proposed a new network module, named ”Con-\\nvolutional block attention module” (CBAM), which sequen-\\ntially applies channel attention and spatial attention submod-\\nules on the input feature maps.\\nCBAM comprises of two components, viz. the channel at-\\ntention module and the spatial attention module. The follow-\\ning equations can be used to summarize the overall attention\\nprocess:\\nF\\n′ = Mc(F)\\nO\\nF\\n(1)\\nF\\n′′ = Ms(F\\n′)\\nO\\nF\\n′,\\n(2)\\nwhere N denotes element-wise multiplication, F is the in-\\nput feature map, F\\n′′ is the ﬁnal output of the CBAM mod-\\nule, and Mc and Ms denote the channel and spatial attention\\noperations, respectively. The channel attention module ex-\\nploits inter-channel relationship of features and generates a\\n1-D channel attention map by squeezing the spatial dimen-\\nsions of the input feature map by max-pooling and average\\npooling, followed by projection using a shared MLP layer.\\nThe spatial attention module utilizes the inter-spatial relation-\\nship of features, focusing on the spatial location of objects\\nof interest. It applies and concatenates outputs of average-\\npooling and max-pooling operations along the channel axis\\nwhich generates an efﬁcient feature descriptor, followed by a\\n7x7 convolution layer.\\nHowever, unlike computer vision where the modality rep-\\nresents highly correlated points in space and the axes repre-\\nsent the spatial location of an object in a cartesian coordinate\\nsystem, the axes of a spectrogram represent entirely differ-\\nent domains: frequency and time. This disconnect between\\nthe entities represented by the axes of the feature space of\\nthe two modalities necessitates the need for targeted convolu-\\ntional attention modules since preconditions required by ex-\\nisting methods of convolutional attention to effectively model\\nattention in the speech domain might no longer apply.\\n3. PROPOSED APPROACH\\nThe channel attention module (Eq.1) extracts general infor-\\nmation regarding channel importance in the input feature\\nmap, and is used as is. We propose appropriate changes to\\nthe spatial attention submodule for modelling frequency and\\ntemporal attention, viz. f-CBAM and t-CBAM, respectively,\\nfor spectrogram inputs.\\nHence, the input to our proposed modules is F\\n′ (Eq. 1),\\nsuch that F\\n′ ∈RC×H×T where C denotes the number of\\ninput channels, and H and T denote the dimensions along the\\nfrequency and temporal axes, respectively.\\n3.1. f-CBAM\\nFor modelling frequency attention, we need to limit the recep-\\ntive ﬁeld of the attention module to focus only on the y-axis\\nof the input.\\nFig. 1. Proposed f-CBAM Module. z-axis represents tempo-\\nral axis (pictorially represented using dimensions > 1).\\nWe aggregate temporal information averaging the input\\nfeature map F\\n′ along the x-axis to generate an efﬁcient fea-\\nture descriptor Ffreq ∈RC×H×1 which essentially assigns\\nequal statistical importance to each temporal frame.\\nFfreq = AvgPool1×T (F\\n′)\\n(3)\\nwhere AvgPool1×t represents the average pooling operation\\nwith a kernel of size 1 × T over the input feature map.\\nSimilar to spatial attention submodule, we then aggre-\\ngate channel information by generating two feature maps:\\nF f\\navg, F f\\nmax ∈R1×H×1, denoting average and max pooling\\noperations applied across the channel dimension on Ffreq,\\nand concatenate them. Finally, on this concatenated feature\\ndescriptor, we apply a rectangular 7x1 convolution kernel to\\ngenerate a frequency attention map Mfreq(F\\n′) ∈RH×1,\\nwhere H denotes total number of frequency bins in the input\\nfeature F\\n′.\\nMfreq(F\\n′) = σ(f 7×1([F f\\navg; F f\\nmax])\\n(4)\\nHere, σ denotes the sigmoid function and f 7×1 represents\\na convolution operation with a rectangular 7 × 1 kernel.\\nMfreq(F\\n′) is then broadcasted along the temporal dimension\\non the original input feature map F\\n′.\\n3.2. t-CBAM\\nt-CBAM follows a procedure similar to f-CBAM for mod-\\nelling temporal attention, albeit limiting the receptive ﬁeld of\\nthe attention module to the temporal-axis, i.e. the x-axis.\\nFtemp = AvgPoolH×1(F\\n′)\\n(5)\\nMtemp(F\\n′) = σ(f 1×7([F t\\navg; F t\\nmax])\\n(6)\\nwhere Ftemp ∈RC×H×1; and F t\\navg, F t\\nmax ∈R1×1×T .\\n3.3. ft-CBAM\\nft-CBAM comprises of f-CBAM and t-CBAM applied in par-\\nallel on the input feature map. The feature maps generated by\\nthe two are then averaged. ft-CBAM can be seen as a special\\ncase of the original spatial CBAM, with the 7×7 convolution\\nﬁlter of the latter represented by two independent 7 × 1 and\\n1 × 7 operations.\\n3.4. Proposed Pipeline\\nCNN Front-end:\\nWe propose a modiﬁed 50-layer Pre-\\nActivation ResNet [18], henceforth denoted as PRN-50v2,\\nas our CNN front-end to encode spectrogram input of arbi-\\ntrary length (Table 1). By changing the order of layers in\\nthe residual block to BN-ReLU-Conv, pre-activation ResNets\\nimprove the ease of optimization as well as the generalization\\nperformance over comparable ResNet [19] counterparts.\\nAttention:\\nWherever applicable, the appropriate CBAM\\nmodule is integrated at the end of each residual block in the\\nproposed front-end module.\\nFeature Aggregation: Following [7], where they demon-\\nstrate the inadequacies of temporal averaging, GhostVLAD\\n[8] pooling layer is applied after the CNN front-end. For ref-\\nerence, experimental results using temporal average pooling\\nare also provided. A 256-dimensional fully-connected em-\\nbedding layer is applied after the GhostVLAD pooling layer,\\nyielding a compact utterance-level feature descriptor. Finally,\\nInput Spectrogram (1 × 161 × T)\\nOutput size\\nconv, 7x7, 64, stride (2,1)\\n64 × 80 × T\\nmaxpool, 2x2\\n64 × 40 × T/2\\n\"\\nconv, 1x1, 32\\nconv, 3x3, 32\\nconv, 1x1, 64\\n#\\n× 3\\n64 × 40 × T/2\\n\"\\nconv, 1x1, 64\\nconv, 3x3, 64\\nconv, 1x1, 128\\n#\\n× 4\\n128 × 20 × T/4\\n\"\\nconv, 1x1, 128\\nconv, 3x3, 128\\nconv, 1x1, 256\\n#\\n× 6\\n256 × 10 × T/8\\n\"\\nconv, 1x1, 256\\nconv, 3x3, 256\\nconv, 1x1, 512\\n#\\n× 3\\n512 × 5 × T/16\\nconv, 5x1, 256\\n512 × 1 × T/16\\nTable 1. The modiﬁed PreActResNet front-end. ReLU and\\nBatchNorm layers are omitted. Each row depicts the ﬁlter\\nsizes, # of ﬁlters and the corresponding output sizes. Com-\\npared to the standard PreActResNet-50 with around 25 M pa-\\nrameters, the proposed model has 4.7 M.\\nthe last fully-connected layer with softmax output for training\\nthe model in an end-to-end classiﬁcation setting, using the\\nArcSoftmax [20] optimization function.\\n4. EXPERIMENTS AND RESULTS\\n4.1. Benchmark dataset and training details\\nWe use VoxCeleb datasets for evaluation of the proposed ap-\\nproach, training our models on the VoxCeleb2 ‘dev’ set [3]\\nwhich comprises of 5, 994 speakers and test on the VoxCeleb1\\n[2] veriﬁcation test set [3].\\nTraining Details: For training, spectrograms are gen-\\nerated using a hamming window 20 ms wide with a hop\\nlength of 10 ms, and a 320-point FFT corresponding to a\\nrandom 2-second temporal crop per utterance, followed by\\nper-frequency-bin mean and variance normalization. Stochas-\\ntic gradient descent optimizer with an initial learning rate of\\n0.01 decayed every 15 epochs by a factor of 0.1 is used for\\ntraining.\\n4.2. Experiments\\nUsing the proposed model with no attention along with results\\nfrom previous works that follow similar benchmark protocol\\nas baselines, we ﬁrst perform a direct comparative analysis\\nto study the effect of attention on speaker veriﬁcation perfor-\\nmance.\\nFurther, for a more thorough assessment of the proposed\\nattention modules and to imitate real-world conditions where\\nsimilar perturbations might occur, we conduct three ablation\\nexperiments: (i) random frequency masking; (ii) random\\nFront-end Model\\nFront-end Attention\\nDims\\nAggregation\\nTraining Set\\nEER (%)\\nNagrani et al.[2]\\nI-vectors + PLDA\\n-\\n-\\n-\\nVoxCeleb1\\n8.8\\nNagrani et al.[2]\\nVGG-M\\n-\\n1024\\nTAP\\nVoxCeleb1\\n10.2\\nCai et al.[21]\\nResNet-34\\n-\\n128\\nSAP\\nVoxCeleb1\\n4.40\\nOkabe et al.[11]\\nX-vector\\n-\\n1500\\nASP\\nVoxCeleb1\\n3.85\\nHajibabei et al.[22]\\nResNet-29\\n-\\n128\\nTAP\\nVoxCeleb1\\n4.30\\nIndia et al.[12]\\nCNN\\n-\\n-\\nMHA\\nVoxCeleb1\\n4\\nChung et al.[3]\\nResNet-50\\n-\\n512\\nTAP\\nVoxCeleb2\\n4.19\\nXie et al.[7]\\nThin ResNet-34\\n-\\n512\\nGhostVLAD\\nVoxCeleb2\\n3.22\\nHajavi et al.[6]\\nUtterIdNet\\n-\\n512\\nTDV\\nVoxCeleb2\\n4.26\\nProposed\\nPRN-50v2\\n-\\n256\\nTAP\\nVoxCeleb2\\n2.557\\nProposed\\nPRN-50v2\\nSpatial CBAM\\n256\\nTAP\\nVoxCeleb2\\n2.515\\nProposed\\nPRN-50v2\\nf-CBAM\\n256\\nTAP\\nVoxCeleb2\\n2.457\\nProposed\\nPRN-50v2\\nt-CBAM\\n256\\nTAP\\nVoxCeleb2\\n2.28\\nProposed\\nPRN-50v2\\nft-CBAM\\n256\\nTAP\\nVoxCeleb2\\n2.194\\nProposed\\nPRN-50v2\\n-\\n256\\nGhostVLAD\\nVoxCeleb2\\n2.4\\nProposed\\nPRN-50v2\\nSpatial CBAM\\n256\\nGhostVLAD\\nVoxCeleb2\\n2.404\\nProposed\\nPRN-50v2\\nf-CBAM\\n256\\nGhostVLAD\\nVoxCeleb2\\n2.13\\nProposed\\nPRN-50v2\\nt-CBAM\\n256\\nGhostVLAD\\nVoxCeleb2\\n2.17\\nProposed\\nPRN-50v2\\nft-CBAM\\n256\\nGhostVLAD\\nVoxCeleb2\\n2.031\\nTable 2. Veriﬁcation results on the VoxCeleb1 test set. TAP: Temporal Average Pooling, SAP: Self-Attentive Pooling, ASP: At-\\ntentive Statistics Pooling, MHA: Multi-Head Attention, TDV: Time-distributed Voting. All of the proposed models outperform\\nexisting baselines by a signiﬁcant margin.\\ntemporal masking; and (iii) random frequency and temporal\\nmasking. Every input spectrogram has a 40% probability of\\nbeing augmented, with up to two mask instances per input.\\nPer mask instance, up to 30 randomly selected frequency bins\\nand up to 40 randomly selected timesteps are masked.\\n4.3. Results\\nTable. 2 compares the performance of the proposed mod-\\nels with existing benchmarks on the VoxCeleb1 test set. All\\nthe proposed models outperform previous results by a signif-\\nicant margin, with the best ft-CBAM based model achieving\\nan EER of 2.031%. As evidenced by [7], using GhostVLAD\\ninstead of TAP improves performance all across the board. t-\\nCBAM variants already model temporal attention and there-\\nfore experience the smallest gains, whereas f-CBAM variants\\nexperience the most drastic improvement (EER of 2.457 % vs\\n2.13%).\\nThe spatial CBAM variants perform on-par with the no-\\nattention variants of the proposed PRN-50v2 model.\\nThe\\nlarge disparity in performance between spatial CBAM and ft-\\nCBAM can be attributed to the differences in receptive ﬁelds:\\nunlike ft-CBAM, the receptive ﬁeld of spatial CBAM’s sin-\\ngle square 7x7 kernel will essentially span across different\\nentities in the feature space for spectrogram inputs.\\nTable. 3 shows the results of the ablation experiments. ft-\\nCBAM outperforms all other variants by a signiﬁcant margin\\nin all conditions. The performance gap between speciﬁc at-\\ntention variants depends on the kind of deformation applied:\\nthe difference between f-CBAM and t-CBAM grows from\\nAttention Variant\\nTemporal\\nFrequency\\nFreq+Temp\\nNone\\n2.43%\\n3.50%\\n3.54%\\nSpatial\\n2.43%\\n3.36%\\n3.41%\\nf-CBAM\\n2.15%\\n3.16%\\n3.16%\\nt-CBAM\\n2.20%\\n3.27%\\n3.30%\\nft-CBAM\\n2.05%\\n3.01%\\n3.05%\\nTable 3. Ablation experiment results on the VoxCeleb1 test\\nset (EER%). Every experiment is repeated 5 times and mean\\nvalues are reported.\\nOnly GhostVLAD aggregation based\\nmodels are used.\\n0.05% (temporal masking) to 0.11% (frequency masking).\\nCollectively, results from tables 2 and 3 suggest that simul-\\ntaneous modelling of temporal and frequency importance im-\\nproves speaker veriﬁcation performance.\\n5. CONCLUSION\\nIn this paper, we propose methods of convolutional atten-\\ntion for speaker recognition, viz. f-CBAM and t-CBAM for\\nmodelling frequency and temporal attention, along with a\\ncomposite module that models both simultaneously, aptly\\nnamed ft-CBAM. The proposed PRN-50v2 model equipped\\nwith ft-CBAM and GhostVLAD [8] signiﬁcantly outperforms\\nall baselines, achieving an EER of 2.03% on the VoxCeleb1\\ntest set. Empirical evidence suggests that modelling attention\\nin the DNN front-end, as well as simultaneously modelling\\ntemporal and frequency attention, improves speaker veriﬁca-\\ntion performance.\\n6. REFERENCES\\n[1] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and\\nIn So Kweon, “Cbam: Convolutional block attention\\nmodule,” in Proceedings of the European Conference\\non Computer Vision (ECCV), 2018, pp. 3–19.\\n[2] Arsha Nagrani, Joon Son Chung, and Andrew Zisser-\\nman,\\n“Voxceleb: A large-scale speaker identiﬁcation\\ndataset,” Proc. Interspeech 2017, pp. 2616–2620, 2017.\\n[3] Joon Son Chung, Arsha Nagrani, and Andrew Zisser-\\nman,\\n“Voxceleb2: Deep speaker recognition,”\\nProc.\\nInterspeech 2018, pp. 1086–1090, 2018.\\n[4] Sarthak Yadav and Atul Rai, “Learning discriminative\\nfeatures for speaker identiﬁcation and veriﬁcation,” in\\nProc. Interspeech 2018, 2018, pp. 2237–2241.\\n[5] David Snyder, Daniel Garcia-Romero, Daniel Povey,\\nand Sanjeev Khudanpur, “Deep neural network embed-\\ndings for text-independent speaker veriﬁcation,” Proc.\\nInterspeech 2017, pp. 999–1003, 2017.\\n[6] Amirhossein Hajavi and Ali Etemad, “A deep neural\\nnetwork for short-segment speaker recognition,” Proc.\\nInterspeech 2019, pp. 2878–2882, 2019.\\n[7] Weidi Xie, Arsha Nagrani, Joon Son Chung, and\\nAndrew Zisserman,\\n“Utterance-level aggregation for\\nspeaker recognition in the wild,” in ICASSP 2019-2019\\nIEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP). IEEE, 2019, pp.\\n5791–5795.\\n[8] Yujie Zhong, Relja Arandjelovi´\\nc, and Andrew Zisser-\\nman,\\n“Ghostvlad for set-based face recognition,”\\nin\\nAsian Conference on Computer Vision. Springer, 2018,\\npp. 35–50.\\n[9] Gautam Bhattacharya, Jahangir Alam, and Patrick\\nKenny, “Deep speaker embeddings for short-duration\\nspeaker veriﬁcation,” Proc. Interspeech 2017, pp. 1517–\\n1521, 2017.\\n[10] Yingke Zhu, Tom Ko, David Snyder, Brian Mak, and\\nDaniel Povey, “Self-attentive speaker embeddings for\\ntext-independent speaker veriﬁcation,” in Proc. Inter-\\nspeech 2018, 2018, pp. 3573–3577.\\n[11] Koji Okabe, Takafumi Koshinaka, and Koichi Shinoda,\\n“Attentive statistics pooling for deep speaker embed-\\nding,” Proc. Interspeech 2018, pp. 2252–2256, 2018.\\n[12] Miquel India, Pooyan Safari, and Javier Hernando, “Self\\nMulti-Head Attention for Speaker Recognition,”\\nin\\nProc. Interspeech 2019, 2019, pp. 4305–4309.\\n[13] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang,\\nCheng Li, Honggang Zhang, Xiaogang Wang, and Xi-\\naoou Tang, “Residual attention network for image clas-\\nsiﬁcation,” in Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, 2017, pp.\\n3156–3164.\\n[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\\nand Illia Polosukhin, “Attention is all you need,” in Ad-\\nvances in neural information processing systems, 2017,\\npp. 5998–6008.\\n[15] Lanhua You, Wu Guo, Li-Rong Dai, and Jun Du, “Deep\\nNeural Network Embeddings with Gating Mechanisms\\nfor Text-Independent Speaker Veriﬁcation,” in Proc. In-\\nterspeech 2019, 2019, pp. 1168–1172.\\n[16] David Snyder, Daniel Garcia-Romero, Gregory Sell,\\nDaniel Povey, and Sanjeev Khudanpur, “X-vectors: Ro-\\nbust dnn embeddings for speaker recognition,” in 2018\\nIEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP). IEEE, 2018, pp.\\n5329–5333.\\n[17] Sirui Xu and Eric Fosler-Lussier, “Spatial and channel\\nattention based convolutional neural networks for mod-\\neling noisy speech,”\\nin ICASSP 2019-2019 IEEE In-\\nternational Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP). IEEE, 2019, pp. 6625–6629.\\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\\nSun,\\n“Identity mappings in deep residual networks,”\\nin European conference on computer vision. Springer,\\n2016, pp. 630–645.\\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\\nSun, “Deep residual learning for image recognition,” in\\nProceedings of the IEEE conference on computer vision\\nand pattern recognition, 2016, pp. 770–778.\\n[20] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou, “Arcface: Additive angular margin loss for\\ndeep face recognition,” in Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition,\\n2019, pp. 4690–4699.\\n[21] Weicheng Cai, Jinkun Chen, and Ming Li,\\n“Explor-\\ning the encoding layer and loss function in end-to-end\\nspeaker and language recognition system,”\\nin Proc.\\nOdyssey 2018 The Speaker and Language Recognition\\nWorkshop, 2018, pp. 74–81.\\n[22] Mahdi Hajibabaei and Dengxin Dai,\\n“Uniﬁed hy-\\npersphere embedding for speaker recognition,”\\narXiv\\npreprint arXiv:1807.08312, 2018.\\n', metadata={'Published': '2019-10-19', 'Title': 'Frequency and temporal convolutional attention for text-independent speaker recognition', 'Authors': 'Sarthak Yadav, Atul Rai', 'Summary': 'Majority of the recent approaches for text-independent speaker recognition\\napply attention or similar techniques for aggregation of frame-level feature\\ndescriptors generated by a deep neural network (DNN) front-end. In this paper,\\nwe propose methods of convolutional attention for independently modelling\\ntemporal and frequency information in a convolutional neural network (CNN)\\nbased front-end. Our system utilizes convolutional block attention modules\\n(CBAMs) [1] appropriately modified to accommodate spectrogram inputs. The\\nproposed CNN front-end fitted with the proposed convolutional attention modules\\noutperform the no-attention and spatial-CBAM baselines by a significant margin\\non the VoxCeleb [2, 3] speaker verification benchmark, and our best model\\nachieves an equal error rate of 2:031% on the VoxCeleb1 test set, improving the\\nexisting state of the art result by a significant margin. For a more thorough\\nassessment of the effects of frequency and temporal attention in real-world\\nconditions, we conduct ablation experiments by randomly dropping frequency bins\\nand temporal frames from the input spectrograms, concluding that instead of\\nmodelling either of the entities, simultaneously modelling temporal and\\nfrequency attention translates to better real-world performance.'})]\n",
      "<class 'str'>\n",
      "methods Squeeze and Excitation (SE) block and Convolutional Block Attention Module (CBAM). Experimental results on different\n",
      "datasets and model architectures show that learning to ignore, i.e., implicit attention, yields superior performance compared to the\n",
      "standard approaches.\n",
      "Index Terms—Computer vision, CNNs, attention mechanisms, CBAM, SE\n",
      "!\n",
      "1\n",
      "INTRODUCTION\n",
      "I\n",
      "NSPIRED by the properties of the human visual system,\n",
      "attention mechanisms have been recently applied in the\n",
      "ﬁeld of deep learning, resulting in improved performance\n",
      "of the existing models across multiple applications. In the\n",
      "context of computer vision, learning to attend, i.e., learning\n",
      "to highlight and emphasize relevant attributes of images,\n",
      "have led to development of novel approaches [1], [2] in\n",
      "Convolutional Neural Networks (CNNs), improving their\n",
      "capabilities in many tasks [3], [4], [5].\n",
      "Related to the concept of attention, recent studies in neu-\n",
      "roscience suggest that the ability of humans to successfully\n",
      "perform visual tasks is related to the ability to ignore and\n",
      "suppress distractive information [6], [7], [8]. For example,\n",
      "the authors of [7] show that differences in visual working\n",
      "memory capacity, i.e., ability to remember visual features\n",
      "of multiple objects, are speciﬁcally related to distractor-\n",
      "suppression activity in visual cortex. This idea is reinforced\n",
      "in [8], where the authors provide evidence on an inhibitory\n",
      "mechanism of suppression of salient distractors aimed at\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"Ask a research question!\")\n",
    "\n",
    "# Create multiple search queries\n",
    "search_split_prompt = f\"\"\"\n",
    "Your role is that of a researcher attempting to answer a question. Given a question from the user,\n",
    "your job is to come up with one of more ArXiv queries that searches for the exact information needed to answer the question.\n",
    "A single question may need multiple queries. If there are multiple queries, make sure they are separated by \"|\"\n",
    "\n",
    "You can include all syntax that involves including multiple terms, search by abstract, title, etc.\n",
    "\n",
    "Example 1:\n",
    "Given question: What are the ethical concerns associated with the use of facial recognition technology?\n",
    "Your Answer: (\"facial recognition technology\" OR \"facial recognition systems\" OR \"facial recognition software\") AND (\"ethical concerns\" OR \"ethical implications\" OR \"ethical issues\")\n",
    "\n",
    "Example 2:\n",
    "Given question: What are some prominent attention mechanisms for convolutional neural networks, and how are they used in the autonomous vehicle industry?\n",
    "Your Answer: all:\"attention mechanisms\" AND (\"convolutional neural networks\" OR \"CNN\") | all:\"attention mechanisms\" AND (\"autonomous vehicles\" OR \"self-driving cars\")\n",
    "\n",
    "Question: {user_query},\n",
    "\n",
    "As shown above, your response should be formatted as follows:\n",
    "\n",
    "ArXiv Query 1 | ArXiv Query 2 | ArXiv Query 3 | ...\n",
    "\n",
    "\"\"\"\n",
    "response = completion(\n",
    "    api_key=apikey,\n",
    "    base_url=\"https://drchat.xyz\",\n",
    "    model = \"gpt4-1106-preview\",\n",
    "    custom_llm_provider=\"openai\",\n",
    "    messages = [{ \"content\": search_split_prompt,\"role\": \"user\"}],\n",
    "    temperature=0.5\n",
    ")\n",
    "print(\"Response recieved\")\n",
    "print(response.choices[0].message.content)\n",
    "arxiv_queries_list = response.choices[0].message.content.split(\"|\")\n",
    "\n",
    "\n",
    "# Each element contains vector stores for each search query developed by LLM\n",
    "chunks_for_queries = []\n",
    "for q in arxiv_queries_list:\n",
    "    docs = ArxivLoader(query=q, load_max_docs=5).load()\n",
    "    print(docs)\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=350, chunk_overlap=50\n",
    "    )\n",
    "    chunked_documents = text_splitter.split_documents(docs)\n",
    "    chunks_for_queries.append(chunked_documents)\n",
    "\n",
    "print(type(chunks_for_queries[0][0].page_content))\n",
    "print(chunks_for_queries[0][1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(type(chunks_for_queries))\n",
    "print(len(chunks_for_queries[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.vectorstores.faiss.FAISS'>\n",
      "<class 'langchain_community.vectorstores.faiss.FAISS'>\n",
      "<class 'langchain_community.vectorstores.faiss.FAISS'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Embedding Model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=apikey, base_url=\"https://drchat.xyz\")\n",
    "# Create Index- Load document chunks into the vectorstore\n",
    "vectorstore_list = []\n",
    "for x in chunks_for_queries:\n",
    "    faiss_vectorstore = FAISS.from_documents(\n",
    "        documents=x,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    print(type(faiss_vectorstore))\n",
    "    vectorstore_list.append(faiss_vectorstore)\n",
    "\n",
    "\n",
    "# Create a retriver and retrieve relevant documents for each vector store\n",
    "relevant_documents_list = []\n",
    "for x in vectorstore_list:\n",
    "    \n",
    "    relevant_documents = x.similarity_search(user_query, k = 5)\n",
    "    print(type(relevant_documents))\n",
    "    relevant_documents_list.append(relevant_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Document(page_content='different attention maps compared to the explicit attention,\\ni.e., learning to attend. Noticeably, standard CBAM attention\\ntries to capture the relevant parts of the image directly,\\nleading to the prediction being made based on the small\\npart of the input that is considered by the model as the most\\nimportant. This leads to the possibility that the model can\\nmiss some important parts of the class of interest on the\\nimage. As an example, only one of the plants on the lower\\n6\\nMMTM\\nIgn1(α=1)\\nIgn1(α=0.5)\\nIgn1(α=0.8)\\nIgn2\\nIgn3\\nNTU-RGBD\\n89.98\\n89.99\\n90.52\\n88.70\\n90.21\\n90.36\\nTABLE 4\\nAccuracy on NTU-RGBD dataset\\nFig. 1. Validation loss curves of ResNet50 on CIFAR100 using the different attention approaches.\\nﬁgure is considered in CBAM model, as well as only a side\\nof the bus in the middle image. On the other hand, our ap-\\nproach by learning to identify the non-relevant background\\nregions ﬁrst and subsequently suppressing them, simpliﬁes\\nthe problem and typically results in an attention mask that\\nis broader and captures the object of interest better, hence\\nreducing the risk of suppressing relevant attributes of it.\\n5\\nCONCLUSION\\nIn this paper, we provide a new perspective on attention in\\nCNNs where the main target is learning to ignore instead', metadata={'Published': '2021-11-10', 'Title': 'Learning to ignore: rethinking attention in CNNs', 'Authors': 'Firas Laakom, Kateryna Chumachenko, Jenni Raitoharju, Alexandros Iosifidis, Moncef Gabbouj', 'Summary': 'Recently, there has been an increasing interest in applying attention\\nmechanisms in Convolutional Neural Networks (CNNs) to solve computer vision\\ntasks. Most of these methods learn to explicitly identify and highlight\\nrelevant parts of the scene and pass the attended image to further layers of\\nthe network. In this paper, we argue that such an approach might not be\\noptimal. Arguably, explicitly learning which parts of the image are relevant is\\ntypically harder than learning which parts of the image are less relevant and,\\nthus, should be ignored. In fact, in vision domain, there are many\\neasy-to-identify patterns of irrelevant features. For example, image regions\\nclose to the borders are less likely to contain useful information for a\\nclassification task. Based on this idea, we propose to reformulate the\\nattention mechanism in CNNs to learn to ignore instead of learning to attend.\\nSpecifically, we propose to explicitly learn irrelevant information in the\\nscene and suppress it in the produced representation, keeping only important\\nattributes. This implicit attention scheme can be incorporated into any\\nexisting attention mechanism. In this work, we validate this idea using two\\nrecent attention methods Squeeze and Excitation (SE) block and Convolutional\\nBlock Attention Module (CBAM). Experimental results on different datasets and\\nmodel architectures show that learning to ignore, i.e., implicit attention,\\nyields superior performance compared to the standard approaches.'}), Document(page_content='in a variety of tasks, including sequence learning [11], image\\ncaptioning [5], and others [12], [13]. A subset of attention-\\ndriven methods is directed at CNNs and aims at selecting\\nand highlighting relevant attributes in the feature space\\nduring training [1], [2]. Conventionally, this is achieved by\\nlearning attention masks over feature representations that\\nencode the importance of different attributes in form of\\nweights and applying these masks on intermediate feature\\nrepresentations. This results in higher inﬂuence of features\\nrelevant for decision making in subsequent layers.\\nOther tasks adjacent to this line of research include\\nsaliency estimation, image segmentation, and weakly-\\nsupervised object localization. In saliency estimation, the\\ngoal is to estimate salient, i.e., signiﬁcant regions of the scene\\nwithout any prior knowledge on the scene in unsupervised\\n[14], [15] or supervised manner [16], [17], [18]. In image\\nsegmentation, the task is to partition a given image into\\na set of segments, based on either semantics (semantic\\nsegmentation) or individual objects (instance segmentation)\\n[19]. In weakly-supervised object localization, the goal is\\nto predict the location of the object given only image-level\\nlabels [20].\\nWithin the attention mechanisms utilized in CNNs, two\\nof the notable ones include Squeeze-and-Excitation block\\n(SE) [1] and Convolutional Block Attention Module (CBAM)\\n[2]. In SE, an attention mask is learned channel-wise based', metadata={'Published': '2021-11-10', 'Title': 'Learning to ignore: rethinking attention in CNNs', 'Authors': 'Firas Laakom, Kateryna Chumachenko, Jenni Raitoharju, Alexandros Iosifidis, Moncef Gabbouj', 'Summary': 'Recently, there has been an increasing interest in applying attention\\nmechanisms in Convolutional Neural Networks (CNNs) to solve computer vision\\ntasks. Most of these methods learn to explicitly identify and highlight\\nrelevant parts of the scene and pass the attended image to further layers of\\nthe network. In this paper, we argue that such an approach might not be\\noptimal. Arguably, explicitly learning which parts of the image are relevant is\\ntypically harder than learning which parts of the image are less relevant and,\\nthus, should be ignored. In fact, in vision domain, there are many\\neasy-to-identify patterns of irrelevant features. For example, image regions\\nclose to the borders are less likely to contain useful information for a\\nclassification task. Based on this idea, we propose to reformulate the\\nattention mechanism in CNNs to learn to ignore instead of learning to attend.\\nSpecifically, we propose to explicitly learn irrelevant information in the\\nscene and suppress it in the produced representation, keeping only important\\nattributes. This implicit attention scheme can be incorporated into any\\nexisting attention mechanism. In this work, we validate this idea using two\\nrecent attention methods Squeeze and Excitation (SE) block and Convolutional\\nBlock Attention Module (CBAM). Experimental results on different datasets and\\nmodel architectures show that learning to ignore, i.e., implicit attention,\\nyields superior performance compared to the standard approaches.'}), Document(page_content='[15] J. Zhang, T. Zhang, Y. Dai, M. Harandi, and R. Hartley, “Deep\\nunsupervised saliency detection: A multiple noisy labeling per-\\nspective,” in Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 2018, pp. 9029–9038.\\n[16] N. Liu, J. Han, and M.-H. Yang, “Picanet: Learning pixel-wise\\ncontextual attention for saliency detection,” in Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recognition, 2018,\\npp. 3089–3098.\\n[17] N. Liu, N. Zhang, K. Wan, J. Han, and L. Shao, “Visual saliency\\ntransformer,” arXiv preprint arXiv:2104.12099, 2021.\\n[18] N. Liu and J. Han, “Dhsnet: Deep hierarchical saliency network\\nfor salient object detection,” in Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, 2016, pp. 678–686.\\n7\\nFig. 2. Visual results of different CBAM-based attention mechanisms on three different samples from validation set of ImageNet. The attention\\nmasks are obtained as in [38].\\n[19] S. Minaee, Y. Y. Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz, and\\nD. Terzopoulos, “Image segmentation using deep learning: A sur-', metadata={'Published': '2021-11-10', 'Title': 'Learning to ignore: rethinking attention in CNNs', 'Authors': 'Firas Laakom, Kateryna Chumachenko, Jenni Raitoharju, Alexandros Iosifidis, Moncef Gabbouj', 'Summary': 'Recently, there has been an increasing interest in applying attention\\nmechanisms in Convolutional Neural Networks (CNNs) to solve computer vision\\ntasks. Most of these methods learn to explicitly identify and highlight\\nrelevant parts of the scene and pass the attended image to further layers of\\nthe network. In this paper, we argue that such an approach might not be\\noptimal. Arguably, explicitly learning which parts of the image are relevant is\\ntypically harder than learning which parts of the image are less relevant and,\\nthus, should be ignored. In fact, in vision domain, there are many\\neasy-to-identify patterns of irrelevant features. For example, image regions\\nclose to the borders are less likely to contain useful information for a\\nclassification task. Based on this idea, we propose to reformulate the\\nattention mechanism in CNNs to learn to ignore instead of learning to attend.\\nSpecifically, we propose to explicitly learn irrelevant information in the\\nscene and suppress it in the produced representation, keeping only important\\nattributes. This implicit attention scheme can be incorporated into any\\nexisting attention mechanism. In this work, we validate this idea using two\\nrecent attention methods Squeeze and Excitation (SE) block and Convolutional\\nBlock Attention Module (CBAM). Experimental results on different datasets and\\nmodel architectures show that learning to ignore, i.e., implicit attention,\\nyields superior performance compared to the standard approaches.'}), Document(page_content='f sp(Fch) = σ(Conv7×7(GAP(Fch) ⌢GMP(Fch))),\\n(7)\\nwhere f ch and f sp denote channel and spatial attention,\\nrespectively, GAP(·) and GMP(·) correspond to Global\\nAverage Pooling and Global Max Pooling, respectively, δ is\\na ReLU activation, σ is the sigmoid activation, W1 ∈Rc× c\\nr\\nand W2 ∈R\\nc\\nr ×c are linear layers, c is the number of\\nchannels in F, and r is the reduction rate in the bottleneck\\nblock, similarly to SE. Fch is the channel-wise attended\\nfeature map, Conv7×7 denotes a convolutional layer with\\n7 × 7 kernel, and ⌢denotes concatenation.\\nAs can be seen, channel and spatial attention masks are\\napplied sequentially and channel-attended feature represen-\\ntations are used as input to compute spatial attention. Fol-\\nlowing this, we transform CBAM for ignoring by addition\\nof inversion function T(·) on top of both channel function\\nf ch(·) and spatial function f sp(·) to reformulate their objec-\\ntives as learning of features and regions to ignore. In both\\ncases, variants of T1(·) and T2(·) are applied directly on the\\noutput of corresponding functions, and T3(·) is applied on\\npre-sigmoid output.\\n4\\nEXPERIMENTAL RESULTS', metadata={'Published': '2021-11-10', 'Title': 'Learning to ignore: rethinking attention in CNNs', 'Authors': 'Firas Laakom, Kateryna Chumachenko, Jenni Raitoharju, Alexandros Iosifidis, Moncef Gabbouj', 'Summary': 'Recently, there has been an increasing interest in applying attention\\nmechanisms in Convolutional Neural Networks (CNNs) to solve computer vision\\ntasks. Most of these methods learn to explicitly identify and highlight\\nrelevant parts of the scene and pass the attended image to further layers of\\nthe network. In this paper, we argue that such an approach might not be\\noptimal. Arguably, explicitly learning which parts of the image are relevant is\\ntypically harder than learning which parts of the image are less relevant and,\\nthus, should be ignored. In fact, in vision domain, there are many\\neasy-to-identify patterns of irrelevant features. For example, image regions\\nclose to the borders are less likely to contain useful information for a\\nclassification task. Based on this idea, we propose to reformulate the\\nattention mechanism in CNNs to learn to ignore instead of learning to attend.\\nSpecifically, we propose to explicitly learn irrelevant information in the\\nscene and suppress it in the produced representation, keeping only important\\nattributes. This implicit attention scheme can be incorporated into any\\nexisting attention mechanism. In this work, we validate this idea using two\\nrecent attention methods Squeeze and Excitation (SE) block and Convolutional\\nBlock Attention Module (CBAM). Experimental results on different datasets and\\nmodel architectures show that learning to ignore, i.e., implicit attention,\\nyields superior performance compared to the standard approaches.'}), Document(page_content='methods Squeeze and Excitation (SE) block and Convolutional Block Attention Module (CBAM). Experimental results on different\\ndatasets and model architectures show that learning to ignore, i.e., implicit attention, yields superior performance compared to the\\nstandard approaches.\\nIndex Terms—Computer vision, CNNs, attention mechanisms, CBAM, SE\\n!\\n1\\nINTRODUCTION\\nI\\nNSPIRED by the properties of the human visual system,\\nattention mechanisms have been recently applied in the\\nﬁeld of deep learning, resulting in improved performance\\nof the existing models across multiple applications. In the\\ncontext of computer vision, learning to attend, i.e., learning\\nto highlight and emphasize relevant attributes of images,\\nhave led to development of novel approaches [1], [2] in\\nConvolutional Neural Networks (CNNs), improving their\\ncapabilities in many tasks [3], [4], [5].\\nRelated to the concept of attention, recent studies in neu-\\nroscience suggest that the ability of humans to successfully\\nperform visual tasks is related to the ability to ignore and\\nsuppress distractive information [6], [7], [8]. For example,\\nthe authors of [7] show that differences in visual working\\nmemory capacity, i.e., ability to remember visual features\\nof multiple objects, are speciﬁcally related to distractor-\\nsuppression activity in visual cortex. This idea is reinforced\\nin [8], where the authors provide evidence on an inhibitory\\nmechanism of suppression of salient distractors aimed at', metadata={'Published': '2021-11-10', 'Title': 'Learning to ignore: rethinking attention in CNNs', 'Authors': 'Firas Laakom, Kateryna Chumachenko, Jenni Raitoharju, Alexandros Iosifidis, Moncef Gabbouj', 'Summary': 'Recently, there has been an increasing interest in applying attention\\nmechanisms in Convolutional Neural Networks (CNNs) to solve computer vision\\ntasks. Most of these methods learn to explicitly identify and highlight\\nrelevant parts of the scene and pass the attended image to further layers of\\nthe network. In this paper, we argue that such an approach might not be\\noptimal. Arguably, explicitly learning which parts of the image are relevant is\\ntypically harder than learning which parts of the image are less relevant and,\\nthus, should be ignored. In fact, in vision domain, there are many\\neasy-to-identify patterns of irrelevant features. For example, image regions\\nclose to the borders are less likely to contain useful information for a\\nclassification task. Based on this idea, we propose to reformulate the\\nattention mechanism in CNNs to learn to ignore instead of learning to attend.\\nSpecifically, we propose to explicitly learn irrelevant information in the\\nscene and suppress it in the produced representation, keeping only important\\nattributes. This implicit attention scheme can be incorporated into any\\nexisting attention mechanism. In this work, we validate this idea using two\\nrecent attention methods Squeeze and Excitation (SE) block and Convolutional\\nBlock Attention Module (CBAM). Experimental results on different datasets and\\nmodel architectures show that learning to ignore, i.e., implicit attention,\\nyields superior performance compared to the standard approaches.'})], [Document(page_content='B. Convolutional Block Attention Module (CBAM)\\nThe above basic CNN network transforms the image data X into a feature map F as the input to CBAM. The CBAM\\ncontains two independent sub-modules, channel attention module (CAM) and spatial attention module (SAM), which can save\\nparameters and computational power by performing attention mechanism on channel and space respectively, as shown in Fig.\\n2.\\nCAM\\nSAM\\nFeature\\nRefined feature\\n\\uf0c4\\n\\uf0c4\\nFig. 2: Convolutional block attention module\\nCBAM inferred the attention maps one at a time along two independent dimensions (channel and space) and then multiplied\\nthe attention maps by the input feature maps for adaptive feature reﬁnement. The process of the input feature map F being\\nprocessed in CBAM can be summarized as:\\nF′ =Mc(F) ⊗F\\nF′′ =Ms(F′) ⊗F′\\n(2)\\nwhere ⊗denotes element multiplication, F′ denotes the result of multiplying the feature map with the channel attention map,\\nand F′′ is the ﬁnal reﬁned output.\\nBelow, we will detail how the two separate dimensions, CAM and SAM, work.\\n1) CAM: To accomplish feature extraction and reduce data loss, the channel attention module compresses the feature maps\\non the spatial dimension using the global average pooling layer and the global maximum pooling layer. Fig. 3 shows the\\nchannel attention module. The global average pooling layer obtains the overall information, while the global maximum pooling', metadata={'Published': '2022-02-14', 'Title': 'Convolutional Neural Network with Convolutional Block Attention Module for Finger Vein Recognition', 'Authors': 'Zhongxia Zhang, Mingwen Wang', 'Summary': 'Convolutional neural networks have become a popular research in the field of\\nfinger vein recognition because of their powerful image feature representation.\\nHowever, most researchers focus on improving the performance of the network by\\nincreasing the CNN depth and width, which often requires high computational\\neffort. Moreover, we can notice that not only the importance of pixels in\\ndifferent channels is different, but also the importance of pixels in different\\npositions of the same channel is different. To reduce the computational effort\\nand to take into account the different importance of pixels, we propose a\\nlightweight convolutional neural network with a convolutional block attention\\nmodule (CBAM) for finger vein recognition, which can achieve a more accurate\\ncapture of visual structures through an attention mechanism. First, image\\nsequences are fed into a lightweight convolutional neural network we designed\\nto improve visual features. Afterwards, it learns to assign feature weights in\\nan adaptive manner with the help of a convolutional block attention module. The\\nexperiments are carried out on two publicly available databases and the results\\ndemonstrate that the proposed method achieves a stable, highly accurate, and\\nrobust performance in multimodal finger recognition.'}), Document(page_content='representation capability of CNN. Then, CBAM blocks are embedded in the original network to infer the attention mapping\\naccording to two independent dimensions - channel and spatial order, and the attention mapping is multiplied into the input\\nfeature mapping with adaptive feature reﬁnement. Finally, the output features are classiﬁed using the softmax function, and the\\nmethod proposed in this paper has signiﬁcant improvements in two different databases. The main contributions are as follows:\\n(1) To our knowledge, this paper is the ﬁrst to successfully apply lightweight CBAM blocks into CNN for ﬁnger vein\\nrecognition.\\n(2) We combine a convolutional block attention module (CBAM) with a lightweight CNN to simulate visual attention\\nmechanisms and enhance the ﬂow of information in channels and spaces.\\n(3) Our network structure is simple and guarantees the smallest possible computation without sacriﬁcing network performance.\\nExperimental results show that the scheme in this paper has competitive potential in ﬁnger vein recognition systems.\\nThe rest of the paper is organized as follows. Section II is to describe the related work. Section III presents the theoretical\\nbackground of CNN and CBAM. Proposed approach is discussed in Section IV. In Section V, we present experimental results.\\nFinally, Section VI concludes the paper.\\nII. RELATED WORK\\nUndoubtedly, a proper feature extraction method in ﬁnger vein recognition systems can be of great beneﬁt in improving the', metadata={'Published': '2022-02-14', 'Title': 'Convolutional Neural Network with Convolutional Block Attention Module for Finger Vein Recognition', 'Authors': 'Zhongxia Zhang, Mingwen Wang', 'Summary': 'Convolutional neural networks have become a popular research in the field of\\nfinger vein recognition because of their powerful image feature representation.\\nHowever, most researchers focus on improving the performance of the network by\\nincreasing the CNN depth and width, which often requires high computational\\neffort. Moreover, we can notice that not only the importance of pixels in\\ndifferent channels is different, but also the importance of pixels in different\\npositions of the same channel is different. To reduce the computational effort\\nand to take into account the different importance of pixels, we propose a\\nlightweight convolutional neural network with a convolutional block attention\\nmodule (CBAM) for finger vein recognition, which can achieve a more accurate\\ncapture of visual structures through an attention mechanism. First, image\\nsequences are fed into a lightweight convolutional neural network we designed\\nto improve visual features. Afterwards, it learns to assign feature weights in\\nan adaptive manner with the help of a convolutional block attention module. The\\nexperiments are carried out on two publicly available databases and the results\\ndemonstrate that the proposed method achieves a stable, highly accurate, and\\nrobust performance in multimodal finger recognition.'}), Document(page_content='arXiv:2202.06673v1  [cs.CV]  14 Feb 2022\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT\\n2\\nIt is known that an important feature of the human visual system is that it does not try to process the whole scene\\nimmediately, but selectively focuses on salient parts in order to better capture the visual structure. Attention can be directed to\\nfocus, and expressivity can be improved by using attentional mechanisms, i.e., focusing on important features and suppressing\\nunnecessary ones. The convolutional block attention module (CBAM) [14] is a simple and effective attention module for\\nfeedforward convolutional neural networks that can attend to important information in channels and spaces separately, which\\nnot only saves parameters and computational power, but also ensures its integration into existing network architectures as a\\nplug-and-play module.\\nMotivated by the success of CNN and CBAM, we propose a novel and effective feature representation of lightweight\\nCNN based on CBAM blocks for the ﬁnger recognition to save computational power. By embedding CBAM in the designed\\nlightweight CNN architecture, the accuracy is improved while ensuring small computational power. Speciﬁcally, the image is\\nfed into a lightweight base CNN architecture, and the initial features of the ﬁnger vein are extracted using the powerful feature\\nrepresentation capability of CNN. Then, CBAM blocks are embedded in the original network to infer the attention mapping\\naccording to two independent dimensions - channel and spatial order, and the attention mapping is multiplied into the input', metadata={'Published': '2022-02-14', 'Title': 'Convolutional Neural Network with Convolutional Block Attention Module for Finger Vein Recognition', 'Authors': 'Zhongxia Zhang, Mingwen Wang', 'Summary': 'Convolutional neural networks have become a popular research in the field of\\nfinger vein recognition because of their powerful image feature representation.\\nHowever, most researchers focus on improving the performance of the network by\\nincreasing the CNN depth and width, which often requires high computational\\neffort. Moreover, we can notice that not only the importance of pixels in\\ndifferent channels is different, but also the importance of pixels in different\\npositions of the same channel is different. To reduce the computational effort\\nand to take into account the different importance of pixels, we propose a\\nlightweight convolutional neural network with a convolutional block attention\\nmodule (CBAM) for finger vein recognition, which can achieve a more accurate\\ncapture of visual structures through an attention mechanism. First, image\\nsequences are fed into a lightweight convolutional neural network we designed\\nto improve visual features. Afterwards, it learns to assign feature weights in\\nan adaptive manner with the help of a convolutional block attention module. The\\nexperiments are carried out on two publicly available databases and the results\\ndemonstrate that the proposed method achieves a stable, highly accurate, and\\nrobust performance in multimodal finger recognition.'}), Document(page_content='3-2) CBAM Model \\nThe CBSM is a model designed to enhance the precision of chest X-ray lung segmentation in \\nthe U-net architecture, as opposed to the conventional CNN, particularly when dealing with limited \\nsample sizes. The amalgamation of channel, spatial, and pixel attention mechanisms marks a \\nsubstantial progression in honing the focus on pertinent features within X-ray images. \\n1. Channel Attention: This mechanism emphasizes inter-channel associations, enabling the \\nmodel to concentrate more on channels that are more informative. It aids the model in \\nfocusing on the most significant features across various channels, thereby boosting the \\nmodel’s capability to identify relevant features. \\n2. Spatial Attention: This mechanism allows the model to concentrate on crucial spatial \\nlocations. It enables the model to pay more attention to the spatial correlations between \\nfeatures, thereby enhancing the model’s precision in localization. \\n3. Pixel Attention: This mechanism empowers the model to focus on individual pixels. It \\nfurther sharpens the model’s focus, enabling it to pay more attention to the most \\ninformative pixels, thereby improving the accuracy of segmentation. \\nThese three attention mechanisms collaborate to provide a more comprehensive representation \\nof features, thereby enhancing the model’s performance in tasks such as image segmentation. They \\nenable the model to capture global contextual information more effectively and improve the \\nnetwork’s attention to specific regions.', metadata={'Published': '2024-05-07', 'Title': 'A Novel Approach to Chest X-ray Lung Segmentation Using U-net and Modified Convolutional Block Attention Module', 'Authors': 'Mohammad Ali Labbaf Khaniki, Mohammad Manthouri', 'Summary': \"Lung segmentation in chest X-ray images is of paramount importance as it\\nplays a crucial role in the diagnosis and treatment of various lung diseases.\\nThis paper presents a novel approach for lung segmentation in chest X-ray\\nimages by integrating U-net with attention mechanisms. The proposed method\\nenhances the U-net architecture by incorporating a Convolutional Block\\nAttention Module (CBAM), which unifies three distinct attention mechanisms:\\nchannel attention, spatial attention, and pixel attention. The channel\\nattention mechanism enables the model to concentrate on the most informative\\nfeatures across various channels. The spatial attention mechanism enhances the\\nmodel's precision in localization by focusing on significant spatial locations.\\nLastly, the pixel attention mechanism empowers the model to focus on individual\\npixels, further refining the model's focus and thereby improving the accuracy\\nof segmentation. The adoption of the proposed CBAM in conjunction with the\\nU-net architecture marks a significant advancement in the field of medical\\nimaging, with potential implications for improving diagnostic precision and\\npatient outcomes. The efficacy of this method is validated against contemporary\\nstate-of-the-art techniques, showcasing its superiority in segmentation\\nperformance.\"}), Document(page_content='over the attention given to each pixel, potentially improving the accuracy of the segmentation. The \\nCBSM is visualized in Figure 4. \\n \\nFig. 4:  Visualization of the channel, spatial, and pixel attention mechanisms in the CBSM model. \\nBased on the Fig. 4, the formulas of the mentioned attention mechanisms are shown as \\nfollowing. \\n𝑀𝐶= 𝜎(𝐶𝑁𝑁2 (𝑅𝑒𝐿𝑈(𝐶𝑁𝑁1 (𝐺𝑃\\n𝑎𝑣𝑔(𝑥))))),                                                                                 (1)', metadata={'Published': '2024-05-07', 'Title': 'A Novel Approach to Chest X-ray Lung Segmentation Using U-net and Modified Convolutional Block Attention Module', 'Authors': 'Mohammad Ali Labbaf Khaniki, Mohammad Manthouri', 'Summary': \"Lung segmentation in chest X-ray images is of paramount importance as it\\nplays a crucial role in the diagnosis and treatment of various lung diseases.\\nThis paper presents a novel approach for lung segmentation in chest X-ray\\nimages by integrating U-net with attention mechanisms. The proposed method\\nenhances the U-net architecture by incorporating a Convolutional Block\\nAttention Module (CBAM), which unifies three distinct attention mechanisms:\\nchannel attention, spatial attention, and pixel attention. The channel\\nattention mechanism enables the model to concentrate on the most informative\\nfeatures across various channels. The spatial attention mechanism enhances the\\nmodel's precision in localization by focusing on significant spatial locations.\\nLastly, the pixel attention mechanism empowers the model to focus on individual\\npixels, further refining the model's focus and thereby improving the accuracy\\nof segmentation. The adoption of the proposed CBAM in conjunction with the\\nU-net architecture marks a significant advancement in the field of medical\\nimaging, with potential implications for improving diagnostic precision and\\npatient outcomes. The efficacy of this method is validated against contemporary\\nstate-of-the-art techniques, showcasing its superiority in segmentation\\nperformance.\"})], [Document(page_content='humans exploit a sequence of partial glimpses and selectively focus on salient\\nparts in order to capture visual structure better [26].\\nRecently, there have been several attempts [27, 28] to incorporate attention\\nprocessing to improve the performance of CNNs in large-scale classiﬁcation tasks.\\nWang et al. [27] propose Residual Attention Network which uses an encoder-\\ndecoder style attention module. By reﬁning the feature maps, the network not\\nonly performs well but is also robust to noisy inputs. Instead of directly com-\\nputing the 3d attention map, we decompose the process that learns channel\\nattention and spatial attention separately. The separate attention generation\\nprocess for 3D feature map has much less computational and parameter over-\\n4\\nWoo, Park, Lee, Kweon\\nhead, and therefore can be used as a plug-and-play module for pre-existing base\\nCNN architectures.\\nMore close to our work, Hu et al. [28] introduce a compact module to exploit\\nthe inter-channel relationship. In their Squeeze-and-Excitation module, they use\\nglobal average-pooled features to compute channel-wise attention. However, we\\nshow that those are suboptimal features in order to infer ﬁne channel attention,\\nand we suggest to use max-pooled features as well. They also miss the spatial\\nattention, which plays an important role in deciding ‘where’ to focus as shown in\\n[29]. In our CBAM, we exploit both spatial and channel-wise attention based on', metadata={'Published': '2018-07-18', 'Title': 'CBAM: Convolutional Block Attention Module', 'Authors': 'Sanghyun Woo, Jongchan Park, Joon-Young Lee, In So Kweon', 'Summary': 'We propose Convolutional Block Attention Module (CBAM), a simple yet\\neffective attention module for feed-forward convolutional neural networks.\\nGiven an intermediate feature map, our module sequentially infers attention\\nmaps along two separate dimensions, channel and spatial, then the attention\\nmaps are multiplied to the input feature map for adaptive feature refinement.\\nBecause CBAM is a lightweight and general module, it can be integrated into any\\nCNN architectures seamlessly with negligible overheads and is end-to-end\\ntrainable along with base CNNs. We validate our CBAM through extensive\\nexperiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets.\\nOur experiments show consistent improvements in classification and detection\\nperformances with various models, demonstrating the wide applicability of CBAM.\\nThe code and models will be publicly available.'}), Document(page_content='CBAM: Convolutional Block Attention Module\\nSanghyun Woo*1, Jongchan Park*†2, Joon-Young Lee3, and In So Kweon1\\n1 Korea Advanced Institute of Science and Technology, Daejeon, Korea\\n{shwoo93, iskweon77}@kaist.ac.kr\\n2 Lunit Inc., Seoul, Korea\\njcpark@lunit.io\\n3 Adobe Research, San Jose, CA, USA\\njolee@adobe.com\\nAbstract. We propose Convolutional Block Attention Module (CBAM),\\na simple yet eﬀective attention module for feed-forward convolutional\\nneural networks. Given an intermediate feature map, our module se-\\nquentially infers attention maps along two separate dimensions, channel\\nand spatial, then the attention maps are multiplied to the input feature\\nmap for adaptive feature reﬁnement. Because CBAM is a lightweight and\\ngeneral module, it can be integrated into any CNN architectures seam-\\nlessly with negligible overheads and is end-to-end trainable along with\\nbase CNNs. We validate our CBAM through extensive experiments on\\nImageNet-1K, MS COCO detection, and VOC 2007 detection datasets.\\nOur experiments show consistent improvements in classiﬁcation and de-\\ntection performances with various models, demonstrating the wide ap-\\nplicability of CBAM. The code and models will be publicly available.\\nKeywords: Object recognition, attention mechanism, gated convolu-\\ntion\\n1\\nIntroduction', metadata={'Published': '2018-07-18', 'Title': 'CBAM: Convolutional Block Attention Module', 'Authors': 'Sanghyun Woo, Jongchan Park, Joon-Young Lee, In So Kweon', 'Summary': 'We propose Convolutional Block Attention Module (CBAM), a simple yet\\neffective attention module for feed-forward convolutional neural networks.\\nGiven an intermediate feature map, our module sequentially infers attention\\nmaps along two separate dimensions, channel and spatial, then the attention\\nmaps are multiplied to the input feature map for adaptive feature refinement.\\nBecause CBAM is a lightweight and general module, it can be integrated into any\\nCNN architectures seamlessly with negligible overheads and is end-to-end\\ntrainable along with base CNNs. We validate our CBAM through extensive\\nexperiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets.\\nOur experiments show consistent improvements in classification and detection\\nperformances with various models, demonstrating the wide applicability of CBAM.\\nThe code and models will be publicly available.'}), Document(page_content='attention, which plays an important role in deciding ‘where’ to focus as shown in\\n[29]. In our CBAM, we exploit both spatial and channel-wise attention based on\\nan eﬃcient architecture and empirically verify that exploiting both is superior to\\nusing only the channel-wise attention as [28]. Moreover, we empirically show that\\nour module is eﬀective in detection tasks (MS-COCO and VOC). Especially, we\\nachieve state-of-the-art performance just by placing our module on top of the\\nexisting one-shot detector [30] in the VOC2007 test set.\\n3\\nConvolutional Block Attention Module\\nGiven an intermediate feature map F ∈RC×H×W as input, CBAM sequentially\\ninfers a 1D channel attention map Mc ∈RC×1×1 and a 2D spatial attention\\nmap Ms ∈R1×H×W as illustrated in Fig. 1. The overall attention process can\\nbe summarized as:\\nF′ = Mc(F) ⊗F,\\nF′′ = Ms(F′) ⊗F′,\\n(1)\\nwhere ⊗denotes element-wise multiplication. During multiplication, the atten-\\ntion values are broadcasted (copied) accordingly: channel attention values are\\nbroadcasted along the spatial dimension, and vice versa. F′′ is the ﬁnal reﬁned\\noutput. Fig. 2 depicts the computation process of each attention map. The fol-\\nlowing describes the details of each attention module.', metadata={'Published': '2018-07-18', 'Title': 'CBAM: Convolutional Block Attention Module', 'Authors': 'Sanghyun Woo, Jongchan Park, Joon-Young Lee, In So Kweon', 'Summary': 'We propose Convolutional Block Attention Module (CBAM), a simple yet\\neffective attention module for feed-forward convolutional neural networks.\\nGiven an intermediate feature map, our module sequentially infers attention\\nmaps along two separate dimensions, channel and spatial, then the attention\\nmaps are multiplied to the input feature map for adaptive feature refinement.\\nBecause CBAM is a lightweight and general module, it can be integrated into any\\nCNN architectures seamlessly with negligible overheads and is end-to-end\\ntrainable along with base CNNs. We validate our CBAM through extensive\\nexperiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets.\\nOur experiments show consistent improvements in classification and detection\\nperformances with various models, demonstrating the wide applicability of CBAM.\\nThe code and models will be publicly available.'}), Document(page_content='For spatial attention module, the module ﬁrst applies an average-pooling operation\\n(AP) and a max-pooling (MP) operation along the channel axis of input features and\\ngenerate an average samples and a max samples. Then it concatenate these two samples\\nand apply a convolutional layer to generate the spatial attention maps. This maps can\\nbe viewed as the weight of each time points. Finally, an element-wise multiplication is\\ncomputed between the input features and the spatial attention maps. Spatial attention\\nmodule is computed as:\\nMs(F) = σ(γ([AP(F); MP(F)])) ⊗F\\n(4)\\nwhere σ denotes the sigmoid function and r denotes the convolutional layer. Ms denotes\\nthe output of the spatial attention module.\\n3\\nCNN Architecture\\nIn this section, we introduce the architecture of the new CNN model and the application of\\nCBAM in the new network. CBAM makes the attention network focus on the informative\\npoints and suppresses irrelative points to improve the performance of CNNs.\\n3.1\\nBasic Architecture of the Enhanced Network\\nWith the development of CNNs, some classical architectures have been proposed such\\nas VGG [SZ14] and Residual Networks (ResNets) [HZRS16].\\nSome researchers have\\ninvestigated the application of VGG architecture in SCA [BPS+18,KPH+19] and indicated\\nthat the architecture performs well. In this work, we investigate another architecture\\n- ResNets in SCA and propose a new CNN model called attention network. The basic', metadata={'Published': '2020-09-18', 'Title': 'An Enhanced Convolutional Neural Network in Side-Channel Attacks and Its Visualization', 'Authors': 'Minhui Jin, Mengce Zheng, Honggang Hu, Nenghai Yu', 'Summary': 'In recent years, the convolutional neural networks (CNNs) have received a lot\\nof interest in the side-channel community. The previous work has shown that\\nCNNs have the potential of breaking the cryptographic algorithm protected with\\nmasking or desynchronization. Before, several CNN models have been exploited,\\nreaching the same or even better level of performance compared to the\\ntraditional side-channel attack (SCA). In this paper, we investigate the\\narchitecture of Residual Network and build a new CNN model called attention\\nnetwork. To enhance the power of the attention network, we introduce an\\nattention mechanism - Convolutional Block Attention Module (CBAM) and\\nincorporate CBAM into the CNN architecture. CBAM points out the informative\\npoints of the input traces and makes the attention network focus on the\\nrelevant leakages of the measurements. It is able to improve the performance of\\nthe CNNs. Because the irrelevant points will introduce the extra noises and\\ncause a worse performance of attacks. We compare our attention network with the\\none designed for the masking AES implementation called ASCAD network in this\\npaper. We show that the attention network has a better performance than the\\nASCAD network. Finally, a new visualization method, named Class Gradient\\nVisualization (CGV) is proposed to recognize which points of the input traces\\nhave a positive influence on the predicted result of the neural networks. In\\nanother aspect, it can explain why the attention network is superior to the\\nASCAD network. We validate the attention network through extensive experiments\\non four public datasets and demonstrate that the attention network is efficient\\nin different AES implementations.'}), Document(page_content='B. Convolutional Block Attention Module (CBAM)\\nThe above basic CNN network transforms the image data X into a feature map F as the input to CBAM. The CBAM\\ncontains two independent sub-modules, channel attention module (CAM) and spatial attention module (SAM), which can save\\nparameters and computational power by performing attention mechanism on channel and space respectively, as shown in Fig.\\n2.\\nCAM\\nSAM\\nFeature\\nRefined feature\\n\\uf0c4\\n\\uf0c4\\nFig. 2: Convolutional block attention module\\nCBAM inferred the attention maps one at a time along two independent dimensions (channel and space) and then multiplied\\nthe attention maps by the input feature maps for adaptive feature reﬁnement. The process of the input feature map F being\\nprocessed in CBAM can be summarized as:\\nF′ =Mc(F) ⊗F\\nF′′ =Ms(F′) ⊗F′\\n(2)\\nwhere ⊗denotes element multiplication, F′ denotes the result of multiplying the feature map with the channel attention map,\\nand F′′ is the ﬁnal reﬁned output.\\nBelow, we will detail how the two separate dimensions, CAM and SAM, work.\\n1) CAM: To accomplish feature extraction and reduce data loss, the channel attention module compresses the feature maps\\non the spatial dimension using the global average pooling layer and the global maximum pooling layer. Fig. 3 shows the\\nchannel attention module. The global average pooling layer obtains the overall information, while the global maximum pooling', metadata={'Published': '2022-02-14', 'Title': 'Convolutional Neural Network with Convolutional Block Attention Module for Finger Vein Recognition', 'Authors': 'Zhongxia Zhang, Mingwen Wang', 'Summary': 'Convolutional neural networks have become a popular research in the field of\\nfinger vein recognition because of their powerful image feature representation.\\nHowever, most researchers focus on improving the performance of the network by\\nincreasing the CNN depth and width, which often requires high computational\\neffort. Moreover, we can notice that not only the importance of pixels in\\ndifferent channels is different, but also the importance of pixels in different\\npositions of the same channel is different. To reduce the computational effort\\nand to take into account the different importance of pixels, we propose a\\nlightweight convolutional neural network with a convolutional block attention\\nmodule (CBAM) for finger vein recognition, which can achieve a more accurate\\ncapture of visual structures through an attention mechanism. First, image\\nsequences are fed into a lightweight convolutional neural network we designed\\nto improve visual features. Afterwards, it learns to assign feature weights in\\nan adaptive manner with the help of a convolutional block attention module. The\\nexperiments are carried out on two publicly available databases and the results\\ndemonstrate that the proposed method achieves a stable, highly accurate, and\\nrobust performance in multimodal finger recognition.'})]]\n"
     ]
    }
   ],
   "source": [
    "print(relevant_documents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBAM (Convolutional Block Attention Module) sequentially infers attention maps along two separate dimensions, channel and spatial. It first applies global average pooling and global maximum pooling operations along the channel axis to obtain overall information and spatial correlations, respectively. These operations generate average samples and max samples, which are then concatenated and passed through a convolutional layer to generate the spatial attention maps. The channel attention module emphasizes inter-channel associations by focusing on the most significant features across various channels. The spatial attention module focuses on crucial spatial locations by considering the spatial correlations between features. The attention maps are multiplied with the input feature map for adaptive feature refinement, resulting in a final refined output. CBAM can be integrated into any CNN architecture with negligible overhead and is end-to-end trainable along with the base CNNs.\n"
     ]
    }
   ],
   "source": [
    "question_prompt = f\"\"\"\n",
    "Given the following context: {relevant_documents_list}\n",
    "\n",
    "Answer the following question: {user_query}\n",
    "\n",
    "Only answer the question if the answer is in the context. Otherwise, say that you don't know.\n",
    "\"\"\"\n",
    "\n",
    "response = completion(\n",
    "    api_key=apikey,\n",
    "    base_url=\"https://drchat.xyz\",\n",
    "    model = \"gpt-3.5-turbo-16k\",\n",
    "    custom_llm_provider=\"openai\",\n",
    "    messages = [{ \"content\": question_prompt,\"role\": \"user\"}],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
