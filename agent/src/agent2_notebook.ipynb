{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import litellm\n",
    "from litellm import completion\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load API keys\n",
    "load_dotenv(\".env\")\n",
    "apikey = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Request Successful\n",
      "Response: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3ARetrieval%20Augmented%20Generation%26id_list%3D%26start%3D0%26max_results%3D5\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=all:Retrieval Augmented Generation&amp;id_list=&amp;start=0&amp;max_results=5</title>\n",
      "  <id>http://arxiv.org/api/UOKvU43HASfzk48jVrFyBsmll1A</id>\n",
      "  <updated>2024-05-24T00:00:00-04:00</updat\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Example query to the arXiv API directly\n",
    "url = \"http://export.arxiv.org/api/query\"\n",
    "params = {\n",
    "    \"search_query\": \"all:Retrieval Augmented Generation\",\n",
    "    \"start\": 0,\n",
    "    \"max_results\": 5\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"API Request Successful\")\n",
    "    print(\"Response:\", response.text[:500])  # Print first 500 characters of the response\n",
    "else:\n",
    "    print(\"Failed to fetch data from arXiv:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document pertaining to a particular topic\n",
    "docs = ArxivLoader(query=\"\"\" all:\"attention mechanisms\" AND (all:\"convolutional neural networks\" OR all:\"CNN\") AND NOT all:\"transformer\" \"\"\", load_max_docs=5).load()\n",
    "\n",
    "# Split the dpocument into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=350, chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunked_documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "5\n",
      "<class 'str'>\n",
      "<class 'list'>\n",
      "199\n",
      "page_content='1\\nPulmonary Disease Classiﬁcation Using Globally\\nCorrelated Maximum Likelihood:\\nan Auxiliary Attention mechanism for\\nConvolutional Neural Networks\\nEdward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, and Faraz Hussain\\nAbstract—Convolutional neural networks (CNN) are now being\\nwidely used for classiﬁying and detecting pulmonary abnormal-\\nities in chest radiographs. Two complementary generalization\\nproperties of CNNs, translation invariance and equivariance,\\nare particularly useful in detecting manifested abnormalities\\nassociated with pulmonary disease, regardless of their spatial\\nlocations within the image. However, these properties also come\\nwith the loss of exact spatial information and global relative\\npositions of abnormalities detected in local regions. Global\\nrelative positions of such abnormalities may help distinguish\\nsimilar conditions, such as COVID-19 and viral pneumonia. In\\nsuch instances, a global attention mechanism is needed, which\\nCNNs do not support in their traditional architectures that\\naim for generalization afforded by translation invariance and\\nequivariance. Vision Transformers provide a global attention\\nmechanism, but lack translation invariance and equivariance,\\nrequiring signiﬁcantly more training data samples to match\\ngeneralization of CNNs. To address the loss of spatial information\\nand global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that\\nserves as an auxiliary attention mechanism to existing CNN\\narchitectures, in order to extract global correlations between\\nsalient features.\\nImpact Statement—We improve sensitivity of Convolutional' metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}\n"
     ]
    }
   ],
   "source": [
    "print(type(docs))\n",
    "print(len(docs))\n",
    "print(type(docs[0].page_content))\n",
    "\n",
    "print(type(chunked_documents))\n",
    "print(len(chunked_documents))\n",
    "print(chunked_documents[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "OpenAIException - Traceback (most recent call last):\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py\", line 67, in map_httpcore_exceptions\n    yield\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py\", line 231, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py\", line 99, in handle_request\n    raise exc\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py\", line 154, in _connect\n    stream = stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_backends\\sync.py\", line 152, in start_tls\n    with map_exceptions(exc_map):\n  File \"c:\\ProgramData\\anaconda3\\Lib\\contextlib.py\", line 155, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 952, in _request\n    response = self._client.send(\n               ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py\", line 915, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py\", line 943, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py\", line 980, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py\", line 1016, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py\", line 230, in handle_request\n    with map_httpcore_exceptions():\n  File \"c:\\ProgramData\\anaconda3\\Lib\\contextlib.py\", line 155, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py\", line 84, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\llms\\openai.py\", line 361, in completion\n    raise e\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\llms\\openai.py\", line 324, in completion\n    response = openai_client.chat.completions.create(**data, timeout=timeout)  # type: ignore\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\resources\\chat\\completions.py\", line 590, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 1240, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 921, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 976, in _request\n    return self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 1053, in _retry_request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 976, in _request\n    return self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 1053, in _retry_request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 986, in _request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py:67\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py:231\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 231\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[0;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(request)\n\u001b[0;32m     78\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py:154\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_tls\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m--> 154\u001b[0m     stream \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mstart_tls(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    155\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_backends\\sync.py:152\u001b[0m, in \u001b[0;36mSyncStream.start_tls\u001b[1;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[0;32m    148\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    149\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[0;32m    151\u001b[0m }\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 952\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    953\u001b[0m         request,\n\u001b[0;32m    954\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m    955\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py:915\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    913\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 915\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[0;32m    916\u001b[0m     request,\n\u001b[0;32m    917\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    918\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    919\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    920\u001b[0m )\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py:943\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[0;32m    944\u001b[0m         request,\n\u001b[0;32m    945\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    946\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m    947\u001b[0m     )\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py:980\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    978\u001b[0m     hook(request)\n\u001b[1;32m--> 980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py:1016\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1016\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py:230\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    218\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    219\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    220\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    229\u001b[0m )\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m    231\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py:84\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\llms\\openai.py:361\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[1;34m(self, model_response, timeout, model, messages, print_verbose, api_key, api_base, acompletion, logging_obj, optional_params, litellm_params, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider)\u001b[0m\n\u001b[0;32m    360\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 361\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\llms\\openai.py:324\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[1;34m(self, model_response, timeout, model, messages, print_verbose, api_key, api_base, acompletion, logging_obj, optional_params, litellm_params, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider)\u001b[0m\n\u001b[0;32m    313\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mpre_call(\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    315\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_client\u001b[38;5;241m.\u001b[39mapi_key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m     },\n\u001b[0;32m    322\u001b[0m )\n\u001b[1;32m--> 324\u001b[0m response \u001b[38;5;241m=\u001b[39m openai_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata, timeout\u001b[38;5;241m=\u001b[39mtimeout)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    325\u001b[0m stringified_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\resources\\chat\\completions.py:590\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    591\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    592\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    593\u001b[0m             {\n\u001b[0;32m    594\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    595\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    596\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    597\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    598\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    599\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    600\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    601\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    602\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    603\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    604\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    605\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    606\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    607\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    608\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    609\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    610\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    611\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    612\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    613\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    614\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    615\u001b[0m             },\n\u001b[0;32m    616\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    617\u001b[0m         ),\n\u001b[0;32m    618\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    619\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    620\u001b[0m         ),\n\u001b[0;32m    621\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    622\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    623\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    624\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1237\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m )\n\u001b[1;32m-> 1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    922\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    923\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    924\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    925\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    926\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    927\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:976\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    977\u001b[0m         options,\n\u001b[0;32m    978\u001b[0m         cast_to,\n\u001b[0;32m    979\u001b[0m         retries,\n\u001b[0;32m    980\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    981\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    982\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    983\u001b[0m     )\n\u001b[0;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1054\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1055\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1056\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1057\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1058\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1059\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:976\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    977\u001b[0m         options,\n\u001b[0;32m    978\u001b[0m         cast_to,\n\u001b[0;32m    979\u001b[0m         retries,\n\u001b[0;32m    980\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    981\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    982\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    983\u001b[0m     )\n\u001b[0;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1054\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1055\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1056\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1057\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1058\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1059\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:986\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    985\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    988\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    990\u001b[0m     request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m     response\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    995\u001b[0m )\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m: Connection error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\main.py:881\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m    875\u001b[0m     logging\u001b[38;5;241m.\u001b[39mpost_call(\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    877\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m    878\u001b[0m         original_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    879\u001b[0m         additional_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: headers},\n\u001b[0;32m    880\u001b[0m     )\n\u001b[1;32m--> 881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\main.py:855\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 855\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai_chat_completions\u001b[38;5;241m.\u001b[39mcompletion(\n\u001b[0;32m    856\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    857\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    858\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    859\u001b[0m         model_response\u001b[38;5;241m=\u001b[39mmodel_response,\n\u001b[0;32m    860\u001b[0m         print_verbose\u001b[38;5;241m=\u001b[39mprint_verbose,\n\u001b[0;32m    861\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m    862\u001b[0m         api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[0;32m    863\u001b[0m         acompletion\u001b[38;5;241m=\u001b[39macompletion,\n\u001b[0;32m    864\u001b[0m         logging_obj\u001b[38;5;241m=\u001b[39mlogging,\n\u001b[0;32m    865\u001b[0m         optional_params\u001b[38;5;241m=\u001b[39moptional_params,\n\u001b[0;32m    866\u001b[0m         litellm_params\u001b[38;5;241m=\u001b[39mlitellm_params,\n\u001b[0;32m    867\u001b[0m         logger_fn\u001b[38;5;241m=\u001b[39mlogger_fn,\n\u001b[0;32m    868\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    869\u001b[0m         custom_prompt_dict\u001b[38;5;241m=\u001b[39mcustom_prompt_dict,\n\u001b[0;32m    870\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,  \u001b[38;5;66;03m# pass AsyncOpenAI, OpenAI client\u001b[39;00m\n\u001b[0;32m    871\u001b[0m         organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[0;32m    872\u001b[0m     )\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\llms\\openai.py:369\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[1;34m(self, model_response, timeout, model, messages, print_verbose, api_key, api_base, acompletion, logging_obj, optional_params, litellm_params, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, message\u001b[38;5;241m=\u001b[39mtraceback\u001b[38;5;241m.\u001b[39mformat_exc())\n",
      "\u001b[1;31mOpenAIError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py\", line 67, in map_httpcore_exceptions\n    yield\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py\", line 231, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py\", line 99, in handle_request\n    raise exc\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py\", line 154, in _connect\n    stream = stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_backends\\sync.py\", line 152, in start_tls\n    with map_exceptions(exc_map):\n  File \"c:\\ProgramData\\anaconda3\\Lib\\contextlib.py\", line 155, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 952, in _request\n    response = self._client.send(\n               ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py\", line 915, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py\", line 943, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py\", line 980, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py\", line 1016, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py\", line 230, in handle_request\n    with map_httpcore_exceptions():\n  File \"c:\\ProgramData\\anaconda3\\Lib\\contextlib.py\", line 155, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py\", line 84, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\llms\\openai.py\", line 361, in completion\n    raise e\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\llms\\openai.py\", line 324, in completion\n    response = openai_client.chat.completions.create(**data, timeout=timeout)  # type: ignore\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\resources\\chat\\completions.py\", line 590, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 1240, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 921, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 976, in _request\n    return self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 1053, in _retry_request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 976, in _request\n    return self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 1053, in _retry_request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 986, in _request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m completion(\n\u001b[0;32m      3\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapikey,\n\u001b[0;32m      4\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://drchat.xyz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-16k\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     custom_llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is a cat?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m}],\n\u001b[0;32m      8\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\utils.py:2551\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2547\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2548\u001b[0m             liteDebuggerClient \u001b[38;5;129;01mand\u001b[39;00m liteDebuggerClient\u001b[38;5;241m.\u001b[39mdashboard_url \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m         ):  \u001b[38;5;66;03m# make it easy to get to the debugger logs if you've initialized it\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m             e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Check the log in your dashboard - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mliteDebuggerClient\u001b[38;5;241m.\u001b[39mdashboard_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\utils.py:2454\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2452\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2453\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m-> 2454\u001b[0m result \u001b[38;5;241m=\u001b[39m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2455\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m   2456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\main.py:1906\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   1903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m   1904\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1905\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[1;32m-> 1906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[0;32m   1907\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1908\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   1909\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[0;32m   1910\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1911\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\utils.py:7590\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[0;32m   7588\u001b[0m \u001b[38;5;66;03m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[39;00m\n\u001b[0;32m   7589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m-> 7590\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   7591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   7592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m original_exception\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\utils.py:6472\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[0;32m   6470\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6471\u001b[0m         exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 6472\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[0;32m   6473\u001b[0m             status_code\u001b[38;5;241m=\u001b[39moriginal_exception\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[0;32m   6474\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   6475\u001b[0m             llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   6476\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   6477\u001b[0m             request\u001b[38;5;241m=\u001b[39moriginal_exception\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[0;32m   6478\u001b[0m         )\n\u001b[0;32m   6479\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6480\u001b[0m     \u001b[38;5;66;03m# if no status code then it is an APIConnectionError: https://github.com/openai/openai-python#handling-errors\u001b[39;00m\n\u001b[0;32m   6481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(\n\u001b[0;32m   6482\u001b[0m         __cause__\u001b[38;5;241m=\u001b[39moriginal_exception\u001b[38;5;241m.\u001b[39m__cause__,\n\u001b[0;32m   6483\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   6484\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   6485\u001b[0m         request\u001b[38;5;241m=\u001b[39moriginal_exception\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[0;32m   6486\u001b[0m     )\n",
      "\u001b[1;31mAPIError\u001b[0m: OpenAIException - Traceback (most recent call last):\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py\", line 67, in map_httpcore_exceptions\n    yield\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py\", line 231, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py\", line 99, in handle_request\n    raise exc\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py\", line 154, in _connect\n    stream = stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_backends\\sync.py\", line 152, in start_tls\n    with map_exceptions(exc_map):\n  File \"c:\\ProgramData\\anaconda3\\Lib\\contextlib.py\", line 155, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 952, in _request\n    response = self._client.send(\n               ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py\", line 915, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py\", line 943, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py\", line 980, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py\", line 1016, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py\", line 230, in handle_request\n    with map_httpcore_exceptions():\n  File \"c:\\ProgramData\\anaconda3\\Lib\\contextlib.py\", line 155, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py\", line 84, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\llms\\openai.py\", line 361, in completion\n    raise e\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\litellm\\llms\\openai.py\", line 324, in completion\n    response = openai_client.chat.completions.create(**data, timeout=timeout)  # type: ignore\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\resources\\chat\\completions.py\", line 590, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 1240, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 921, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 976, in _request\n    return self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 1053, in _retry_request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 976, in _request\n    return self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 1053, in _retry_request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\adity_724nfxg\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py\", line 986, in _request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "response = completion(\n",
    "    api_key=apikey,\n",
    "    base_url=\"https://drchat.xyz\",\n",
    "    model = \"gpt-3.5-turbo-16k\",\n",
    "    custom_llm_provider=\"openai\",\n",
    "    messages = [{ \"content\": \"What is a cat?\",\"role\": \"user\"}],\n",
    "    temperature=0.5\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = input(\"Ask a research question!\")\n",
    "\n",
    "# Create multiple search queries\n",
    "search_split_prompt = f\"\"\"\n",
    "Your role is that of a researcher attempting to answer a question. Given a question from the user,\n",
    "your job is to come up with an ArXiv query that searches for the exact information needed to answer the question.\n",
    "\n",
    "You can include all syntax that involves including multiple terms, search by abstract, title, etc.\n",
    "\n",
    "Example 1:\n",
    "Given question: What are the ethical concerns associated with the use of facial recognition technology?\n",
    "Your Answer: (\"facial recognition technology\" OR \"facial recognition systems\" OR \"facial recognition software\") AND (\"ethical concerns\" OR \"ethical implications\" OR \"ethical issues\")\n",
    "\n",
    "Example 2:\n",
    "Given question: What are some prominent attention mechanisms for convolutional neural networks, and how are they used in the autonomous vehicle industry?\n",
    "Your Answer: all:\"attention mechanisms\" AND (\"convolutional neural networks\" OR \"CNN\") AND all:\"attention mechanisms\" AND (\"autonomous vehicles\" OR \"self-driving cars\")\n",
    "\n",
    "Question: {user_query},\n",
    "\n",
    "As shown above, your response should solely be an ArXiv Query, and nothing else.\n",
    "\n",
    "\"\"\"\n",
    "response = completion(\n",
    "    api_key=apikey,\n",
    "    base_url=\"https://drchat.xyz\",\n",
    "    model = \"gpt4-1106-preview\",\n",
    "    custom_llm_provider=\"openai\",\n",
    "    messages = [{ \"content\": search_split_prompt,\"role\": \"user\"}],\n",
    "    temperature=0.5\n",
    ")\n",
    "print(\"Response recieved\")\n",
    "print(response.choices[0].message.content)\n",
    "arxiv_queries_list = response.choices[0].message.content.split(\"|\")\n",
    "\n",
    "\n",
    "# Each element contains vector stores for each search query developed by LLM\n",
    "chunks_for_queries = []\n",
    "for q in arxiv_queries_list:\n",
    "    docs = ArxivLoader(query=q, load_max_docs=5).load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=350, chunk_overlap=50\n",
    "    )\n",
    "    chunked_documents = text_splitter.split_documents(docs)\n",
    "    chunks_for_queries.append(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(chunks_for_queries))\n",
    "print(len(chunks_for_queries[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Embedding Model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=apikey, base_url=\"https://drchat.xyz\")\n",
    "# Create Index- Load document chunks into the vectorstore\n",
    "vectorstore_list = []\n",
    "for x in chunks_for_queries:\n",
    "    faiss_vectorstore = FAISS.from_documents(\n",
    "        documents=x,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    print(type(faiss_vectorstore))\n",
    "    vectorstore_list.append(faiss_vectorstore)\n",
    "\n",
    "\n",
    "# Create a retriver and retrieve relevant documents for each vector store\n",
    "relevant_documents_list = []\n",
    "for x in vectorstore_list:\n",
    "    \n",
    "    relevant_documents = x.similarity_search(user_query, k = 5)\n",
    "    print(type(relevant_documents))\n",
    "    relevant_documents_list.append(relevant_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relevant_documents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt = f\"\"\"\n",
    "Given the following context: {relevant_documents_list}\n",
    "\n",
    "Answer the following question: {user_query}\n",
    "\n",
    "Only answer the question if the answer is in the context. Otherwise, say that you don't know.\n",
    "\"\"\"\n",
    "\n",
    "response = completion(\n",
    "    api_key=apikey,\n",
    "    base_url=\"https://drchat.xyz\",\n",
    "    model = \"gpt-3.5-turbo-16k\",\n",
    "    custom_llm_provider=\"openai\",\n",
    "    messages = [{ \"content\": question_prompt,\"role\": \"user\"}],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
