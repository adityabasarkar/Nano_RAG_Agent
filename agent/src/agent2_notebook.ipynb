{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk--lxq9ReUmWxgxJhWPPwRNg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import litellm\n",
    "from litellm import completion\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load API keys\n",
    "load_dotenv(\".env\")\n",
    "apikey = os.getenv('OPENAI_API_KEY')\n",
    "print(apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Request Successful\n",
      "Response: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3ARetrieval%20Augmented%20Generation%26id_list%3D%26start%3D0%26max_results%3D5\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=all:Retrieval Augmented Generation&amp;id_list=&amp;start=0&amp;max_results=5</title>\n",
      "  <id>http://arxiv.org/api/UOKvU43HASfzk48jVrFyBsmll1A</id>\n",
      "  <updated>2024-05-23T00:00:00-04:00</updat\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Example query to the arXiv API directly\n",
    "url = \"http://export.arxiv.org/api/query\"\n",
    "params = {\n",
    "    \"search_query\": \"all:Retrieval Augmented Generation\",\n",
    "    \"start\": 0,\n",
    "    \"max_results\": 5\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"API Request Successful\")\n",
    "    print(\"Response:\", response.text[:500])  # Print first 500 characters of the response\n",
    "else:\n",
    "    print(\"Failed to fetch data from arXiv:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='1\\nPulmonary Disease Classiﬁcation Using Globally\\nCorrelated Maximum Likelihood:\\nan Auxiliary Attention mechanism for\\nConvolutional Neural Networks\\nEdward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, and Faraz Hussain\\nAbstract—Convolutional neural networks (CNN) are now being\\nwidely used for classiﬁying and detecting pulmonary abnormal-\\nities in chest radiographs. Two complementary generalization\\nproperties of CNNs, translation invariance and equivariance,\\nare particularly useful in detecting manifested abnormalities\\nassociated with pulmonary disease, regardless of their spatial\\nlocations within the image. However, these properties also come\\nwith the loss of exact spatial information and global relative\\npositions of abnormalities detected in local regions. Global\\nrelative positions of such abnormalities may help distinguish\\nsimilar conditions, such as COVID-19 and viral pneumonia. In\\nsuch instances, a global attention mechanism is needed, which\\nCNNs do not support in their traditional architectures that\\naim for generalization afforded by translation invariance and\\nequivariance. Vision Transformers provide a global attention\\nmechanism, but lack translation invariance and equivariance,\\nrequiring signiﬁcantly more training data samples to match\\ngeneralization of CNNs. To address the loss of spatial information\\nand global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that\\nserves as an auxiliary attention mechanism to existing CNN\\narchitectures, in order to extract global correlations between\\nsalient features.\\nImpact Statement—We improve sensitivity of Convolutional', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='serves as an auxiliary attention mechanism to existing CNN\\narchitectures, in order to extract global correlations between\\nsalient features.\\nImpact Statement—We improve sensitivity of Convolutional\\nNeural Networks (CNNs) using an auxiliary global attention\\nmechanism (GCML) that enables CNNs to utilize global spatial\\ninformation similar to Vision Transformers (ViTs). Our tech-\\nnique retains the beneﬁts of spatial invariance and equivariance\\ninherent to CNNs, while allowing spatial information of features\\nto be used as discriminators. GCML retains these inductive\\nbiases in data starved environments, which ViTs lack due their\\narchitecture, and hence require signiﬁcantly more training data\\nto achieve a similar level of generalization. Finally, we show\\nimpirically, that GCML improves the sensitivity of standard\\nCNNs when classifying pulmonary conditions in chest X-rays. We\\nprovide all associated code, data, and models for reproducibility\\nand improvement through further research.\\nIndex Terms—COVID-19 detection, convolutional neural net-\\nworks, global attention mechanism, data starved environment.\\nI. INTRODUCTION\\nW\\nITH the emergence of the COVID-19 pandemic, the\\nuse of Convolutional Neural Networks (CNNs) to\\ndetect presence of pulmonary diseases in medical imagery\\nhas quickly gained momentum, where a signiﬁcant majority\\nof approaches center around ﬁne-tuning pre-trained CNNs on', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='detect presence of pulmonary diseases in medical imagery\\nhas quickly gained momentum, where a signiﬁcant majority\\nof approaches center around ﬁne-tuning pre-trained CNNs on\\nnew data, as reported by Roberts et al. [1]. The beneﬁts of\\nutilizing such techniques are intuitive, including the ability\\nof CNNs to detect features that are difﬁcult for humans to\\nidentify, learning certain correlations between positive cases,\\nand the speed with which such predictions can be made in\\norder to aid in timely diagnosis. CNNs have been shown\\nto outperform individual radiologists in detecting pneumonia\\nin chest X-rays as reported by Rajpurkar et al. [2]. In that\\nwork, X-ray based pneumonia diagnoses made by a group\\nof four radiologists were compared to a custom CNN, using\\nthe F1 metric, where only one radiologist performed better\\nthan the model. This result prompted us to explore a possible\\nmechasism of visual analysis of X-rays by a radiologist who\\noutperformed the CNN, and whether that might translate into\\nmore accurate CNNs for classiﬁcation of pulmonary diseases.\\nOur hypothesis is that while human radiologists may not\\nbe as effective as CNNs at identifying individual salient fea-\\ntures, their ability to quickly consider global spatial relations\\nbetween those features may play a factor. Our intution for\\nthis came from recent work by Borghesi et al. [3], where\\nan experimental scoring system for chest X-rays was used\\nby radiologists to quantify and monitor disease progression', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='this came from recent work by Borghesi et al. [3], where\\nan experimental scoring system for chest X-rays was used\\nby radiologists to quantify and monitor disease progression\\nin COVID-19 patients. The scoring worked by dividing the\\nfrontal X-ray of the lungs into six zones, where each zone\\nwas assessed with a quantitative score ranging from 0, rep-\\nresenting no lung abnormalities, through 3, representing the\\nhighest severity of abnormalities. To obtain the ﬁnal score,\\neach region’s scores were summed, and used as a measure\\nof COVID-19 severity on the lungs. Our idea is to enable\\nthe CNN model to track abnormalities and their relations to\\neach other across regions, and base our predictions on these\\nlearned spatial relationships and not just a cummulative score,\\nsimilar to what a human subject matter expert might do to\\ndistinguish different diseases.\\nIn the remainder of this section, we brieﬂy describe how im-\\nage classiﬁcation using CNNs works, including their strengths\\nand limitations, with the goal to introduce the notion of\\nadditive classiﬁcation that we argue standard CNNs perform.\\nWe then discuss how certain positive generalization properties\\nof CNNs limit their ability to account for global spatial\\ncorrelations between regions of an image, thereby lacking the\\nability to utilize positional information as a discriminating\\nfactor.\\narXiv:2109.00573v1  [cs.CV]  1 Sep 2021\\n2\\n(a) CNN Equavariance\\n(b) CNN Invariance', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='factor.\\narXiv:2109.00573v1  [cs.CV]  1 Sep 2021\\n2\\n(a) CNN Equavariance\\n(b) CNN Invariance\\n(d) GCML Attention\\n(c) CNN Multi-object\\nFig. 1: Examples of inductive biases inherent to Convolutional Neural Networks (a,b,c). Small translational shifts in a target\\nobject (black circle) within the receptive ﬁeld of the kernel (dotted square) will shift the activation equally, and will still\\nclassify the target correctly due to (a) translation equivariance. Major translational shift of the target object (black circle) to\\na new spatial position within the image will also result in the correct classiﬁcation, regardless of its global position, due to\\n(b) translational invariance. Due to the scoring function, presence of multiple target objects (c) within the image will result\\nin a classiﬁcation that the object is present in the image, regardless of quantity or spatial locations of the objects within the\\nimage. Our GCML attention mechansim (d) uses spatial information of features, and their global interrelations, to distinguish\\nbetween classes that can exhibit the same features in different locations, similar to Vision Transformers (ViTs) [4], but with\\nsigniﬁcantly fewer training samples needed by ViTs to achieve similar generalization. GCML achieves this by not performing\\ntokenization of input images, but rather tokenizing class activation maps generated by the CNN with respect to the class of\\ninput images.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='tokenization of input images, but rather tokenizing class activation maps generated by the CNN with respect to the class of\\ninput images.\\nA. Image Classiﬁcation with CNNs\\nYamashita et al. [5] provide an excellent overview of\\nCNNs and their application in radiology, while we provide\\na general summary. In a feed forward Convolutional Neural\\nNetwork, image classiﬁcation is performed by propogating\\ninput through a series of convolutional layers, where ﬁlters\\nthat were previously trained to recognize speciﬁc features of an\\nimage get activated and are used as input to subsequent layers.\\nTypically, this process involves downsampling or reducing the\\nmapping resolution of activated output with respect to input, as\\nimage data moves further through the layers. This is done by\\nutilizing convolutional ﬁlters of size k > 1, but signiﬁcantly\\nless than the size of the input. By learning local correlation\\nstructures within the bounds of the window deﬁned by k,\\nparticular convolutional layers learn speciﬁc local features.\\nDuring inference, this allows the network to recognize objects\\nor larger features, that are compositions of smaller features,\\nwhich were recognized by earlier layers. After the last convo-\\nlutional layer, 2 dimensional activation maps are ﬂattened to\\n1 dimensional data and are passed to a fully connected layer\\nthat tallies the contributions of activated ﬁlters with respect to', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='lutional layer, 2 dimensional activation maps are ﬂattened to\\n1 dimensional data and are passed to a fully connected layer\\nthat tallies the contributions of activated ﬁlters with respect to\\nclasses represented in the ﬁnal connected layer. A classiﬁcation\\ncan then be made by taking the maximum tally, or top K\\ntallies for top K classiﬁcation. Additional layers, such as a\\nlayer representing a SoftMax function, can be utilized after\\nthe ﬁnal connected layer to obtain class probabilities from the\\njoint distribution of represented classes, but the core process\\ndescribed above is the same.\\nThe use of convolutions in CNNs provides two important\\ngeneralization properties with respect to image classiﬁcation,\\ntranslation equavarience and invariance, where a convolution\\nis equavarient to translation if an object in an image is spatially\\nshifted, convolution’s output is equally shifted. Invariance is a\\nresult of a position-independent pooling operation that follows\\na convolution, and results in a loss of absolute location, but\\nenables the visual inductive prior of convolutional operators\\n[6]. At a higher level, they enable features to be reliably\\ndetected regardless of their spatial location within an input\\nimage.\\nCNNs became the go-to method for image classiﬁcation\\nafter Krizhevsky et al. [7] published their results on ImageNet\\n[8]. Within the medical domain, CNNs started to be em-', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='after Krizhevsky et al. [7] published their results on ImageNet\\n[8]. Within the medical domain, CNNs started to be em-\\nployed for diagnostic assistance of pulmonary diseases through\\nclassiﬁcation of medical imagery, including CT scans and\\nX-rays [9] [10]. With the rise of the COVID-19 pandemic,\\nmuch work has been done in applying CNNs for image\\nclassiﬁcation of medical imagery in order to rapidly detect\\npulmonary manifestations of COVID-19 in chest X-rays [11].\\nMuch of this work, which we further discuss in Section II,\\nutilizes CNNs for classifying chest X-rays in a manner we\\ndescribed above. For the purposes of this work, we refer to\\nthis method of image classiﬁcation as standard or additive,\\nbecause classiﬁcation is performed by summing contributions\\nof various ﬁlters at the ﬁnal connected layer of the network.\\nB. Limitations of Image Classiﬁcation with CNNs\\nEven with state-of-the-art results in image classiﬁcation and\\nobject detection, CNNs have certain limitations relevant to\\nimage classiﬁcation in certain domains. For example, Hosseini\\net al. [12] report degraded image classiﬁcation performance on\\nimages with reversed brightness, or negative images. This may\\nbe mitigated by proper pre-processing of data, but it may signal\\nthat color channel information or texture may be learned by', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='images with reversed brightness, or negative images. This may\\nbe mitigated by proper pre-processing of data, but it may signal\\nthat color channel information or texture may be learned by\\nthe network, along with shapes. Although not a limitation in\\nitself, it introduces additional considerations when evaluating\\nnetwork performance on new and out-of-distribution data.\\nThe main limitation that we address in this work has to\\ndo with the localized perceptive ﬁelds of convolutional ﬁlters.\\nAlthough CNNs exhibit translational invariance and equivari-\\nance, which improves generalization in many instances, it can\\n3\\nalso hurt generalization through loss of spatial information\\nwithin the pooling layers of the CNN [13]. The canonical\\nexample of this problem relates to classifying an image of\\na human face, where features such as nose, eyes, lips are\\nidentiﬁed, yet placing them in different parts of the image\\nin a manner that does not resemble a face can still yield\\na face classiﬁcation [14]. Relating this to the chest radio-\\ngraphs domain, small manifestations of pulmonary disease are\\nidentiﬁed by a CNN, but their spatial interrelationships are\\nmostly lost. In the natural imagery and radiology domains, the\\nloss of spatial information, due to the contraints imposed by\\nconvolutional ﬁlter size and pooling, can result in incorrect\\nclassiﬁcations. Figure 1 provides a visual intuition to the\\ninductive biases that CNNs exhibit in terms of identiﬁying', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='classiﬁcations. Figure 1 provides a visual intuition to the\\ninductive biases that CNNs exhibit in terms of identiﬁying\\ntarget objects or certain features, for example abnormalities\\nin X-rays. While CNNs are particularly good at detecting\\nabnormalities regardless of their spatial locations, the scoring\\nfunction used for classiﬁcation in standard CNNs, shown in\\nEq 1, does not consider spatial locations nor spatial relations\\nbetween features as discriminators. We show in this work, that\\naccounting for global spatial interrelation of features improves\\nclassiﬁcation of pulmonary conditions.\\nC. Contribution\\nOur main goal in this work is to provide empirical evidence\\nto the hypothesis that accounting for global correlations be-\\ntween activated regions of an image improves classiﬁcation of\\npulmonary conditions in chest radiographs. This improvement\\nalso extends to image classiﬁcation domains where discrim-\\ninating a target class involves accounting for global spatial\\ncorrelations between features. Our secondary goal is to show\\nthat the classiﬁcation approach, using our novel Globally\\nCorrelated Maximum Likelihood (GCML) auxiliary attention\\nmechanism, is competitive to standard CNN classiﬁcation,\\nwhile utilizing signiﬁcantly fewer model parameters, less\\ncomputational resources, and much fewer training samples.\\nTo that end, our contributions are the following:\\n• We developed a novel auxiliary attention mechanism,\\nGCML, that is utilized with existing Convolutional Neural\\nNetwork architectures in order to account for global spa-', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='To that end, our contributions are the following:\\n• We developed a novel auxiliary attention mechanism,\\nGCML, that is utilized with existing Convolutional Neural\\nNetwork architectures in order to account for global spa-\\ntial correlations between salient features of represented\\nclasses.\\n• Using GCML, we show competitive results for image\\nclassiﬁcation on a benchmark dataset, CIFAR-10, using\\nsigniﬁcantly less model parameters and computation,\\nwhile not utilizing any pre-training on samples in relevant\\ndomains.\\n• We show that by utilizing the GCML attention mechanism,\\nwe improve image classiﬁcation, speciﬁcally increasing\\nmodel sensitivity or recall of pulmonary diseases in chest\\nradiographs. This also includes effective generalization\\non a previously unseen dataset, suggesting that the GCML\\nattention mechanism improves and complements visual\\ninductive priors learned by CNNs by accounting for\\nspatial relations.\\n• Our results show that standard CNN and our GCML tech-\\nnique for image classiﬁcation have particular strengths,\\nand show potential utility when utilized as ensemble\\nmethods.\\n• Finally, we provide an open source1 reference implemen-\\ntation of our technique, allowing further research into its\\nimprovement and utility.\\nThe remainder of this paper is structured as follows. Section\\nII describes work related to image classiﬁcation of pulmonary\\ndiseases using medical imagery. Section III outlines our ap-\\nproach, while brieﬂy discussing work relevant to attention', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='II describes work related to image classiﬁcation of pulmonary\\ndiseases using medical imagery. Section III outlines our ap-\\nproach, while brieﬂy discussing work relevant to attention\\nmechanisms. We report results of our experiments in Section\\nIV, including a benchmark dataset of natural imagery and two\\nseparate datasets of chest radiographs. Finally we conclude by\\ndiscussing our results, limitations of our approach, and future\\nresearch directions.\\nII. RELATED WORK\\nIn this section we describe work related to pulmonary\\ndisease classiﬁcation using chest X-rays or CT scans. Some\\napproaches mentioned also include forms of weakly super-\\nvised localization, which in contrast to object detection in\\nimagery, does not require that training labels be accompanied\\nwith spatial coordinates of objects to be detected. These types\\nof training labels are also referred to as image level labels, as\\nthey only provide information on whether certain target classes\\nare present in the image, not their spatial locations. To the best\\nof our knowledge and at the time of this work, CNN attention\\nmechanisms have not been used for the purpose of improving\\nimage classiﬁcation of pulmonary diseases.\\nRahaman et al [15] employed transfer learning to ﬁne-tune\\nseveral CNN architectures to classify chest X-ray images into\\nthree classes: COVID-19, Healthy, and Pneumonia. A total\\nof 860 images were used in their study, which reported the\\nVGG19 [16] architecture having the best performance achiev-', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='three classes: COVID-19, Healthy, and Pneumonia. A total\\nof 860 images were used in their study, which reported the\\nVGG19 [16] architecture having the best performance achiev-\\ning an accuracy of 89.3 percent, average precision of 0.90, a\\nrecall of 0.89, and F1 score of 0.90. Khan et al. [17] proposed\\nCoroNet, a network based on the Xception [18] architecture\\nfor diagnosis of COVID-19 from chest X-rays. They used a\\ndataset of X-ray images [19] to train their network, achieving\\n95 percent accuracy when classifying images for COVID-\\n19, Normal, and Pneumonia. Tamal et al. [20] used their\\nradiology classiﬁcation model for COVID-19 classiﬁcation\\nwith low severity, as scored by radiologists, on new data from\\npatients at a local hospital, showing generalization to out-\\nof-distribution data and achieving an overall accuracy of 90\\npercent. Kim et al. [21] reported relevant ﬁndings on the effect\\nof dataset composition practices on classiﬁcation performance.\\nThe authors reported that higher classiﬁcation performance\\nwas observed on datasets where data composing each class\\ncame from different sources. Heidari et al. [22] showed\\nthat their image preprocessing scheme improved pulmonary\\ndisease classiﬁcation in X-ray images. Similar diagnostic aid', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='came from different sources. Heidari et al. [22] showed\\nthat their image preprocessing scheme improved pulmonary\\ndisease classiﬁcation in X-ray images. Similar diagnostic aid\\napproaches to classifying COVID-19 were reported in [23],\\n[24], and [25], where the use of transer learning using a pre-\\ntrained Convolutional Neural Network architecture to perform\\nmulti-class classiﬁcation was the common factor.\\n1https://gitlab.com/verenich/gcmlpub\\n4\\nIn addition to classifying images as representing pulmonary\\nconditions, work has been proposed to provide some explain-\\nability of those classiﬁcations. Tsiknakis et al. [26] utilized\\nthe Inception [27] architecture to ﬁrst classify X-ray images\\ninto speciﬁc diseases, and then applied a weakly supervised\\nlocalization technique GradCAM [28] to identify regions\\nwithin the image responsible for a particular classiﬁcation.\\nWang et al. [29] proposed a similar approach to diagnose and\\nlocalize disease maniﬁstation in X-rays. Verenich et al. [30]\\nproposed a method to reduce aleatoric uncertainty in weakly\\nsupervised localization that can arise from signiﬁcant class\\noverlap between features associated with similar pulmonary\\ndiseases. Gupta et al. [31] proposed an approach that classiﬁes\\nand performs weakly supervised localization using standard', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='overlap between features associated with similar pulmonary\\ndiseases. Gupta et al. [31] proposed an approach that classiﬁes\\nand performs weakly supervised localization using standard\\nClass Activation Maps [32].\\nRecently, Transformers, originally proposed by Vaswani et\\nal. [33], have shown state-of-the-art performance on natural\\nlanguage processing tasks. These ideas have been utilized in\\nVision Transformers (ViT) [4] to introduce positional attention\\nmechanisms for image classiﬁcation, attaining results com-\\nparable to state-of-the-art CNNs. However, ViTs lack trans-\\nlational invariance and equavarience of CNNs, thus require\\nsigniﬁcanly more training data to generalize [4]. To the best\\nof our knowledge, and at the time of this work, ViTs have\\nnot been used for classiﬁcation of pulmonary diseases using\\nX-rays. One possible reason for this is insufﬁcient amount of\\ntraining data, to achieve same levels of generalization as CNNs\\nwith the data that is currently available.\\nIII. APPROACH: GCML ATTENTION MECHANISM\\nThe goal of our attention mechanism is to preserve spatial\\ninterrelationships of activated regions in a given image I\\nrelative to target classes C learned by a convolutional neu-\\nral network G. Our hypothesis is that localized pulmonary\\nabnormalities, detected by the convolutional neural network\\nin different regions, can yield additional discriminative power', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='ral network G. Our hypothesis is that localized pulmonary\\nabnormalities, detected by the convolutional neural network\\nin different regions, can yield additional discriminative power\\nwhen their global interrelationships are considered. The main\\nintuition for this is that translation invariance and equivariance\\nproperties of the CNNs result in class discrimination that is\\nadditive, with respect to activation strengths of convolutional\\nﬁlters, but not their spatial relation to each other. This section\\ndescribes the architecture of our approach, that accounts for\\nspatial interrelations of salient features.\\nA. Attention Type\\nOur technique explores complementing convolutional neural\\nnetworks with attention mechanisms. The main difference\\nbetween prior work [34]–[37] and GCML is that we do not\\nalter the architecture of the CNN using self-attention layers,\\nbut instead provide a separate auxiliary structure that is created\\nusing a pre-trained network. In addition, the stochastic struc-\\nture of GCML does not require it to be trained simultaneously\\nwith the CNN using backpropagation as in [37], thus it does\\nnot have to be differentiable and is signiﬁcantly faster to\\ntrain. Finally, tokenization of input images into patches is\\nalso not required, instead we use downsampled output of the\\nlast convolutional layer, scaled by class weights from the ﬁnal\\nconnected layer, to generate class activation maps, as proposed\\nby Zhou et al. [32]. These class activation maps are used\\nas input to our GCML attention mechanism. In addition to', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='connected layer, to generate class activation maps, as proposed\\nby Zhou et al. [32]. These class activation maps are used\\nas input to our GCML attention mechanism. In addition to\\nbeing effective at performing weakly supervised localization,\\nclass activation maps were also used to distinguish overlapping\\nregions of images that may belong to different but overlapping\\nclasses [30].\\nThe work that is in closest alignment with our approach\\nis the UL-Hopﬁeld model [38], which uses pre-trained CNNs\\nwith an associative memory bank to perform image classiﬁ-\\ncation. Our approach differs in several ways: the UL-Hopﬁeld\\nauxiliary memory learns class-speciﬁc core patterns from a\\npre-trained CNN in an unsupervised manner, while we use\\nthe CNN to generate a pattern using training labels. Second,\\nthe type of input to the attention function that is used to train\\ntheir memory structure is extracted from the last pooling layer\\nwithout weighting activated feature maps by a speciﬁc class,\\nwhich is done in our approach during training. Finally, for\\nexperiments we use a CNN with 42x less parameters than the\\nCNN they use for feature extraction, as we discuss in Section\\nIV.\\nB. Input Features\\nFormally, to compute the attention function input tensor\\nMc, where c is a class represented in the CNN, we do the\\nfollowing: given an input image, let fk(x, y) be the activation\\nof ﬁlter k at the last convolutional layer of of a CNN G and', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='following: given an input image, let fk(x, y) be the activation\\nof ﬁlter k at the last convolutional layer of of a CNN G and\\n(x, y) be the spatial location. During classiﬁcation, for each\\nﬁlter k, a pooling layer outputs a global average Fk deﬁned\\nas P\\nx,y fk(x, y), which is then used as input to the fully\\nconnected layer, giving us the class score Sc in Eq 1, where\\nwc\\nk is a scalar weight indicating the importance of Fk to class\\nc.\\nSc =\\nX\\nk\\nwc\\nkFk\\n(1)\\nTo compute each spatial element of the class activation map\\n[32], or 2-dimensional tensor Mc, we use\\nMc(x, y) =\\nX\\nk\\nwc\\nkfk(x, y)\\n(2)\\nThe resulting tensor Mc effectively splits the input image I\\ninto regions or tokens, where entries M c\\ni,j represent activation\\nintensities of those regions for class c.\\nOur technique would be equally applicable to be used with\\nclass activation maps generated by another weakly supervised\\nlocalization technique called Grad-CAM, proposed by Sel-\\nvaraju et al. [28]. It requires that we compute the gradients of\\noutput of the network with respect to feature map activations\\nfor each class c, making it slower than the standard CAM\\nmethod for the purposes of training the GCML structure on\\nlarge datasets.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='output of the network with respect to feature map activations\\nfor each class c, making it slower than the standard CAM\\nmethod for the purposes of training the GCML structure on\\nlarge datasets.\\nC. Attention Function\\nSimilar to the original work on transformers [33], our\\nattention function is a mapping of a query Q, based on a\\n5\\nFig. 2: Example of a 2-dimensional tensor M, which is the\\nsame as the class activation map, that is used as an input to\\nthe attention function Q. The dimensions of M are dictated by\\nthe mapping resolution of the convolutional layer that we use\\nto compute the class activation map. Values at Mi,j represent\\nactivation intensities for class C after an image I is passed\\nthrough the network G.\\nFig. 3: Example of a 2-dimensional tensor M in its interme-\\ndiate state after normalization and application of threshold τ\\nas computed within attention function Q. Hyperparameter τ is\\nalso optimized during the training stage of the GCML structure\\nand its optimized value is persisted for inference.\\ndatastore key K to a retrieved value V . In our case, the query\\nfunction Q takes as input a real tensor M of size H×W, which\\nmaps it to a datastore key K, to retrieve likelihood value V . In\\ntraining mode, when learning the GCML structure, key K is\\nused to update the value at that index, while during inference\\nmode, it is used to retrieve V at position K. In both cases,\\nattention function Q remains the same. The states of input,', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='used to update the value at that index, while during inference\\nmode, it is used to retrieve V at position K. In both cases,\\nattention function Q remains the same. The states of input,\\nintermediate states, and output of Q are illustrated in Figures\\n2, 3, and 4. Parameter τ is used as a threshold to convert input\\nMc to its intermediate state shown in Fig 3 using Equation (3).\\nfτ(Mi,j) =\\n(\\n1,\\nif Mi,j ≥τ\\n0,\\notherwise\\n(3)\\nThe full algorithm for obtaining datastore key K from tensor\\nMc using the attention function Q is described in Algorithm\\n1.\\nFig. 4: Intermediate state of tensor M is ﬂattened to a vector\\nB where entries Bi represent bits that are set to 0 or 1.\\nConverting this vector of bits to an integer yields our datastore\\nkey K, which would be 144 in the vector displayed. This value\\nis the output of the attention function Q. Note that given the\\nlength L of vector B, the number of possible entries Ki is\\n2L. Also, either big or little endianness can be used for bit\\narrangment, as long as it is consistent throughout training and\\ninference.\\nAlgorithm 1 Attention function Q\\n1: procedure Q(Mc)\\n▷Computes K from Mc\\n2:\\nM ′\\nc ←normalize(Mc)\\n3:\\nM ′′\\nc ←fτ(M ′\\nc)\\n▷Threshold τ\\n4:', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='2:\\nM ′\\nc ←normalize(Mc)\\n3:\\nM ′′\\nc ←fτ(M ′\\nc)\\n▷Threshold τ\\n4:\\nB ←flatten(M ′′\\nc )\\n▷From 2-d to 1-d\\n5:\\nK ←binToInt(B)\\n6:\\nreturn K\\n▷Datastore key K\\n7: end procedure\\nD. Training GCML\\nThe GCML datastore S is a tensor with dimensions C × P,\\nwhere C is the number of classes represented in network G and\\nP is of size 2L, where L is the length of the ﬂattened vector B\\nfrom which key K is derived. For a given class y, and possible\\nactivations x of M y\\ni,j, being in on or off states, each row of S\\nrepresents a discrete Conditional Probability Distribution of\\nclass y given activations x, as shown in Equation (4).\\nS[y] = P(y|x0, y|x1, . . . , y|(x0, x1, . . . , xL))\\n(4)\\nTo compute these likelihood distributions for all classes, we\\nutilize a fully trained convolutional neural network G along\\nwith the training data set DT , which was used to train G.\\nAlgorithm 2 GCML training procedure (1 epoch)\\n1: procedure UPDATE(S, DT , G) ▷Update likelihoods in S\\n2:', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='with the training data set DT , which was used to train G.\\nAlgorithm 2 GCML training procedure (1 epoch)\\n1: procedure UPDATE(S, DT , G) ▷Update likelihoods in S\\n2:\\nfor i, lc in DT do\\n▷Image i, with label lc\\n3:\\nMc ←Gm(i, lc) ▷Compute Mc via G and Eq 2\\n4:\\nK ←Q(Mc)\\n▷Compute key K\\n5:\\nS[lc][K] ←+1\\n▷Increment state\\n6:\\nend for\\n7:\\nS ←normalize(S)\\n▷Optional normalization\\n8:\\nreturn S\\n▷GCML store S\\n9: end procedure\\nAlgorithm 2 shows the training procedure for the GCML\\ndatastore. For simplicity, we show the procedure for single\\nimages sequentially. In practice however, we implement these\\nprocedures on batches of images provided by a dataloader.\\nWe also note that none of the weights in network G are being\\nupdated during this procedure, and the network is set to eval-\\nuation mode. The procedure Gm(i, lc) involves propagating\\n6\\nthe image through network G, and computing Mc for that\\nimage using class label lc. Finally, the normalization procedure\\nof S is marked as optional because this normalization can\\nalso happen before inference using GCML is performed. One\\nreason to hold off on normalization, is to allow S to be further', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='of S is marked as optional because this normalization can\\nalso happen before inference using GCML is performed. One\\nreason to hold off on normalization, is to allow S to be further\\ntrained on additional data, as we will discuss later in this paper.\\nAnother consdideration when training the GCML structure\\nis to account for data transformations that were performed\\nwhile training the original network G. For example random\\ncrops or ﬂips along a horizontal or vertical axes may alter\\nthe class activation map, thus using more epochs with random\\ntransformations should improve the GCML structure in some\\ndomains, such as natural imagery of CIFAR-10. We note\\nhowever, that in a domain such as chest radiology, major trans-\\nformations such as ﬂips will actually degrade performance as\\norientation of X-ray images is consistent and generalizing to\\nsuch tranformations is not needed and is actually harmful.\\nTherefore, in order to improve generalization of attention\\nmechanisms, appropriate transformations should be considered\\nbased on the target domains.\\nE. Inference with GCML\\nTo perform inference using the GCML attention mecha-\\nnism we utilize a trained Convolutional Neural Network G\\nalong with the GCML datastore S. As mentioned earlier, the\\nattention function Q remains the same during training and\\ninference, the main difference is that during inference we\\ncompute Mc for all classes represented in G instead of just\\nthe class label provided with training data. Additionally, before\\ninference is performed, we must make sure that the datastore S', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='compute Mc for all classes represented in G instead of just\\nthe class label provided with training data. Additionally, before\\ninference is performed, we must make sure that the datastore S\\nis normalized, as the normalization step during training shown\\non line 7 in Algorithm 2 is optional, in order to enable further\\ntraining.\\nAlgorithm 3 GCML inference procedure\\n1: procedure PREDICT(I, G, S) ▷Predict class on image I\\n2:\\n⃗\\nMc ←G(I)\\n▷Tensors Mc for all classes in G\\n3:\\n⃗\\nKc ←Q( ⃗\\nMc)\\n▷Compute K for all classes in G\\n4:\\n⃗\\nVc ←lookup(S, ⃗\\nKc)\\n▷Get class likelihoods\\n5:\\nCP ←argmax( ⃗\\nVc)\\n▷Get Max Likelihood class\\n6:\\nreturn CP\\n▷Return predicted class\\n7: end procedure\\nAlgorithm 3 shows the inference procedure of our approach,\\nwhich expects datastore S to be normalized. For each image\\nI, we propagate it through the convolutional neural network\\nG, where we compute inputs ⃗\\nMc to the attention function Q,\\nwhere each item in ⃗\\nMc is a class activation map computed\\nfor each class represented in G, given input I. In other words,\\nsingle input I will generate N inputs to the attention function,\\nwhere N is the number of classes in G. The next step is', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='for each class represented in G, given input I. In other words,\\nsingle input I will generate N inputs to the attention function,\\nwhere N is the number of classes in G. The next step is\\nto compute vector ⃗\\nKc, where each entry Ki is a key to a\\nlikelihood value representing class i. Class likelihood values\\n⃗\\nVc are then retrieved from S and maximum value is taken to\\ndetermine the class that input I belongs to.\\nFigure 5 shows a full view of the inference procedure\\nusing the GCML attention mechanism, as well as the standard\\ninference process using the convolutional neural network.\\nAs shown in the diagram, in addition to two classiﬁcations,\\nlabeled (a) for standard CNN classiﬁcation and (b) for classiﬁ-\\ncation using the attention mechanism, Q function input tensors\\n⃗\\nMc can also be used to perform weakly supervised localization\\n(c) by upsampling them to the same size as the input image\\nto produce a heatmap of relevant regions for a given class.\\nF. On Attention Input Size\\nThe dimensions of the input Mc to the attention function\\nQ are determined by the ﬁnal mapping resolution of the last\\nconvolutional layer of network G. For example, ResNet50, a\\nversion of a widely used convolutional architecture utilizing\\nresidual layers [39], has a ﬁnal mapping resolution of 7 × 7,\\nwhen used with input images that are 224 × 224. This would', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='version of a widely used convolutional architecture utilizing\\nresidual layers [39], has a ﬁnal mapping resolution of 7 × 7,\\nwhen used with input images that are 224 × 224. This would\\nresult in GCML datastores with 249 entries for each class,\\nas this is the number of possible binary combinations of\\nthreshold activated regions within Mc of that size. Instead, we\\ndownsample the ﬁnal resolution layer to a more manageable\\nsize, 4 × 4 and 5 × 5 as we will discuss in Section IV.\\nIV. EXPERIMENTS\\nIn this section we report the results of using our GCML\\nattention mechanism to perform image classiﬁcation. We per-\\nformed experiments on three datasets: (1) dataset of natural\\nimagery CIFAR10 [40], (2) COVID-19 radiology dataset [41]\\ncontaining X-ray imagery of patients diagnosed with COVID-\\n19, viral pneumonia, and no ﬁndings, (3) dataset of COVID-\\n19, pneumonia, and no ﬁndings images taken from [19]. The\\npurpose of the third dataset is to assess generalization of our\\nmethod to new data.\\nThe CIFAR10 dataset contains 60000 32 × 32 images rep-\\nresenting 10 mutually exclusive classes, meaning each image\\nbelongs to only one class, which are: airplane, automobile,\\nbird, cat, deer, dog, frog, horse, ship, and truck. Dataset authors\\nnote that automobile and truck classes do not overlap. The', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='belongs to only one class, which are: airplane, automobile,\\nbird, cat, deer, dog, frog, horse, ship, and truck. Dataset authors\\nnote that automobile and truck classes do not overlap. The\\nmain reason we use CIFAR10 for our experiments is that\\nthe authors of the approach most similar to ours [38] provide\\nextensive evaluation results, as well as being the largest dataset\\nevaluated in that work.\\nAs the main data for our experiments, the COVID-19\\nradiology dataset [41] contains a total of 2905 images, where\\nCOVID-19 cases are only represented in 219 of them, with the\\nrest evenly distributed between viral pneumonia and images\\nwith no ﬁndings. This data set is particularly interesting to\\nus as it represents both class imbalance and a data starved\\nenvironment, as in such cases vision transformers do not out-\\nperform state-of-the-art CNN models [4] without pre-training\\non very large datasets in the similar domain.\\nTo remain within reasonably accessible hardware con-\\nstraints, all of our experiments were performed on a single\\nmachine with the following hardware characteristics: Intel\\nCore i7 CPU, 64 GiB of RAM, NVIDIA RTX 2080 GPU.\\nWe used Ubuntu 20 as the operating system, NVIDIA CUDA\\n11 GPU acceleration library, and Pytorch 1.9.0 as our model\\nimplementation framework.\\n7\\nFig. 5: Inference ﬂow using a Convolutional Neural Network G and the auxiliary attention mechanism GCML. For a given', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='implementation framework.\\n7\\nFig. 5: Inference ﬂow using a Convolutional Neural Network G and the auxiliary attention mechanism GCML. For a given\\ninput image (I), outputs (a) and (b) represent classiﬁcations made by the Convolutional Neural Network through its standard\\nclassiﬁcation layer (a), and using the GCML attention mechanism (b). Both classiﬁcations can be utlized as an ensemble in a\\nsingle forward pass during inference. In addition, input tensor Mc, to the attention function Q, can be upsampled using bilinear\\nsampling to obtain a heatmap that performs weakly supervised localization of relevant regions for a given class as shown with\\noutput (c). We note that we include path (c) as an example of weakly supervised localization that can be performed using\\nMc, which we do not perform in our experiments. The input to the attention function Q is marked as a vector because 2-d\\ntensors Mc are computed for every class represented in network G, hence a vector of 2-d tensors is passed to Q to compute a\\nvector of class keys ⃗\\nKc that is then used to retrieve a vector of class likelihood probabilities ⃗\\nV as shown in Algorithm 3. We\\nalso note that the block labeled as GCML represents GCML datastore tensor S as shown in Algorithm 2.\\nA. CIFAR-10 Results\\nHere we describe our experimental settings and results on\\nthe CIFAR-10 benchmark dataset. As mentioned earlier, the', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='A. CIFAR-10 Results\\nHere we describe our experimental settings and results on\\nthe CIFAR-10 benchmark dataset. As mentioned earlier, the\\nmodel we use as our feature extractor is signiﬁcantly smaller\\nthan that used in [38], where a pretrained ResNet50 architec-\\nture was used. We implemented a smaller version of a residual\\nnetwork architecture, which contains only 787,482 parameters\\ncompared to 33,554,432 for ResNet50. Additionally, we did\\nnot perform resizing of input images to 224×224, but instead\\nused the original input size of 32×32. The mapping resolution\\nof the last convolutional layer in our network is 4×4 compared\\nto 7×7 utilized in [38]. The authors [38] did not ﬁne-tune their\\nfeature extractor on the CIFAR-10 dataset, while we trained\\nours from scratch for 80 epochs on a single GPU (Nvidia RTX\\n2080) taking around 15 minutes or about 0.25 GPU hours.\\nThe only data augmentation we performed was padding input\\nimages by 4 pixels and randomly cropping at the original size\\nof 32×32. The training phase of the GCML auxiliary attention\\nmechanism took 28 minutes on the same machine with several\\nτ values. Training the GCML structure with different values\\nof τ does not require any retraining of the CNN feature\\nextractor. The test partition of the CIFAR-10 dataset (10,000\\nimages) was not used for either of training phases. We also', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='of τ does not require any retraining of the CNN feature\\nextractor. The test partition of the CIFAR-10 dataset (10,000\\nimages) was not used for either of training phases. We also\\ndid not utilize a validation set during training of our feature\\nextractor in order to replicate the training environment of [38]\\nas much as possible.\\nTable I shows our results marked with (*) along with\\nTABLE I: CIFAR-10 Results for image classiﬁcation using\\nour GCML approach, UL-Hopﬁeld network [38], and state-\\nof-the-art vision transformer model [4]. We include vision\\ntransformer models to illustrate efﬁciency of our approach in\\nterms of number of parameters and computational cost. The\\ntwo transformer models were pre-trained on 300M images\\nusing cloud TPUv3 with 8 cores for 8 days [4], while our\\nmethod took less than 1 hour on a single GPU. The numbers\\nof trainable parameters are reported for each method according\\nto standard practice.\\nModel\\n% Correct\\nParameters\\nExtra Training Data\\nViT-H/14 [4]\\n99.5\\n632M\\n300M\\nViT-L/16 [4]\\n99.42\\n307M\\n300M\\nUL-Hopﬁeld [38]\\n83.1\\n33.6M\\n1.28M\\nRes4Cif GCML*\\n85.5\\n0.79M\\n0\\nreported results in [38] and the state-of-the-art performance on', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='33.6M\\n1.28M\\nRes4Cif GCML*\\n85.5\\n0.79M\\n0\\nreported results in [38] and the state-of-the-art performance on\\nthe CIFAR-10 dataset reported in [4]. Only predictions made\\nusing the GCML data structure, marked (b) in Figure 5, were\\nused. Our methods outperforms [38], but we note that the train-\\ning of their memory module was unsupervised, even though\\ntheir pre-trained feature extractor was signiﬁcantly larger. One\\nobservation in these results is the signiﬁcant reduction in\\nparameters and extra training costs shown by our method,\\nwhere our model has 800x fewer parameters than ViT-H/14\\nand 42x fewer parameters than UL-Hopﬁeld while requiring no\\n8\\nFig. 6: Confusion matrix on the CIFAR-10 dataset reported in\\n[38] We note that they report true labels on the y-axis of the\\nconfusion matrix.\\nFig. 7: Confusion matrix on the CIFAR-10 dataset using our\\nRes4Cif and GCML with parameter τ = 0.001. Our method\\noutperforms UL-Hopﬁeld networks [38] with signiﬁcantly\\nfewer parameters and without pre-training. We also note that in\\nanother experiment using τ = 0.009, classiﬁcation improved,\\nas shown in Table II, implying further potential improvement.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='fewer parameters and without pre-training. We also note that in\\nanother experiment using τ = 0.009, classiﬁcation improved,\\nas shown in Table II, implying further potential improvement.\\nextra data to pre-train. Both ViT-H/14 and ViT-L/16 are vision\\ntranformer models, which are pretrained on large datasets. ViT-\\nL/16 model, is pre-trained on the ImageNet-21k dataset, and\\nis trained on cloud TPUv3 with 8 cores for approximately 8\\ndays [4]. Our method had a combined training time of less\\nthan 1 hour on a single machine with 1 GPU.\\nTable II shows the performance of our method on CIFAR-\\n10 at different values of the τ hyperparameter. Optimal values\\nof this parameter depend on the the type of data normalization\\nthat is performed on the original data, and normalization\\nmethods of activated feature maps that are used to compute\\nTABLE II: CIFAR-10 Results for classiﬁcation using GCML\\nand different values of the τ hyperparameter. This threshold\\nparameter is used to train the GCML structure as well as\\nperform inference.\\nHyperparameter τ value\\nPercent correct\\nTraining epochs\\n0.3\\n76.59\\n80\\n0.1\\n82.21\\n80\\n0.05\\n83.6\\n80\\n0.009\\n85.51\\n80\\n0.001\\n85.5\\n80\\nTABLE III: COVID-19 Dataset [41] partitions. The validation', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='80\\n0.05\\n83.6\\n80\\n0.009\\n85.51\\n80\\n0.001\\n85.5\\n80\\nTABLE III: COVID-19 Dataset [41] partitions. The validation\\npartition is used as a model selection criteria during ﬁne-tuning\\nof the feature extractor model.\\nData Class Samples\\nTraining\\nValidation\\nTest\\nCOVID-19\\n131\\n44\\n44\\nNo Finding\\n804\\n269\\n268\\nViral Pneumonia\\n807\\n269\\n268\\ninput to our attention function Q. In this experiment we did not\\nperform any normalization on the input data and used simple\\nmin-max normalization on activated feature maps. Figure 6\\nshows the confusion matrix on the CIFAR-10 dataset reported\\nin [38] and Figure 7 shows our method’s confusion matrix on\\nthe same data.\\nB. COVID-19 Radiology Results\\nThe main goal of this work was to evaluate the potential\\nbeneﬁt of attention mechanisms in identifying pulmonary\\nconditions in chest radiographs that may be missed by spatial\\ninvariance of convolutional neural networks. Vision transform-\\ners are an active area of research and convolutional neural nets\\nare currently the main approach for diagnostic assistance in\\nthe chest radiology domain [42]–[46], thus we focus on these\\ntechniques. Data in the radiology domain is not as well curated\\nas in the natural imagery domain, with new diseases like\\nCOVID-19 presenting new classes to identify. This presents', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='techniques. Data in the radiology domain is not as well curated\\nas in the natural imagery domain, with new diseases like\\nCOVID-19 presenting new classes to identify. This presents\\nboth, class imbalanced and data starved environments. Here\\nwe present our results of applying our method to the publicly\\navailable COVID-19 Radiology dataset [41].\\nFor this experiment we randomly split the COVID-19 Radi-\\nology dataset into train, validation, and test partitions as shown\\nin Table III. The validation set is used to select the best set\\nof weights during the model G ﬁne-tuning process, which is\\na standard practice in model training. Since validation data\\nis never used to update weights of G during training, in the\\nsecond part of our experiment we utilize the validation set to\\nfurther reﬁne the GCML structure to investigate the effect of\\nupdating just the attention mechanism without ﬁne-tuning the\\nfeature extractor model on this data.\\nSince we have relatively few training points, we utilized\\ntransfer learning by ﬁne-tuning a ResNet50 architecture that\\nwas pre-trained on the ImageNet dataset. The standard mod-\\niﬁcation that is done during transfer learning is to remove\\nthe last connected layer that represents original classes and\\nreplace it with new target classes before ﬁne-tuning with new\\n9\\nTABLE IV: COVID-19 dataset [41] combined results for\\naccuracy, F1 score, and sensitivity using standard CNN, our\\nGCML approach, and further tuned GCMLT structure using', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='9\\nTABLE IV: COVID-19 dataset [41] combined results for\\naccuracy, F1 score, and sensitivity using standard CNN, our\\nGCML approach, and further tuned GCMLT structure using\\nthe test partition of the COVID-19 Radiology dataset [41].\\nMETRIC (95% CI)\\nCNN\\nGCML\\nGCMLT\\nAccuracy\\n0.948 ±0.018\\n0.938 ±0.020\\n0.942 ±0.019\\nF1 Score\\n0.973 ±0.013\\n0.968 ±0.014\\n0.970 ±0.014\\nSensitivity\\n0.962 ±0.016\\n0.955 ±0.017\\n0.958 ±0.016\\ndata. One other modiﬁcation that we performed was to reduce\\nthe ﬁnal mapping resolution of the network from 7 × 7 to\\n5 × 5. This is done by adding a single convolutional layer\\nwith a kernel (ﬁlter) size of 3, padding of 0, and unit stride,\\nwhile preserving the same number of in and out channels as\\nthe previous convolutional layer. This follows from simple\\nouput resolution calculus for a given dimension of a CNN\\nconvolutional layer, where for any equilateral input i, kernel\\nsize k, padding p, and stride s = 1, the output resolution is\\ngiven by o = (i −k) + 2p + 1.\\nWe then ﬁne-tuned all parameters of this architecture using', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='size k, padding p, and stride s = 1, the output resolution is\\ngiven by o = (i −k) + 2p + 1.\\nWe then ﬁne-tuned all parameters of this architecture using\\nthe training partition of the COVID-19 dataset [41] for 30\\nepochs, while using the validation partition to keep track of\\nthe best performing weights, selecting them as our ﬁnal feature\\nextractor model. Fine-tuning was done using the following\\nsettings and hyperparameters:\\n• input was resized to 224 × 224 through a random resize\\ncrop and normalized using ImageNet mean and standard\\ndeviation values\\n• Stochastic Gradient Descent was used as our optimization\\nfunction\\n• Cross Entropy Loss was our training criterion\\n• learning rate was set to 0.001\\n• momentum was set to 0.9\\nFor our next step we trained the GCML structure using the\\ntraining partition of [41] and our convolutional neural network\\nfrom the previous step using several cam activation points or\\nτ values for 15 epochs each, with the best performing value on\\nthe test portion being τ = 0.05. Training for multiple epochs\\nwas done because we used the same random resize crop data\\ntransform that was done during training of our feature extractor\\nmodel, presenting slight positional variations. Finally, we run\\nour method on the test partition of the COVID-19 Radiology\\ndataset keeping track of both classiﬁcations, standard CNN\\nand GCML attention mechanism, as shown in paths (a) and\\n(b) of Figure 5.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='dataset keeping track of both classiﬁcations, standard CNN\\nand GCML attention mechanism, as shown in paths (a) and\\n(b) of Figure 5.\\nDuring the training phase of the GCML structure, we\\nexplicitly made the normalization step, shown on line 7\\nof Algorithm 2, optional. This allows us to further train a\\ngiven GCML structure on new data without having to track\\ndistribution statistics of the original training data. For our\\nnext experiment we evaluated the effect of further training\\nthe GCML structure on additional data points without further\\ntuning the convolutional neural network used as the feature\\nextractor. We used the validation portion of [41] to further train\\nonly the GCML structure with the same threshold τ = 0.05.\\nWe used 44 samples of COVID-19, 269 samples of No\\nTABLE V: COVID-19 per-class classiﬁcation accuracy (95%\\nCI) for standard CNN, our GCML approach, and further tuned\\nGCMLT structure using the test partition of the COVID-19\\nRadiology dataset [41]. We also note that GCMLT structure\\nwas further tuned on the validation partition, while our feature\\nextractor used with GCMLT was not further trained with the\\nvalidation partition.\\nMETHOD\\nCOVID-19\\nNo Finding\\nViral Pneumonia\\nCNN\\n1.0\\n0.978 ±0.018\\n0.911 ±0.034\\nGCML\\n1.0\\n0.929 ±0.031\\n0.937 ±0.029', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='CNN\\n1.0\\n0.978 ±0.018\\n0.911 ±0.034\\nGCML\\n1.0\\n0.929 ±0.031\\n0.937 ±0.029\\nGCMLT\\n1.0\\n0.937 ±0.029\\n0.937 ±0.029\\nFindings, and 269 samples of Viral Pneumonia images to train\\nfor 5 epochs. As we described earlier, forgoing the optinal\\nnormalization step in Algorithm 2 allows for this functionality.\\nWe then ran the test partition, which neither the convolutional\\nneural network nor the GCML structure have been trained\\non, to obtain results for combined accuracy, F1 score, and\\nsensitivity. Table IV shows our results using three models,\\nCNN or standard classiﬁcation using our convolutional neural\\nnetwork, GCML using our attention mechanism, and ﬁnally\\nGCMLT , where we further trained the attention mechanism\\nusing the validation partition.\\nThe combined results in Table IV show several encouraging\\nresults. First, the attention mechanism performed similarly\\nwell with a combined accuracy of 0.938 as compared to the\\ntraditional classiﬁcation method of convolutional neural net-\\nworks with a combined accuracy of 0.948. Second, by further\\ntuning the GCML structure, to obtain GCMLT , with a small\\nnumber of data points we were able to improve all metrics,\\nbringing combined accuracy to 0.942. This is encouraging\\nbecause tuning this structure is signiﬁcantly faster than tuning', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='number of data points we were able to improve all metrics,\\nbringing combined accuracy to 0.942. This is encouraging\\nbecause tuning this structure is signiﬁcantly faster than tuning\\na convolutional neural network. Finally, once we drilled down\\nto class level accuracy, we observe that GCML improves the\\nstandard CNN classiﬁcation on the Viral Pneumonia class by\\nabout 3 percent, as shown in Table V. This was signiﬁcant for\\ntwo reasons, ﬁrst it empirically veriﬁed our main hypothesis\\nthat our auxiliary attention mechanism could identify cases\\nof pulmonary conditions that the standard CNN classiﬁcation\\nmissed. Second, these results support our intuition of the\\nadditive nature of standard CNN classiﬁcation by showing\\nthat this method outperformed the attention method on the No\\nFindings class, as spatial relations between activated features\\nmatter signiﬁcantly less, as expected.\\nDue to the absense of object level labels, or bounding\\nboxes labeling localized manifestations of pulmonary condi-\\ntions in training data, it is difﬁcult to identify feature speciﬁc\\nintersections or lack thereof between images that CNN and\\nGCML classiﬁed correctly or incorrectly. But even with only\\nimage level labels for training, where we know that the\\ncondition is present but have no localized information of its\\nmaniﬁstation, the confusion matrices in Fig 8 clearly show\\nGCML’s improved performance on the Viral Pneumonia class,', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='condition is present but have no localized information of its\\nmaniﬁstation, the confusion matrices in Fig 8 clearly show\\nGCML’s improved performance on the Viral Pneumonia class,\\nwhile both methods performed the same on the COVID-19\\nclass.\\n10\\n(a)\\n(b)\\n(c)\\nFig. 8: Confusion matrices for evaluation results using the test partition of the COVID-19 Radiology dataset [41]. The three\\nresults represent the following: (a) classiﬁcation using our GCML attention mechanism, (b) classiﬁcation using the ﬁnal\\nconnected layer of our ﬁne-tuned Convolutional Neural Network, and (c) classiﬁcation using our GCMLT attention mechanism\\nfurther trained using the validation partition of [41]. Our ﬁrst observation is that our attention mechanism (a) is able to identify\\npulmonary conditions that the standard CNN (b) missed. This provides empirical evidence to our hypothesis for the utility\\nof attention mechanisms in the chest radiology domain. Second, by further training our GCML structure using the validation\\npartition, which was only used as a model selection criteria during the training of our feature extractor, performance was\\nfurther improved (c). This was done without further training of the CNN feature extractor. These results suggest that hybrid\\nCNN plus attention-based ensemble techniques, that utilize different inductive biases, provide a promising set of approaches\\nto incorporating global interactions between dispersed features of pulmonary diseases that are manifested in chest radiographs.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='CNN plus attention-based ensemble techniques, that utilize different inductive biases, provide a promising set of approaches\\nto incorporating global interactions between dispersed features of pulmonary diseases that are manifested in chest radiographs.\\nTABLE VI: Generalization test dataset for COVID-19 Radiol-\\nogy Data derived from [19]. Neither our feature extractor G,\\nnor the GCML structure S were trained on this data.\\nCOVID-19\\nNo Finding\\nPneumonia (Viral and Bacterial)\\n133\\n949\\n390\\nC. Radiology Generalization Performance\\nFor our ﬁnal experiment we examined our method’s ability\\nto generalize to a separate dataset [19] containing X-ray sam-\\nples of patients diagnosed with COVID-19, No Findings, and\\nPneumonia, both viral and bacterial. We created this dataset\\nby extracting images from a publicly available repository [19]\\nin order to create a test partition that was comparable in size\\nto the train partition we used in training our method. The main\\nmotivation for this experiment was an observation that many\\ninstances of work related to diagnosing COVID-19 and other\\npulmonary conditions using chest radiographs omit external\\ngeneralization experiments, as reported in Roberts et al. [1].\\nTable VI shows the generalization test partition that we\\ncreated using [19]. Neither our feature extractor nor the\\nGCML structure were trained using this data. We then tested\\nthis data using GCML classiﬁcation, using τ = 0.05. The\\nonly transformations that we performed during inference were', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='GCML structure were trained using this data. We then tested\\nthis data using GCML classiﬁcation, using τ = 0.05. The\\nonly transformations that we performed during inference were\\nresizing the image to 224 × 224, and normalization of the\\ntensorized image to ImageNet values for mean and standard\\ndeviation.\\nTable VII shows our results, including per class accuracy,\\nwhile Fig 9 shows the confusion matrix. Similar to our initial\\nresults on [41], we saw our attention mechanism perform\\nTABLE VII: Generalization dataset [19] results at 95% conﬁ-\\ndence interval. Combined accuracy, F1 score, and sensitivity\\nare shown using our GCML classiﬁcation approach. Last row\\nshows per-class accuracy for the same classiﬁer.\\n(95% CI)\\nGCML\\nAccuracy\\n0.940 ±0.012\\nF1 Score\\n0.969 ±0.009\\nSensitivity\\n0.955 ±0.011\\nCOVID-19\\nNo Finding\\nPneumonia\\nClass Accuracy\\n0.985 ±0.021\\n0.9294 ±0.016\\n0.951 ±0.021\\nwell at identifying pulmonary conditions, both COVID-19\\nand Pneumonia, and slightly worse on the No Findings class.\\nFurther examining per-class performance we see that sensitivy\\nrates for both COVID-19 and Pneumonia are high, making the\\napproach effective at detecting infected patients, represented\\nin data never seen by the model during training. These gener-', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='rates for both COVID-19 and Pneumonia are high, making the\\napproach effective at detecting infected patients, represented\\nin data never seen by the model during training. These gener-\\nalization results also increase our conﬁdence in that pertinent\\nfeatures of pulmonary conditions are being discovered by the\\nmodel, as opposed to spurious artifacts such as X-ray markings\\nrelated to the origin or medium of radiographs.\\nD. Conﬁdence Level on Hypothesis Test\\nTo evaluate our main hypothesis regarding our attention\\nmechanism outperforming the standard CNN when classifying\\npulmonary conditions, we perform a hypothesis test [47] on\\nthe two classifers as they pertain to pneumonia classiﬁcation.\\nBoth classiﬁers performed equally well on COVID-19 classi-\\nﬁcation. Let p1 = 0.911 be CNN accuracy and p2 = 0.937\\n11\\nFig. 9: Confusion matrix for predictions made on the radiology\\ngeneralization test dataset, created from [19], using the GCML\\nauxiliary attention mechanism. As noted, no training was\\nperformed using this dataset of neither the feature extractor\\nnor the GCML attention mechanism.\\nbe GCML accuracy shown in Table V. Let n = 268 be the\\nnumber of samples of viral pneumaonia images in the test\\npartition of [41]. Then, we calculate the test statistic Z as:\\nZ =\\np1 −p2\\np\\n2ˆ\\np(1 −ˆ\\np)/n\\n(5)', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='partition of [41]. Then, we calculate the test statistic Z as:\\nZ =\\np1 −p2\\np\\n2ˆ\\np(1 −ˆ\\np)/n\\n(5)\\nwhere ˆ\\np = (245 + 252)/2(268), with 245 and 252 being\\nnumbers of correct classiﬁcations from both classiﬁers, as\\nshown in the confusion matrices in Figure 8. To show that\\np1 < p2, or that p2 is better than p1, we need to show\\nZ < −zα, where zα is obtained from a standard normal\\ndistribution pertaining to a signiﬁcance level α. Given our\\nsample size, we compute Z = −1.1607. Our test statistic\\nis slightly better than the Z value of 1.15035 for a 75%\\nconﬁdence interval. Thus, we can state with 75% conﬁdence\\nlevel that GCML is more accurate than standard CNN for\\nclassifying pneumonia cases.\\nV. CONCLUSION\\nDetection of pulmonary disease using Artiﬁcial Intelligence\\ntechniques is an emergent ﬁeld [11], driven not just by\\nacademic curiosity, but a real need to improve accessibility\\nand speed of diagnosis. Well studied approaches to image\\nclassiﬁcation need to be analyzed and improved to be effective\\nin real world applications, where data can be scarce, noisy,\\nand novel. Emergence of new diseases, such as COVID-19,', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='classiﬁcation need to be analyzed and improved to be effective\\nin real world applications, where data can be scarce, noisy,\\nand novel. Emergence of new diseases, such as COVID-19,\\nexposed weaknesses in such applications of machine learning\\nmethods, but also highlighted potential beneﬁts and promise\\nto society if they were to be successfuly addressed [1].\\nIn this work, we developed a method to improve clas-\\nsiﬁcation performance of pulmonary diseases in chest X-\\nrays by expanding the perceptive awareness of convolutional\\nneural networks via GCML, our new attention mechanism.\\nWe showed that our auxiliary attention mechanism improved\\nsensitivity of pulmonary disease classifers by accounting for\\nspatial interrelations of features globally. Our initial results\\nshow a promising direction of research towards improving ex-\\nisting methods of image classiﬁcation for diagnostic assistance\\nof pulmonary diseases.\\nThere are two main limitations of our initial approach.\\nFirst, the current GCML datastore learns a conditional discrete\\nprobability distribution of a given class, more speciﬁcally the\\nuse of the τ parameter to threshold activation values of the\\ninput to attention function Q, results in a binomial distribution.\\nThis results in some information loss compared to a continuous\\ndistribution. Second, as described in Section III, we must\\nreduce the ﬁnal mapping resolution of our CNNs to manage\\nthe size of learned distributions, which essentially increases the\\narea of individual image patches for which we learn global\\ncorrelations. This can have a smoothing effect on multiple', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='the size of learned distributions, which essentially increases the\\narea of individual image patches for which we learn global\\ncorrelations. This can have a smoothing effect on multiple\\ndisease manifestations detected in a single region, causing\\nit to be treated as a single maniﬁstation. To address these\\nlimitations, we plan on improving the attention mechanism\\nby utilizing normalized continuous values of class activation\\nmaps without a threshold parameter by learning a continuous\\nmultivariate distribution, such as a Dirichlet distribution. This\\nwill allow for image patches with smaller areas, resulting in\\nmore patches, to be used in learning global relations among\\neven smaller manifestations.\\nAdditionally, we can improve positional probability distri-\\nbutions learned by GCML structure by reducing class noise\\ndue to class overlapping data in the chest X-ray domain.\\nSimilar classes result in noisy class activation maps that\\ncontain many overlapping activations [30]. By better extracting\\nclass relevant activations, we can learn tighter conditional\\nprobability distributions for appropriate classes of diseases.\\nWe believe that improving well established techniques\\nfor image classiﬁcation towards domain-speciﬁc applications,\\nsuch as pulmonary disease classiﬁcation, is well worth the\\neffort. By utilizing a strong technical foundation of CNNs,\\nprogress towards a useful diagnostic aid can be accelerated,\\nand outcomes of patients, especially in regions where access\\nto sophisticated laboratory diagnostics is limited, can be im-\\nproved. To that end, we make the reference implementation\\nof our attention mechanism freely available, including source\\ncode and models [48].\\nREFERENCES', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='to sophisticated laboratory diagnostics is limited, can be im-\\nproved. To that end, we make the reference implementation\\nof our attention mechanism freely available, including source\\ncode and models [48].\\nREFERENCES\\n[1] M. Roberts, D. Driggs, M. Thorpe, J. Gilbey, M. Yeung, S. Ursprung,\\nA. Aviles-Rivero, C. Etmann, C. McCague, L. Beer, J. Weir-McCall,\\nZ. Teng, E. Gkrania-Klotsas, AIX-COVNET, J. Rudd, E. Sala, and C.-B.\\nSchonlieb, “Common pitfalls and recommendations for using machine\\nlearning to detect and prognosticate for covid-19 using chest radiographs\\nand ct scans,” Nature Machine Intelligence, vol. 3, no. 3, pp. 199–217,\\n2021.\\n[2] P. Rajpurkar, J. A. Irvin, K. Zhu, B. Yang, H. Mehta, T. Duan,\\nD. Ding, A. Bagul, C. Langlotz, K. Shpanskaya, M. Lungren, and A. Ng,\\n“Chexnet: Radiologist-level pneumonia detection on chest x-rays with\\ndeep learning,” ArXiv, vol. abs/1711.05225, 2017.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='“Chexnet: Radiologist-level pneumonia detection on chest x-rays with\\ndeep learning,” ArXiv, vol. abs/1711.05225, 2017.\\n[3] A. Borghesi and R. Maroldi, “Covid-19 outbreak in italy: experimental\\nchest x-ray scoring system for quantifying and monitoring disease\\nprogression,” La radiologia medica, vol. 125, pp. 509–513, 05 2020.\\n[4] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\\nTransformers\\nfor\\nimage\\nrecognition\\nat\\nscale,”\\nin\\nInternational\\n12\\nConference on Learning Representations, 2021. [Online]. Available:\\nhttps://openreview.net/forum?id=YicbFdNTTy\\n[5] R. Yamashita, M. Nishio, R. Do, and K. Togashi, “Convolutional neural\\nnetworks: an overview and application in radiology,” Insights into\\nImaging, vol. 9, 06 2018.\\n[6] O. Semih Kayhan and J. C. van Gemert, “On translation invariance in', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='Imaging, vol. 9, 06 2018.\\n[6] O. Semih Kayhan and J. C. van Gemert, “On translation invariance in\\ncnns: Convolutional layers can exploit absolute spatial location,” in 2020\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\\n(CVPR), 2020, pp. 14 262–14 273.\\n[7] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classiﬁcation with\\ndeep convolutional neural networks,” Neural Information Processing\\nSystems, vol. 25, 01 2012.\\n[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\\nA large-scale hierarchical image database,” in 2009 IEEE Conference on\\nComputer Vision and Pattern Recognition, 2009, pp. 248–255.\\n[9] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers,\\n“Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on\\nweakly-supervised classiﬁcation and localization of common thorax\\ndiseases,” in Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), July 2017.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='weakly-supervised classiﬁcation and localization of common thorax\\ndiseases,” in Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), July 2017.\\n[10] B. Sahiner, A. Pezeshk, L. M. Hadjiiski, X. Wang, K. Drukker, K. H.\\nCha, R. M. Summers, and M. L. Giger, “Deep learning in medical\\nimaging and radiation therapy,” Medical Physics, vol. 46, no. 1, pp.\\ne1–e36, 2019. [Online]. Available: https://aapm.onlinelibrary.wiley.com/\\ndoi/abs/10.1002/mp.13264\\n[11] O. Albahri, A. Zaidan, A. Albahri, B. Zaidan, K. H. Abdulkareem,\\nZ.\\nAl-qaysi,\\nA.\\nAlamoodi,\\nA.\\nAleesa,\\nM.\\nChyad,\\nR.\\nAlesa,\\nL. Kem, M. M. Lakulu, A. Ibrahim, and N. A. Rashid, “Systematic\\nreview of artiﬁcial intelligence techniques in the detection and\\nclassiﬁcation of covid-19 medical images in terms of evaluation\\nand benchmarking: Taxonomy analysis, challenges, future solutions\\nand\\nmethodological\\naspects,”\\nJournal\\nof\\nInfection\\nand', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='and benchmarking: Taxonomy analysis, challenges, future solutions\\nand\\nmethodological\\naspects,”\\nJournal\\nof\\nInfection\\nand\\nPublic\\nHealth, vol. 13, no. 10, pp. 1381–1396, 2020. [Online]. Available:\\nhttps://www.sciencedirect.com/science/article/pii/S187603412030558X\\n[12] H. Hosseini, B. Xiao, M. Jaiswal, and R. Poovendran, “On the limitation\\nof convolutional neural networks in recognizing negative images,” in\\n2017 16th IEEE International Conference on Machine Learning and\\nApplications (ICMLA), 2017, pp. 352–358.\\n[13] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between\\ncapsules,” in Advances in Neural Information Processing Systems,\\nI. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\\nS. Vishwanathan, and R. Garnett, Eds., vol. 30.\\nCurran Associates,\\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/\\n2017/ﬁle/2cad8fa47bbef282badbb8de5374b894-Paper.pdf\\n[14] E. Hjelm˚\\nas and B. K. Low, “Face detection: A survey,” Computer', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='[14] E. Hjelm˚\\nas and B. K. Low, “Face detection: A survey,” Computer\\nVision and Image Understanding, vol. 83, no. 3, pp. 236–274, 2001.\\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\\nS107731420190921X\\n[15] M. Rahaman, C. Li, Y. Yao, F. Kulwa, M. Rahman, Q. Wang, S. Qi,\\nF. Kong, X. Zhu, and Z. X, “Identiﬁcation of covid-19 samples from\\nchest x-ray images using deep learning: A comparison of transfer\\nlearning approaches,” Journal of X-ray science and technology, vol. 28,\\nno. 5, 2020.\\n[16] S. Liu and W. Deng, “Very deep convolutional neural network based\\nimage classiﬁcation using small training sample size,” in 2015 3rd IAPR\\nAsian Conference on Pattern Recognition (ACPR), 2015, pp. 730–734.\\n[17] A. Khan, J. Shah, and M. Bhat, “Coronet: A deep neural network for\\ndetection and diagnosis of covid-19 from chest x-ray images.” Comput\\nMethods Programs Biomed., vol. 196, no. 105581, 2020.\\n[18] F. Chollet, “Xception: Deep learning with depthwise separable convolu-', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='Methods Programs Biomed., vol. 196, no. 105581, 2020.\\n[18] F. Chollet, “Xception: Deep learning with depthwise separable convolu-\\ntions,” in Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), July 2017.\\n[19] J. P. Cohen, P. Morrison, and L. Dao, “Covid-19 image data\\ncollection,”\\narXiv\\n2003.11597,\\n2020.\\n[Online].\\nAvailable:\\nhttps:\\n//github.com/ieee8023/covid-chestxray-dataset\\n[20] M. Tamal, M. Alshammari, M. Alabdullah, R. Hourani, H. A.\\nAlola, and T. M. Hegazi, “An integrated framework with machine\\nlearning and radiomics for accurate and rapid early diagnosis of\\ncovid-19 from chest x-ray,” Expert Systems with Applications, vol. 180,\\np. 115152, 2021. [Online]. Available: https://www.sciencedirect.com/\\nscience/article/pii/S0957417421005935\\n[21] G. Kim, J. Kim, C. Kim, and S.-M. Kim, “Evaluation of deep learning\\nfor covid-19 diagnosis: Impact of image dataset organization,” Journal\\nof Applied Clinical Medical Physics, 06 2021.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='for covid-19 diagnosis: Impact of image dataset organization,” Journal\\nof Applied Clinical Medical Physics, 06 2021.\\n[22] M. Heidari, S. Mirniaharikandehei, A. Zargari, G. Danala, Y. Qiu, and\\nB. Zheng, “Improving the performance of cnn to predict the likelihood\\nof covid-19 using chest x-ray images with preprocessing algorithms,”\\nInternational Journal of Medical Informatics, vol. 144, p. 104284, 09\\n2020.\\n[23] A. Z. Khuzani, M. Heidari, and S. A. Shariati, “Covid-classiﬁer:\\nAn automated machine learning model to assist in the diagnosis\\nof\\ncovid-19\\ninfection\\nin\\nchest\\nx-ray\\nimages,”\\nmedRxiv,\\n2020.\\n[Online]. Available: https://www.medrxiv.org/content/early/2020/05/18/\\n2020.05.09.20096560\\n[24] M. Alruwaili, A. Shehab, and S. Abd ElGhany, “Covid-19 diagnosis us-\\ning an enhanced inception-resnetv2 deep learning model in cxr images,”\\nJournal of Healthcare Engineering, vol. 2021, pp. 1–16, 06 2021.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='ing an enhanced inception-resnetv2 deep learning model in cxr images,”\\nJournal of Healthcare Engineering, vol. 2021, pp. 1–16, 06 2021.\\n[25] K. Purohit, A. Kesarwani, D. R. Kisku, and M. Dalui, “Covid-19\\ndetection on chest x-ray and ct scan images using multi-image\\naugmented deep learning model,” bioRxiv, 2020. [Online]. Available:\\nhttps://www.biorxiv.org/content/early/2020/10/19/2020.07.15.205567\\n[26] N. Tsiknakis, E. Trivizakis, E. Vassalou, Evangelia, Z. Papadakis,\\nGeorgios, A. Spandidos, Demetrios, A. Tsatsakis, J. S´\\nanchez-Garc´\\nıa,\\nR. L´\\nopez-Gonz´\\nalez, H. Karantanas, Apostolos, and K. Marias, “Inter-\\npretable artiﬁcial intelligence framework for covid-19 screening on chest\\nx-rays,” Experimental and therapeutic medicine, vol. 20, no. 2, pp. 727–\\n735, 2020.\\n[27] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='735, 2020.\\n[27] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking\\nthe inception architecture for computer vision,” in 2016 IEEE Confer-\\nence on Computer Vision and Pattern Recognition (CVPR), 2016, pp.\\n2818–2826.\\n[28] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\\nD. Batra, “Grad-cam: Visual explanations from deep networks via\\ngradient-based localization,” in 2017 IEEE International Conference on\\nComputer Vision (ICCV), 2017, pp. 618–626.\\n[29] Z. Wang, Y. Xiao, Y. Li, Z. Jie, F. Lu, M. Hou, and X. Liu, “Automat-\\nically discriminating and localizing covid-19 from community-acquired\\npneumonia on chest x-rays,” Pattern Recognition, vol. 110, p. 107613,\\n08 2020.\\n[30] E. Verenich, A. Velasquez, N. Khan, and F. Hussain, “Improving\\nExplainability of Image Classiﬁcation in Scenarios with Class Overlap:\\nApplication to COVID-19 and Pneumonia,” in Proceedings of the 19th\\nIEEE International Conference on Machine Learning and Applications,\\n2020.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='Application to COVID-19 and Pneumonia,” in Proceedings of the 19th\\nIEEE International Conference on Machine Learning and Applications,\\n2020.\\n[31] A. Gupta, A. Anjum, S. Gupta, and R. Katarya, “Instacovnet-19: A deep\\nlearning classiﬁcation model for the detection of covid-19 patients using\\nchest x-ray,” Applied Soft Computing, vol. 99, p. 106859, 10 2020.\\n[32] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning\\ndeep features for discriminative localization,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition, 2016, pp. 2921–\\n2929.\\n[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you\\nneed,”\\nin\\nAdvances\\nin\\nNeural\\nInformation\\nProcessing\\nSystems,\\nI. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\\nS. Vishwanathan, and R. Garnett, Eds., vol. 30.\\nCurran Associates,\\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='S. Vishwanathan, and R. Garnett, Eds., vol. 30.\\nCurran Associates,\\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/\\n2017/ﬁle/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\\n[34] Y. Cao, J. Xu, S. Lin, F. Wei, and H. Hu, “Gcnet: Non-local networks\\nmeet squeeze-excitation networks and beyond,” in 2019 IEEE/CVF\\nInternational Conference on Computer Vision Workshop (ICCVW), 2019,\\npp. 1971–1980.\\n[35] M. Yin, Z. Yao, Y. Cao, X. Li, Z. Zhang, S. Lin, and H. Hu,\\n“Disentangled non-local neural networks,” in Computer Vision – ECCV\\n2020, A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds.\\nCham:\\nSpringer International Publishing, 2020, pp. 191–207.\\n[36] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\\nworks,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 2018, pp. 7794–7803.', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='works,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 2018, pp. 7794–7803.\\n[37] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and\\nJ. Shlens, “Stand-alone self-attention in vision models,” in NeurIPS,\\n2019.\\n[38] Q. Liu and S. Mukhopadhyay, “Unsupervised learning using pretrained\\nCNN and associative memory bank,” CoRR, vol. abs/1805.01033, 2018.\\n[Online]. Available: http://arxiv.org/abs/1805.01033\\n[39] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in 2016 IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2016, pp. 770–778.\\n[40] A. Krishevsky, “Learning multiple layers of features from tiny\\nimages,” University of Toronto, Tech. Rep., 2009. [Online]. Available:\\nhttps://www.cs.toronto.edu/∼kriz/learning-features-2009-TR.pdf\\n[41] T. Rahman, M. Chowdhury, and A. Khandakar, “Covid-19 radiology\\ndatabase,” Online, 2020, accessed Jan 2021. [Online]. Available:', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='[41] T. Rahman, M. Chowdhury, and A. Khandakar, “Covid-19 radiology\\ndatabase,” Online, 2020, accessed Jan 2021. [Online]. Available:\\nhttps://www.kaggle.com/tawsifurrahman/covid19-radiography-database\\n13\\n[42] C. Butt, J. Gill, D. Chun, and B. A. Babu, “Deep learning system to\\nscreen coronavirus disease 2019 pneumonia,” Applied Intelligence, p. 1,\\n2020.\\n[43] S. Wang, B. Kang, J. Ma, X. Zeng, M. Xiao, J. Guo, M. Cai, J. Yang,\\nY. Li, X. Meng et al., “A deep learning algorithm using ct images to\\nscreen for corona virus disease (covid-19),” MedRxiv, 2020.\\n[44] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++:\\nA nested u-net architecture for medical image segmentation,” in Deep\\nLearning in Medical Image Analysis and Multimodal Learning for\\nClinical Decision Support.\\nSpringer, 2018, pp. 3–11.\\n[45] S. Roy, W. Menapace, S. Oei, B. Luijten, E. Fini, C. Saltori, I. Huijben,', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='[45] S. Roy, W. Menapace, S. Oei, B. Luijten, E. Fini, C. Saltori, I. Huijben,\\nN. Chennakeshava, F. Mento, A. Sentelli et al., “Deep learning for\\nclassiﬁcation and localization of covid-19 markers in point-of-care lung\\nultrasound,” IEEE Transactions on Medical Imaging, 2020.\\n[46] B. Hurt, S. Kligerman, and A. Hsiao, “Deep learning localization of\\npneumonia: 2019 coronavirus (covid-19) outbreak,” Journal of Thoracic\\nImaging, vol. 35, no. 3, pp. W87–W89, 2020.\\n[47] R. Johnson and J. Freund, Miller and Freund’s Probability and Statistics\\nfor Engineers.\\nPrantice Hall International, 2011.\\n[48] E. Verenich, “Gcml artifacts repository,” Online, 2021, accessed Jul\\n2021. [Online]. Available: https://gitlab.com/verenich/gcmlpub', metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}), Document(page_content='TAME: Attention Mechanism Based Feature Fusion\\nfor Generating Explanation Maps of Convolutional\\nNeural Networks\\nMariano Ntrougkas\\nCERTH-ITI\\nThessaloniki, Greece, 57001\\nntrougkas@iti.gr\\nNikolaos Gkalelis\\nCERTH-ITI\\nThessaloniki, Greece, 57001\\ngkalelis@iti.gr\\nVasileios Mezaris\\nCERTH-ITI\\nThessaloniki, Greece, 57001\\nbmezaris@iti.gr\\nAbstract—The apparent “black box” nature of neural networks\\nis a barrier to adoption in applications where explainability is\\nessential. This paper presents TAME (Trainable Attention Mech-\\nanism for Explanations)1, a method for generating explanation\\nmaps with a multi-branch hierarchical attention mechanism.\\nTAME combines a target model’s feature maps from multiple\\nlayers using an attention mechanism, transforming them into an\\nexplanation map. TAME can easily be applied to any convolu-\\ntional neural network (CNN) by streamlining the optimization\\nof the attention mechanism’s training method and the selection\\nof target model’s feature maps. After training, explanation\\nmaps can be computed in a single forward pass. We apply\\nTAME to two widely used models, i.e. VGG-16 and ResNet-50,\\ntrained on ImageNet and show improvements over previous top-\\nperforming methods. We also provide a comprehensive ablation\\nstudy comparing the performance of different variations of', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='trained on ImageNet and show improvements over previous top-\\nperforming methods. We also provide a comprehensive ablation\\nstudy comparing the performance of different variations of\\nTAME’s architecture.2\\nIndex Terms—CNNs, Deep Learning, Explainable AI, Inter-\\npretable ML, Attention.\\nI. INTRODUCTION\\nConvolutional neural networks (CNNs) [17] have achieved\\nexceptional performance in many important visual tasks such\\nas breast tumor detection [6], video summarization [3] and\\nevent recognition [10]. The trade-off between model perfor-\\nmance and explainability, and the end-to-end learning strategy,\\nleads to the development of CNNs that many times act as\\n“black box” models that lack transparency [12]. This fact\\nmakes it difﬁcult to convince users in critical ﬁelds, such as\\nhealthcare, law, and governance to trust and employ such sys-\\ntems, thus limiting the adoption of AI [2], [12]. Therefore, it\\nis necessary to develop solutions that address these challenges.\\nExplainable artiﬁcial intelligence (XAI) is an active research\\narea in machine learning. XAI focuses on developing explain-\\nable techniques that help users of AI systems to comprehend,\\nThis work was supported by the EU Horizon 2020 programme under grant\\nagreement H2020-101021866 CRiTERIA.\\n1Source code is made publicly available at: https://github.com/bmezaris/\\nTAME', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='agreement H2020-101021866 CRiTERIA.\\n1Source code is made publicly available at: https://github.com/bmezaris/\\nTAME\\n2©2022 IEEE. Personal use of this material is permitted. Permission from\\nIEEE must be obtained for all other uses, in any current or future media,\\nincluding reprinting/republishing this material for advertising or promotional\\npurposes, creating new collective works, for resale or redistribution to servers\\nor lists, or reuse of any copyrighted component of this work in other works.\\nFig. 1: An explanation produced by TAME. The input image\\nbelongs to the class ”velvet”, which cannot be localized. The\\nproduced explanation highlights the salient features of the\\nimage explaining the decision of the classiﬁer.\\ntrust and more efﬁciently manage them [4], [20]. For the\\nimage classiﬁcation task, a diverse range of post-hoc expla-\\nnation approaches exist that in a second step take the trained\\nmodel and try to uncover its decision strategy [20]. These\\nmethods produce an explanation map, highlighting salient\\ninput features. We should note that these methods should\\nnot be confused with approaches targeting weakly supervised\\nlearning tasks such as weakly supervised object localization\\nor segmentation [16], which also generate heatmaps as an\\nintermediate step, and their goal is to locate the region of\\nthe target object rather than to explain the classiﬁer’s decision\\n(e.g. see the example depicted in Fig. 1).', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='intermediate step, and their goal is to locate the region of\\nthe target object rather than to explain the classiﬁer’s decision\\n(e.g. see the example depicted in Fig. 1).\\nGradient-based methods [5], [22] were probably among the\\nﬁrst to appear in the XAI domain. These methods use gradient\\ninformation to produce explanations, but they are strongly\\naffected by noisy gradients, and the explanations contain high-\\nfrequency variations [1]. Perturbation-based methods [19],\\n[28], perturb the input and observe changes in the output,\\nthus do not suffer from gradient-based problems as above.\\nSimilarly, response-based methods [8], [21], [27] combine a\\nmodel’s intermediate representations, or features, to generate\\nexplanations. However, most methods of the two latter cate-\\ngories described above are computationally expensive because\\narXiv:2301.07407v1  [cs.CV]  18 Jan 2023\\neach input requires many forward passes for an accurate\\nexplanation map to be produced.\\nTo address the above limitation, L-CAM [11] trains an\\nattention mechanism to combine feature maps from the last\\nconvolutional layer of a frozen CNN model and produce\\nhigh quality explanations in one forward pass. However, L-\\nCAM, by design, uses the feature maps of only the last\\nconvolutional layer, and thus, may not be able to adequately\\ncapture all the information contained in the CNN model. To', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='CAM, by design, uses the feature maps of only the last\\nconvolutional layer, and thus, may not be able to adequately\\ncapture all the information contained in the CNN model. To\\nthis end, we propose TAME (Trainable Attention Mechanism\\nfor Explanations), which exploits intermediate feature maps\\nextracted from multiple layers of any CNN model. These\\nfeatures are then used to train a multi-branch hierarchical\\nattention architecture for generating class-speciﬁc explanation\\nmaps in a single forward pass. We provide a comprehensive\\nevaluation study of the proposed method on ImageNet [7] us-\\ning two popular CNN models (VGG-16 [23], ResNet-50 [13])\\nand popular XAI measures [5], demonstrating that TAME\\nachieves improved explainability performance over other top-\\nperforming methods in this domain.\\nII. RELATED WORK\\nIn this section, we brieﬂy survey the state-of-the-art XAI\\napproaches that are mostly related to ours. For a more com-\\nprehensive review the interested reader is referred to [4], [20].\\nMost XAI approaches can be roughly categorized into\\nresponse-, gradient- and perturbation-based. Gradient-based\\nmethods [5], [22] compute the gradient of a given input with\\nbackpropagation and modify it in various ways to produce\\nan explanation map. Grad-CAM [22], one of the ﬁrst in\\nthis category, uses global average pooling in the gradients of', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='backpropagation and modify it in various ways to produce\\nan explanation map. Grad-CAM [22], one of the ﬁrst in\\nthis category, uses global average pooling in the gradients of\\nthe target network’s logits with respect to the feature maps\\nto compute weights. The explanation maps are obtained as\\nthe weighted combination of feature maps and the computed\\nweights. Grad-CAM++ [5] similarly uses gradients to generate\\nexplanation maps. These methods suffer the same issues as the\\ngradients they use: neural network gradients can be noisy and\\nsuffer from saturation problems for typical activation functions\\nsuch as ReLU and Sigmoid [1].\\nPerturbation-based methods [19], [28] alter the input and\\nproduce explanations based on the change in the conﬁdence\\nof the original prediction; thus, avoid problems related with\\nnoise gradients. For instance, RISE [19] utilizes Monte Carlo\\nsampling to generate random masks, which are then used\\nto perturb the input image and generate a respective CNN\\nscore. Using the generated scores as weights, the explanation\\nis derived as the weighted combination of the various random\\nmasks. Thus, RISE, as most methods in this category, requires\\nmany forward passes through the network to generate an\\nexplanation, increasing the inference time considerably.\\nFinally, response-based methods [8], [11], [21], [27] use\\nfeature maps or activations of layers in the inference stage\\nto interpret the decision-making process of a neural network.', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='Finally, response-based methods [8], [11], [21], [27] use\\nfeature maps or activations of layers in the inference stage\\nto interpret the decision-making process of a neural network.\\nOne of the earliest methods in this category, CAM [29], uses\\nthe output of the global average pooling layer as weights,\\nand computes the weighted average of the features maps at\\nthe ﬁnal convolutional layer. CAM requires the existence of\\nsuch a global average pooling layer, restricting its applicability\\nto only this type of architectures. SISE [21], and later Ada-\\nSISE [27], aggregate feature maps in a cascading manner to\\nproduce explanation maps of any DCNN model. Similarly,\\nPoly-CAM [8] upscales feature maps to the dimension of the\\nlargest spatial dimension feature map and combines them in a\\ncascading manner. The above methods require many forward\\npasses to produce an explanation. L-CAM [11] mitigates\\nthe above limitation using a learned attention mechanism\\nto compute class-speciﬁc explanations in one forward pass.\\nHowever, it can only harness the salient information of one\\nset of feature maps. TAME also falls into the response-based\\ncategory and operates in one forward pass, but contrarily to\\n[11], it uses a trainable hierarchical attention module to exploit\\nfeature maps from multiple layers and generate explanations\\nof higher quality.\\nWe should also note that the methods of [9], [15] take a\\nsomewhat similar approach to ours in that they produce expla-', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='feature maps from multiple layers and generate explanations\\nof higher quality.\\nWe should also note that the methods of [9], [15] take a\\nsomewhat similar approach to ours in that they produce expla-\\nnations using an attention module and multiple sets of feature\\nmaps. However, these methods jointly train the attention model\\nwith the CNN to improve the image classiﬁcation task. In\\ncontrast, TAME does not modify the target model, which\\nhas been pretrained (and remains frozen); instead, TAME\\nfunctions as a post-hoc method, exclusively optimizing the\\nattention module in a supervised learning manner to generate\\nvisual explanations. Thus, no direct comparisons can be drawn\\nwith [9], [15] as they provide explanations for a different (i.e.\\nnot the initial pretrained one), concurrently trained classiﬁer.\\nIII. TAME\\nA. Problem formulation\\nLet f be a trained CNN for which we want to generate\\nexplanation maps,\\nf : I →RClasses,\\n(1)\\nwhere, I is the set of all possible input tensors I\\n=\\n{I | I : C × W × H →R}, C\\n=\\n{1, . . . , C}, W\\n=\\n{1, . . . , W}, H = {1, . . . , H}, C, W, H ∈R are the input\\ntensor dimensions [19], [21], and Classes is the number of\\nclasses that f has been trained to recognize. E.g., for RGB', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='tensor dimensions [19], [21], and Classes is the number of\\nclasses that f has been trained to recognize. E.g., for RGB\\nimages, C = 3, and the elements of a tensor instance are the\\nimage pixel values. Moreover, let Li : Ci × Wi × Hi →R\\nbe the feature map set corresponding to the ith layer of the\\nCNN, where, Ci, Wi, Hi are the respective channel, width and\\nheight dimensions. We deﬁne a feature map set {L}s, where s\\nis the set of layers for which we want to extract feature maps,\\ni.e., {L}s = {Li| i ∈s}.\\nAssume an attention module deﬁned as in the following,\\nAM : {L}s →E,\\n(2)\\nwhere, the tensor E at the output of the attention module is\\nthe generated explanation map, E : Classes × We × He →\\n{x | x ∈R ∩0 < x < 1}, We\\n=\\nmax {W}s and He\\n=\\nmax {H}s. Thus, explanation maps are class discriminative,\\nFig. 2: TAME’s attention module: Feature branches process feature maps to provide attention maps, which are concatenated\\nand processed by the fusion branch (shown at the bottom of the attention module) to derive explanation maps.\\nFig. 3: TAME’s training method. Var: Variation loss, Area:\\nArea loss, CE: Cross-entropy loss. The explanation of an', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='Fig. 3: TAME’s training method. Var: Variation loss, Area:\\nArea loss, CE: Cross-entropy loss. The explanation of an\\ninput image is ﬁrst derived; it is then upscaled and piece-wise\\nmultiplied with the corresponding input image. Subsequently,\\nthe masked image does a second forward pass through the\\nCNN to generate logits, which are used by the loss function to\\ncompute gradients and update the attention module’s weights.\\ni.e., each slice of E along its ﬁrst dimension corresponds to\\none of the classes that f has learned; moreover, the size of\\nthe spatial dimensions of these “class-speciﬁc” slices equal to\\nthe largest spatial dimensions in the set of feature maps.\\nGiven the above formulation, the goal is to ﬁnd an attention\\nmodule architecture that can combine all the salient informa-\\ntion contained in {L}s, and effectively train it.\\nB. Architecture\\nWe propose the attention module architecture depicted in\\nFig. 2. In this architecture, there exists a separate feature\\nbranch for each feature map set that is included in {L}s and\\none fusion branch. Each feature branch takes as input a single\\nfeature map set Li and outputs an attention map set Ai,\\nFB : Li →Ai,\\n(3)\\nwhere, Ai has the same channel and spatial dimensions as\\nLi and the ﬁnal explanation map, respectively, i.e., Ai : Ci ×', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='FB : Li →Ai,\\n(3)\\nwhere, Ai has the same channel and spatial dimensions as\\nLi and the ﬁnal explanation map, respectively, i.e., Ai : Ci ×\\nWe×He →R. The resulting attention maps are concatenated\\ninto an single attention map set {A}s, and forwarded into the\\nfusion branch to generate the explanation map,\\nFS : {A}s →E.\\n(4)\\nThe two branch types consist of different network components,\\nas described in the following:\\nFeature branch: Each feature branch is a neural network\\nthat prepares the feature maps for the fusion branch. It consists\\nof a 1 × 1 convolution layer with the same number of input\\nand output channels, a batch normalization layer, a skip\\nconnection, a ReLU activation, and a bilinear interpolation\\nthat upscales the feature map to match the ﬁnal explanation\\nmap’s dimensions (the ablation study presented in Section IV\\nassesses the importance of each part of the feature branch).\\nFusion branch: It consists of a 1 × 1 convolutional layer\\nthat brings the number of the inputted channels to the number\\nof classes that the CNN has been trained to recognize. Subse-\\nquently, a sigmoid activation function, S(x) =\\n1\\n1−e−x , is used\\nto scale the attention map values to the range (0, 1).\\nC. Training\\nThe training procedure is shown in Fig. 3. An image is\\ninputted to the CNN model, and the derived feature maps are', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='to scale the attention map values to the range (0, 1).\\nC. Training\\nThe training procedure is shown in Fig. 3. An image is\\ninputted to the CNN model, and the derived feature maps are\\nforwarded to the attention module for generating the respective\\nexplanation maps and the model truth label. A model truth\\nlabel is the CNN model’s prediction of the input image’s\\nclass, which may be different from the ground truth label. A\\nsingle channel containing a class discriminative explanation\\nis selected from the explanation map using the model truth\\nlabel; this is used as the explanation of the input image\\nwith respect to the model truth class. The explanation is\\nthen upscaled to the dimensions of the input image using\\nbilinear interpolation, and is piece-wise multiplied with the\\ninput image. The resulting masked image is then fed back into\\nthe CNN to generate logits. The logits, the original explanation\\nmaps, and the model truth labels are then used to compute the\\nloss and through backpropagation update the weights of the\\nattention module, effectively training it. As already mentioned,\\nthe weights of the original CNN remain ﬁxed to their original\\nvalues for the whole training procedure.\\nThe loss function used for training the proposed attention\\nmodule is the weighted sum of three individual loss functions,\\nL(Ψ, logits, labels)\\n=\\nλ1CE(logits, labels)\\n+ λ2Area(Ψ) + λ3Var(Ψ), (5)\\nwhere, Ψ is the slice of the explanation map E corresponding', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='=\\nλ1CE(logits, labels)\\n+ λ2Area(Ψ) + λ3Var(Ψ), (5)\\nwhere, Ψ is the slice of the explanation map E corresponding\\nto the model truth class of the input image, CE(), Area(), Var()\\nare the cross-entropy, area and variation loss, respectively, and\\nλ1, λ2, λ3, are the corresponding regularization parameters.\\nThe cross-entropy loss uses the logits generated from the CNN\\nwith the masked input image and the model truth label to\\ncompute a loss value. This term trains the attention module to\\nfocus on salient parts of the image. The variation loss is the\\nsum of the squares of the partial derivatives of the explanation\\nΨ in the x and y direction. This term penalizes fragmentation\\nin the generated heatmaps. For the partial derivatives, we use\\nthe forward difference approximation. To this end, in the x\\ndirection we have ∂Ψ[xm,ym]\\n∂x\\n≈Ψ[xm + 1, ym] −Ψ[xm, ym].\\nThus, using the forward difference approximation the variation\\nloss is deﬁned as,\\nVar(Ψ) =\\nX\\nx,y\\n\"\\x12∂Ψ[x, y]\\n∂x\\n\\x132\\n+\\n\\x12∂Ψ[x, y]\\n∂y\\n\\x132#\\n.\\n(6)\\nFinally, the area loss is the mean of the explanation map E', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='∂x\\n\\x132\\n+\\n\\x12∂Ψ[x, y]\\n∂y\\n\\x132#\\n.\\n(6)\\nFinally, the area loss is the mean of the explanation map E\\nto the Hadamard power of λ4, i.e.:\\nArea(Ψ) =\\nX\\nx,y\\nΨ[x, y]λ4.\\n(7)\\nThis term forces the attention module to output heatmaps that\\nemphasize small focused regions in the input image instead of\\narbitrarily large areas.\\nD. Inference\\nDuring inference, the ﬁnal sigmoid activation function in\\nthe attention module (Fig. 2) is replaced with a min-max\\nnormalization operator, m(x) =\\nx−min(Ψ)\\nmax(Ψ)−min(Ψ); the min()\\nand max() operators return the smallest and largest element\\nof the input tensor, respectively. This is done for consistency\\nwith other literature works, such as [5], [22], on how the ﬁnal\\nexplanation maps are scaled in order to be evaluated. The test\\nimage is then forward-passed through the CNN, producing\\nexplanation maps, which are then upscaled to the input image\\nsize. The derived model truth label can then be used to provide\\nan explanation concerning the decision of the classiﬁer.\\nIV. EXPERIMENTS\\nA. Datasets and CNNs\\nWe evaluate TAME on two popular CNNs pretrained on', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='an explanation concerning the decision of the classiﬁer.\\nIV. EXPERIMENTS\\nA. Datasets and CNNs\\nWe evaluate TAME on two popular CNNs pretrained on\\nImageNet: VGG-16 [23] and ResNet-50 [13]. We choose these\\ntwo models to test the generality of our method because\\nthere are signiﬁcant differences in the VGG and ResNet\\narchitectures. We obtain these pretrained networks using the\\ntorchvision.models library.\\nWe train the attention module of our method with the\\nImageNet ILSVRC 2012 dataset [7]. This dataset contains\\n1000 classes, 1.3 million and 50k images for training and\\nevaluation, respectively. Due to the prohibitively high cost of\\nexecuting the literature’s perturbation-based approaches that\\nwe use in the experimental comparisons, we use only 2000\\nrandomly-selected testing images for testing (the same as in\\n[11] to allow a fair comparison) and a different 2000 randomly\\nselected images as a validation set.\\nB. Evaluation measures\\nIn the experimental evaluation, two frequently used evalua-\\ntion measures, Increase in Conﬁdence (IC) and Average Drop\\n(AD) [5], are utilized,\\nAD(v) =\\nΥ\\nX\\ni=1\\nmax(0, f(Ii) −f(Ii ⊙φv(Ψi)))\\nΥf(Ii)\\n100,\\n(8)\\nIC(v) =\\nΥ\\nX', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='Υf(Ii)\\n100,\\n(8)\\nIC(v) =\\nΥ\\nX\\ni=1\\nsign(f(Ii ⊙φv(Ψi)) > f(Ii))\\nΥ\\n100,\\n(9)\\nwhere, φv() is a threshold function to select the v% higher-\\nvalued pixels of the explanation map, sign() returns 1 when\\nthe input condition is satisﬁed and 0 otherwise, Υ is the\\nnumber of test images, Ii is the ith test image and Ψi is the\\ncorresponding explanation produced by TAME or any other\\nmethod under evaluation. Intuitively, AD measures how much,\\non average, the produced explanation maps, when used to\\nmask the input images, reduce the conﬁdence of the model.\\nIn contrast, IC measures how often the explanation masks,\\nwhen used to mask the input images, increase the conﬁdence\\nof the model. We threshold the explanation maps to test how\\nwell the pixels of the explanation map are ordered based on\\nimportance. Thus, using a smaller threshold results in a much\\nmore challenging evaluation setup.\\nC. Experimental setup\\nTAME is applied to VGG-16 using feature maps from\\nthree different layers. The VGG-16 consists of ﬁve blocks\\nof convolutions separated by 2 × 2 max-pooling operations,\\nas shown in Fig. 4. We choose one layer from each of the\\nlast three blocks, namely the feature maps output by the max-', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='of convolutions separated by 2 × 2 max-pooling operations,\\nas shown in Fig. 4. We choose one layer from each of the\\nlast three blocks, namely the feature maps output by the max-\\npooling layers of each block. We have also experimented on\\nthe feature maps output by the last convolution layer of each\\nblock. On the other hand, ResNet-50 consists of ﬁve stages.\\nIn the experimental evaluation, we use the feature maps from\\nthe ﬁnal three stages.\\nTAME is trained using the loss function deﬁned in (5) with\\nthe SGD (Stochastic Gradient Descent) algorithm. The biggest\\nbatch size that can ﬁt in the graphics card’s memory is used,\\nas recommended in [25]. The learning rate is varied using the\\nOneCycleLR policy described in [26]. The maximum learning\\nrate used by the OneCycleLR policy is chosen using the LR\\nﬁnder test deﬁned in [24]. The hyperparameters of the loss\\nFig. 4: The layers from which feature map sets are extracted on VGG-16. We denote by “Convolutional Layers” the three\\nlayers before the last three max-pooling layers. In the case of VGG-16, the layer before a max-pooling layer is the ReLU\\nactivation function. We use the same layer naming as the library torchvision.models.feature_extraction.\\nfunction ((5), (7)) are empirically chosen using the validation', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='activation function. We use the same layer naming as the library torchvision.models.feature_extraction.\\nfunction ((5), (7)) are empirically chosen using the validation\\ndataset, as: λ1 = 1.5, λ2 = 2, λ3 = 0.01, λ4 = 0.3.\\nWe train the attention module for eight epochs in total and\\nselect the epoch for which the attention module achieved the\\nbest IC(15%) and AD(15%) in the validation set. That is, in\\nthis model selection procedure we opt for the measures at the\\n15% threshold because they are the most challenging measures\\nto improve upon and provide more focused explanation masks.\\nDuring training, each image is transformed in the same way\\nas with the original CNN, i.e., its smaller spatial dimension\\nis resized to 256 pixels, random-cropped to dimensions W =\\nH = 224, and normalized to zero mean and unit variance.\\nThe same is done during testing, except that center-cropping\\nis used. The feature maps are extracted from the CNN us-\\ning torchvision.models.feature_extraction li-\\nbrary.\\nD. Quantitative Evaluation\\nThe\\nproposed\\nmethod\\nis\\ncompared\\nagainst\\nthe\\ntop-\\nperforming approaches in the visual explanation domain for\\nwhich source code is publicly available i.e. Grad-CAM [22],\\nGrad-CAM++ [5], Score-CAM [28], RISE [19] and L-CAM', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='which source code is publicly available i.e. Grad-CAM [22],\\nGrad-CAM++ [5], Score-CAM [28], RISE [19] and L-CAM\\n[11]. The performance is measured using AD(v) and IC(v)\\non three different thresholds v of increasing difﬁculty, i.e.,\\nv = 100%, 50% and 15%, similarly to the evaluation protocol\\nof [11]. An ablation study is also conducted, to assess the\\nimportance of the different architecture components for VGG-\\n16 and ResNet-50, as well as to showcase the effect of different\\nlayer selections in the VGG-16 model.\\n1) Comparison with the State-Of-The-Art: In Table I we\\nhighlight with bold letters the best result and underline the\\nsecond best result for each measure, separately for each base\\nmodel. We can see that TAME outperforms the gradient-\\nbased methods, and is competitive to the perturbation-based\\nmethods, obtaining the best results for the more demanding\\n15% measures while requiring only one forward pass.\\n2) Ablation Study: In Table II we highlight with bold letters\\nthe best results and underline the second best result for each\\nmeasure in each model and layer selection. For the VGG-16\\nmodel, inspired from similar works in the literature suggesting\\nthat the last layers of the network provide more salient features\\n[15], we report two sets of experiments, one that uses features\\nextracted from the three last max-pooling layers and one where', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='that the last layers of the network provide more salient features\\n[15], we report two sets of experiments, one that uses features\\nextracted from the three last max-pooling layers and one where\\nfeatures are extracted from the layers exactly before the last\\nthree max-pooling layers (Fig. 4). There is a difference in the\\nspatial dimensions of the explanation maps generated using the\\nformer or the latter layers for feature extraction, i.e., 28 × 28\\nversus 56 × 56, since the dimension of the explanation maps\\nobtained by TAME is dictated by the largest feature map set (as\\nexplained in Section III). For the ResNet-50 model, we extract\\nfeatures from the outputs of the ﬁnal three stages, resulting to\\nan explanation map of 28×28 spatial dimensions. We examine\\nthe following variants of the proposed architecture:\\nNo skip connection: It has been shown that the skip\\nconnection promotes a smoother loss landscape [18], thus\\ncontributing to training very deep neural networks. Even for\\nshallower neural networks, such as the proposed attention\\nmodule, we can beneﬁt from using a skip connection. We see\\nthat by omitting the skip connection, we get worse results\\nin ResNet-50. Similarly, for both baseline models we report\\nworse performance for the harder 50% and 15% measures.\\nNo skip + No batch norm: Batch normalization is used\\nin CNNs for speeding up training and combating internal\\ncovariate shift [14]. Compared to the proposed architecture,\\nwe see that this variant generally performs better in the 100%', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='in CNNs for speeding up training and combating internal\\ncovariate shift [14]. Compared to the proposed architecture,\\nwe see that this variant generally performs better in the 100%\\nmeasures, but this does not hold for the other measures. We\\ncompare the masks produced by this variant in Fig. 5.\\nSigmoid in feature branch: In this variant we replace the\\nReLU function with the sigmoid function, which squeezes the\\ninput from (−∞, ∞) to the output (0, 1). It is well known\\nthat the sigmoid function in deeper neural networks causes the\\nvanishing gradient problem, making it more difﬁcult to train\\nthe early layers of the CNN. We see again that the proposed\\narchitecture prevails for the more challenging 15% measures.\\nTwo layers and One layer: In this case, the proposed\\nattention module architecture is employed with fewer feature\\nmaps. The results when using just one layer, i.e., omitting the\\ntwo earlier layers in the CNN pipeline (Fig. 4), are very similar\\nto the L-CAM-Img method (as shown in Table I), which also\\nTABLE I: Comparison of TAME with other methods\\nModel\\nMeasure\\nGrad-CAM [22]\\nGrad-CAM++ [5]\\nScore-CAM [28]\\nRISE [19]\\nL-CAM-Img [11]\\nTAME\\nVGG-16\\nAD(100%)\\n32.12\\n30.75\\n27.75\\n8.74\\n12.15\\n9.33', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='L-CAM-Img [11]\\nTAME\\nVGG-16\\nAD(100%)\\n32.12\\n30.75\\n27.75\\n8.74\\n12.15\\n9.33\\nIC(100%)\\n22.1\\n22.05\\n22.8\\n51.3\\n40.95\\n50\\nAD(50%)\\n58.65\\n54.11\\n45.6\\n42.42\\n37.37\\n36.5\\nIC(50%)\\n9.5\\n11.15\\n14.1\\n17.55\\n20.25\\n22.45\\nAD(15%)\\n84.15\\n82.72\\n75.7\\n78.7\\n74.23\\n73.29\\nIC(15%)\\n2.2\\n3.15\\n4.3\\n4.45\\n4.45\\n5.6\\nForward Passes (Inference)\\n1\\n1\\n512\\n4000\\n1\\n1\\nResNet-50\\nAD(100%)\\n13.61\\n13.63\\n11.01\\n11.12\\n11.09\\n7.81\\nIC(100%)\\n38.1\\n37.95\\n39.55\\n46.15\\n43.75\\n54\\nAD(50%)\\n29.28\\n30.37\\n26.8\\n36.31\\n29.12\\n27.88\\nIC(50%)\\n23.05\\n23.45\\n24.75\\n21.55\\n24.1\\n27.5\\nAD(15%)\\n78.61\\n79.58\\n78.72', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='27.88\\nIC(50%)\\n23.05\\n23.45\\n24.75\\n21.55\\n24.1\\n27.5\\nAD(15%)\\n78.61\\n79.58\\n78.72\\n82.05\\n79.41\\n78.58\\nIC(15%)\\n3.4\\n3.4\\n3.6\\n3.2\\n3.9\\n4.9\\nForward Passes (Inference)\\n1\\n1\\n2048\\n8000\\n1\\n1\\nTABLE II: Ablation study of TAME\\nModel\\nFeature Extraction\\nArchitecture Variant\\nAD(100%)\\nIC(100%)\\nAD(50%)\\nIC(50%)\\nAD(15%)\\nIC(15%)\\nVGG-16\\nMax-pooling layers\\nProposed Architecture\\n9.33\\n50\\n36.5\\n22.45\\n73.29\\n5.6\\nNo skip connection\\n10.09\\n45.25\\n36.44\\n20.65\\n74.85\\n5.15\\nNo skip + No batch norm\\n5.92\\n57.9\\n34.49\\n24.2\\n74.58\\n5.15\\nSigmoid in feature branch\\n7.22\\n55.65\\n38.4\\n21.6\\n79\\n4.85\\nTwo layers\\n10.72\\n45.45\\n34.48\\n23.05\\n71.94\\n5.75\\nOne layer\\n12.1\\n42.1\\n35.81\\n20.8\\n74.19\\n4.85\\nConvolutional layers', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='34.48\\n23.05\\n71.94\\n5.75\\nOne layer\\n12.1\\n42.1\\n35.81\\n20.8\\n74.19\\n4.85\\nConvolutional layers\\nProposed Architecture\\n9.07\\n51.1\\n40.72\\n20.9\\n77.05\\n4.8\\nNo skip connection\\n6.22\\n58.85\\n41.47\\n20.9\\n79.12\\n3.8\\nNo skip + No batch norm\\n6.62\\n56.6\\n40.48\\n20.75\\n77.84\\n4.95\\nSigmoid in feature branch\\n6.8\\n60\\n42.17\\n19.75\\n80.73\\n4.1\\nTwo layers\\n10.99\\n45.85\\n40.89\\n19.55\\n76.66\\n4.8\\nOne layer\\n13.09\\n39.65\\n42.3\\n17.7\\n78.02\\n3.8\\nResNet-50\\nStage Outputs\\nProposed Architecture\\n7.81\\n54\\n27.88\\n27.5\\n78.58\\n4.9\\nNo skip connection\\n5.7\\n62.65\\n46.58\\n18.25\\n89.32\\n2.3\\nNo skip + No batch norm\\n9.29\\n50.25\\n29.43\\n25.95\\n79.81\\n3.95\\nSigmoid in feature branch\\n9.11\\n53.3\\n45.68\\n18.1\\n86.95\\n3.15\\nTwo layers', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='29.43\\n25.95\\n79.81\\n3.95\\nSigmoid in feature branch\\n9.11\\n53.3\\n45.68\\n18.1\\n86.95\\n3.15\\nTwo layers\\n9.48\\n47.05\\n27.83\\n25\\n77.95\\n4.25\\nOne layer\\n11.32\\n43.45\\n29.85\\n24.25\\n79.59\\n3.55\\nuses just one feature map set. All measures are improved when\\nutilizing a second feature map set, i.e., excluding only the\\nearliest layer in the CNN pipeline; however, the case is not\\nthe same clear when going from the two to three feature map\\nsets, which are used in the proposed architecture. These mixed\\nresults could be attributed to the extra noise of feature maps\\ntaken earlier in a CNN pipeline.\\nWe note that by omitting both the skip connection and\\nthe batch normalization in the feature branch architecture,\\nwe obtain generally better results in the case of the VGG-\\n16 model, but this is not the case for the same architecture\\napplied to the ResNet-50 model. In addition, all architectures\\nstruggle with the more difﬁcult 15% measures compared to\\nthe proposed architecture. Although every architecture varies\\nbetween models, the proposed architecture generalizes best\\nacross different models. Thus, our goal of ﬁnding an effec-\\ntive architecture across radically different models is achieved\\nthrough the proposed architecture.\\nFig. 5: Qualitative comparison between the proposed attention', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='across different models. Thus, our goal of ﬁnding an effec-\\ntive architecture across radically different models is achieved\\nthrough the proposed architecture.\\nFig. 5: Qualitative comparison between the proposed attention\\nmodule and the ‘no skip + no batch norm’ variant, applied to\\nVGG-16. We observe that for the ‘no skip + no batch norm’\\narchitecture, the produced explanation map is more spread out,\\nshowing that even if it performs well on the 100% measures,\\nit fails to precisely identify the salient regions in the image.\\nE. Qualitative Analysis\\nAn extensive qualitative analysis is also performed using the\\nILSVRC 2012 ImageNet dataset in order to gain insight of the\\nFig. 6: TAME applied to ResNet-50 and VGG-16 for the\\nground truth class (Top image: “analog clock” (406), Bottom\\nimage: “padlock” (695). The explanation masks produced\\nusing the VGG-16 are more focused in comparison to the ones\\nof ResNet-50.\\nproposed approach and appreciate its usefulness in real-world\\napplications, e.g., understanding why an image was correctly\\nclassiﬁed or misclassiﬁed. The examples used in this study are\\ndepicted in Figs. 5, 6 and 7.\\nFig. 5 compares TAME generated explanation maps with\\nexplanations generated by the “No skip + No batch norm”\\narchitecture examined in Section IV-D2. The improved ability', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='Fig. 5 compares TAME generated explanation maps with\\nexplanations generated by the “No skip + No batch norm”\\narchitecture examined in Section IV-D2. The improved ability\\nof TAME to identify the salient image regions highlights the\\nimportance of evaluating the method using the AD and IC\\nmeasures on multiple thresholds (Table II), and particularly\\nthe signiﬁcance of the 15% measures over the 100% and 50%\\nones in determining the quality of generated explanations.\\nThe differences between explanations produced using\\nTAME on ResNet-50 and VGG-16 are examined in Fig. 6. We\\nobserve that explanations produced for the ResNet-50 model\\nare generally more activated, and, in general, explanations\\nproduced for the two different CNN types attend different\\nareas of the image. This suggests that ResNet-50 and VGG-16\\nclassify images in fundamentally different ways, focusing on\\ndifferent features of an input image to make their predictions.\\nIn Fig. 7, we provide class-speciﬁc explanation masks\\nreferring to the ground truth class but also to an erroneous\\nbut closely related class, for both ResNet-50 and VGG-16\\nmodels. The ﬁrst image of Fig. 7a, depicts a spoonbill, a bird\\nsimilar to the ﬂamingo. Two signiﬁcant differences between\\nthe spoonbill and the ﬂamingo are the characteristic bill and\\nthe darker pink stripe on the wing of the spoonbill. We can see', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='the spoonbill and the ﬂamingo are the characteristic bill and\\nthe darker pink stripe on the wing of the spoonbill. We can see\\nin the explanation maps of both models, that when choosing\\nthe class ﬂamingo, there is no signiﬁcance attributed to the\\nbill, but, on the other hand, when the spoonbill class is chosen,\\nthe bill area is gaining signiﬁcant attention. By comparing the\\nexplanation maps for adversarial classes, we can gain insight\\ninto important features which characterize a speciﬁc object\\nagainst similar ones, and possibly gain new insight from the\\nclassiﬁer. The second image in Fig. 7a is a similar case.\\nThe examples of Fig. 7b demonstrate the potential of the\\nexplanation maps to be used for explaining multiple different\\nclasses contained in a single image, i.e., the “english fox-\\nhound” and “soccer ball” image, and the “head cabbage” and\\n“butternut squash” image.\\nFinally, in Fig. 7c we provide two cases of images that have\\nbeen miscategorized, and use the explanations to understand\\nwhat has gone wrong. The ﬁrst image of Fig. 7c belongs to the\\n“dingo” class (273) but is evidently misclassiﬁed as “timber\\nwolf” from both CNN models. Using the explanations, we can\\nidentify important features on the image for each class and', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='wolf” from both CNN models. Using the explanations, we can\\nidentify important features on the image for each class and\\nCNN model. The second image depicts a lighthouse. VGG-16\\nmisclassiﬁes this image as a “sundial”. Again, using the expla-\\nnations generated by TAME we can understand which features\\nled the model to produce a wrong decision. For instance, in this\\ncase, we see that for both models the “sundial” explanations\\nfocus on the lighthouse roof, which might resemble a sundial,\\nexplaining the erroneous classiﬁcation decision of VGG-16.\\nV. CONCLUSIONS\\nWe proposed TAME, a novel method for generating visual\\nexplanations for various CNNs. This is accomplished by\\ntraining a hierarchical attention module to extract information\\nfrom feature map sets of multiple layers. Experimental results\\nveriﬁed that TAME outperforms gradient-based methods and\\ncompetes with perturbation-based, while, in contrast to them,\\nrequires only a single forward pass to generate explanations.\\nFurther research is needed to discover the limits of the pro-\\nposed approach, e.g., generalizing it to non-CNN architectures.\\nREFERENCES\\n[1] J. Adebayo, J. Gilmer, et al. Sanity checks for saliency maps. In Proc.\\nNIPS, page 9525–9536, Montr´\\neal, Canada, 2018.', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='NIPS, page 9525–9536, Montr´\\neal, Canada, 2018.\\n[2] J. Amann, A. Blasimme, et al. Explainability for artiﬁcial intelligence\\nin healthcare: a multidisciplinary perspective. BMC Medical Informatics\\nand Decision Making, 20(1):1–9, 2020.\\n[3] E. Apostolidis, G. Balaouras, V. Mezaris, and I. Patras. Summarizing\\nvideos using concentrated attention and considering the uniqueness and\\ndiversity of the video frames.\\nIn Proc. ICMR, pages 407–415, New\\nYork, NY, USA, June 2022.\\n[4] A. B. Arrieta, N. D´\\nıaz-Rodr´\\nıguez, et al. Explainable artiﬁcial intelligence\\n(XAI): Concepts, taxonomies, opportunities and challenges toward re-\\nsponsible ai. Information fusion, 58:82–115, 2020.\\n[5] A. Chattopadhay, A. Sarkar, et al. Grad-CAM++: Generalized gradient-\\nbased visual explanations for deep convolutional networks.\\nIn Proc.\\nIEEE WACV, pages 839–847, 2018.\\n[6] J.-Y. Chiao, K.-Y. Chen, et al. Detection and classiﬁcation the breast\\ntumors using mask R-CNN on sonograms. Medicine, 98(19), 2019.', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='tumors using mask R-CNN on sonograms. Medicine, 98(19), 2019.\\n[7] J. Deng, W. Dong, et al. ImageNet: A large-scale hierarchical image\\ndatabase.\\nIn Proc. IEEE CVPR, pages 248–255, Miami, USA, June\\n2009.\\n[8] A. Englebert, O. Cornu, and C. de Vleeschouwer. Backward recursive\\nclass activation map reﬁnement for high resolution saliency map. In\\nProc. ICPR, 2022.\\n[9] H. Fukui, T. Hirakawa, et al. Attention branch network: Learning of\\nattention mechanism for visual explanation. In Proc. IEEE CVPR, pages\\n10705–10714, 2019.\\n[10] N. Gkalelis, A. Goulas, D. Galanopoulos, and V. Mezaris. ObjectGraphs:\\nUsing objects and a graph convolutional network for the bottom-up\\nrecognition and explanation of events in video. In Proc. IEEE CVPRW,\\npages 3375–3383, June 2021.\\n(a)\\n(b)\\n(c)\\nFig. 7: Explanation for six input images. In each case we display four class-speciﬁc explanations, i.e., of the true (top) and\\nan erroneous (bottom) class prediction of the input image, for both ResNet-50 and VGG-16. Figs. 7a, 7b depict examples of', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='an erroneous (bottom) class prediction of the input image, for both ResNet-50 and VGG-16. Figs. 7a, 7b depict examples of\\nimages with similar classes and with images containing multiple classes, respectively. In Fig. 7c two cases of misclassiﬁcation\\nare provided: dataset misclassiﬁcation (left side example) and model misclasssiﬁcation (right side example).\\n[11] I. Gkartzonika, N. Gkalelis, and V. Mezaris. Learning visual explana-\\ntions for DCNN-based image classiﬁers using an attention mechanism.\\nIn Proc. ECCV, Workshop on Vision with Biased or Scarce Data (VBSD),\\nOct. 2022.\\n[12] R. Hamon, H. Junklewitz, I. Sanchez, et al. Bridging the gap between\\nAI and explainability in the GDPR: Towards trustworthiness-by-design\\nin automated decision-making. IEEE Computational Intelligence Mag-\\nazine, 17(1):72–85, 2022.\\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image\\nrecognition. In Proc. IEEE CVPR, pages 770–778, 2016.\\n[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network\\ntraining by reducing internal covariate shift. In Proc. ICML, pages 448–\\n456, 2015.', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network\\ntraining by reducing internal covariate shift. In Proc. ICML, pages 448–\\n456, 2015.\\n[15] S. Jetley, N. A. Lord, et al. Learn to pay attention. In Proc. ICLR,\\nVancouver, BC, Canada, May 2018.\\n[16] P.-T. Jiang, C.-B. Zhang, et al.\\nLayerCAM: Exploring hierarchical\\nclass activation maps for localization.\\nIEEE Transactions on Image\\nProcessing, 30:5875–5888, 2021.\\n[17] A. Krizhevsky, I. Sutskever, et al. ImageNet classiﬁcation with deep\\nconvolutional neural networks. In F. Pereira, C. J. Burges, L. Bottou,\\nand K. Q. Weinberger, editors, Proc. NIPS, volume 25, 2012.\\n[18] H. Li, Z. Xu, et al. Visualizing the loss landscape of neural nets. Proc.\\nNIPS, 31, 2018.\\n[19] V. Petsiuk, A. Das, and K. Saenko. RISE: randomized input sampling\\nfor explanation of black-box models. In Proc. BMVC, Newcastle, UK,\\nSeptember 2018.\\n[20] W. Samek, G. Montavon, et al. Explaining deep neural networks and\\nbeyond: A review of methods and applications. Proc. IEEE, 109(3):247–', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='September 2018.\\n[20] W. Samek, G. Montavon, et al. Explaining deep neural networks and\\nbeyond: A review of methods and applications. Proc. IEEE, 109(3):247–\\n278, 2021.\\n[21] S. Sattarzadeh, M. Sudhakar, et al. Explaining convolutional neural net-\\nworks through attribution-based input sampling and block-wise feature\\naggregation. In Proc. AAAI, volume 35, pages 11639–11647, 2021.\\n[22] R. R. Selvaraju, M. Cogswell, et al. Grad-CAM: Visual explanations\\nfrom deep networks via gradient-based localization.\\nIn Proc. IEEE\\nICCV, pages 618–626, 2017.\\n[23] K. Simonyan and A. Zisserman. Very deep convolutional networks for\\nlarge-scale image recognition. In Proc. ICLR, San Diego, CA, USA,\\nMay 2015.\\n[24] L. N. Smith. Cyclical learning rates for training neural networks. In\\nProc. IEEE WACV, pages 464–472, 2017.\\n[25] L. N. Smith. A disciplined approach to neural network hyper-parameters:\\nPart 1–learning rate, batch size, momentum, and weight decay. arXiv\\npreprint arXiv:1803.09820, 2018.\\n[26] L. N. Smith and N. Topin. Super-convergence: Very fast training of', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='preprint arXiv:1803.09820, 2018.\\n[26] L. N. Smith and N. Topin. Super-convergence: Very fast training of\\nneural networks using large learning rates. In Proc. Artiﬁcial intelligence\\nand machine learning for multi-domain operations applications, volume\\n11006, pages 369–386, 2019.\\n[27] M. Sudhakar, S. Sattarzadeh, et al. Ada-SISE: adaptive semantic input\\nsampling for efﬁcient explanation of convolutional neural networks. In\\nProc. IEEE ICASSP, pages 1715–1719, 2021.\\n[28] H. Wang, Z. Wang, et al. Score-CAM: Score-weighted visual explana-\\ntions for convolutional neural networks. In Proc. IEEE CVPRW, pages\\n24–25, 2020.\\n[29] B. Zhou, A. Khosla, et al. Learning deep features for discriminative\\nlocalization. In Proc. IEEE CVPR, pages 2921–2929, 2016.', metadata={'Published': '2023-01-18', 'Title': 'TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks', 'Authors': 'Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris', 'Summary': \"The apparent ``black box'' nature of neural networks is a barrier to adoption\\nin applications where explainability is essential. This paper presents TAME\\n(Trainable Attention Mechanism for Explanations), a method for generating\\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\\ncombines a target model's feature maps from multiple layers using an attention\\nmechanism, transforming them into an explanation map. TAME can easily be\\napplied to any convolutional neural network (CNN) by streamlining the\\noptimization of the attention mechanism's training method and the selection of\\ntarget model's feature maps. After training, explanation maps can be computed\\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\\nand ResNet-50, trained on ImageNet and show improvements over previous\\ntop-performing methods. We also provide a comprehensive ablation study\\ncomparing the performance of different variations of TAME's architecture. TAME\\nsource code is made publicly available at https://github.com/bmezaris/TAME\"}), Document(page_content='DMFORMER: CLOSING THE GAP BETWEEN CNN AND VISION TRANSFORMERS\\nZimian Wei1, Hengyue Pan1, Lujun Li2, Menglong Lu1, Xin Niu1, Peijie Dong1, Dongsheng Li1\\n1 College of Computer, National University of Defense Technology\\n2 Chinese Academy of Sciences, Beijing, China\\nABSTRACT\\nVision transformers have shown excellent performance\\nin computer vision tasks. As the computation cost of their\\nself-attention mechanism is expensive, recent works tried\\nto replace the self-attention mechanism in vision transform-\\ners with convolutional operations, which is more efﬁcient\\nwith built-in inductive bias.\\nHowever, these efforts either\\nignore multi-level features or lack dynamic prosperity, lead-\\ning to sub-optimal performance. In this paper, we propose\\na Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel\\nsizes and enables input-adaptive weights with a gating mech-\\nanism.\\nBased on DMA, we present an efﬁcient backbone\\nnetwork named DMFormer.\\nDMFormer adopts the over-\\nall architecture of vision transformers, while replacing the\\nself-attention mechanism with our proposed DMA. Extensive\\nexperimental results on ImageNet-1K and ADE20K datasets\\ndemonstrated that DMFormer achieves state-of-the-art per-\\nformance, which outperforms similar-sized vision transform-\\ners(ViTs) and convolutional neural networks (CNNs).', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='demonstrated that DMFormer achieves state-of-the-art per-\\nformance, which outperforms similar-sized vision transform-\\ners(ViTs) and convolutional neural networks (CNNs).\\nIndex Terms—\\nvision transformer;\\nCNN; attention\\nmechanism; multi-level feature\\n1. INTRODUCTION\\nRecently, vision transformers (ViT) [1, 2] have drawn grow-\\ning attention in computer vision research. Due to the capac-\\nity of modeling long-range dependencies, ViTs are special-\\nized in extracting global features. However, the self-attention\\nmechanism in transformers brings about heavy computation\\ncosts, making them unaffordable for high-resolution down-\\nstream tasks(e.g., semantic segmentation). Although recent\\nlocal vision transformer methods [3, 4] alleviate this problem\\nto some extent, the implementation of cross-window strate-\\ngies in local self-attention is still sophisticated.\\nAs a complementary, convolution neural network(CNN)\\nfocus on capturing local relations with high efﬁciency. With\\nbuilt-in inductive biases, CNNs are easy to train with quick\\nconvergence. To this end, there is a trend to take the mer-\\nits of both CNNs and ViTs by migrating desired properties\\nof ViTs to CNNs, including the overall architecture design,\\nlarge receptive ﬁeld, and data speciﬁcity provided by the at-\\n\\x04\\n\\x02\\n\\x05\\n\\x02\\n\\x06\\n\\x02\\n\\x07\\n\\x02\\n\\x08\\n\\x02\\n\\t\\n\\x02', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='\\x02\\n\\x0b\\n\\x02\\n\\t\\n\\x0b\\n\\x01\\n\\x07\\n\\n\\x02\\n\\x01\\n\\x02\\n\\n\\x02\\n\\x01\\n\\x07\\n\\n\\x03\\n\\x01\\n\\x02\\n\\n\\x03\\n\\x01\\n\\x07\\n\\n\\x04\\n\\x01\\n\\x02\\n\\n\\x04\\n\\x01\\n\\x07\\n\\n\\x05\\n\\x01\\n\\x02\\n\\n\\x05\\n\\x01\\n\\x07\\n\\n\\x06\\n\\x01\\n\\x02\\n\\x08\\n\\x0b\\n\\x0c\\n\\x05\\n\\x06\\n\\x01\\n\\x07\\n\\n\\n\\x0e\\n\\r\\n\\t\\n\\n\\x0f\\n\\x03\\n\\x02\\n\\x04\\n\\n\\x11\\n\\x18\\n\\x11\\n\\x15\\n\\x12\\n\\x1a\\n\\x12\\n\\x18\\n\\x19\\n\\x02\\n\\x07\\n\\x03\\n\\x01\\n\\x0b\\n\\x12\\n\\x13\\n\\x08\\n\\x12\\n\\x1a\\n\\x10\\n\\x01\\n\\x0c\\n\\x1d\\n\\x14\\n\\x16\\n\\x01\\n\\x05\\n\\x17\\n\\x16\\n\\x1c\\n\\x08\\n\\x12\\n\\x0f\\n\\x1a\\n\\x01\\n\\x06\\n\\x12\\n\\x14\\n\\r\\n\\x01\\n\\x0b\\n\\x12\\n\\x19\\n\\x08\\n\\x12\\n\\x1a\\n\\x01', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='\\x0e\\n\\r\\n\\x1c\\n\\x04\\n\\x01\\n\\t\\n\\x1b\\n\\x18\\n\\x19\\nFig. 1.\\nResults of different models on ImageNet-1K valida-\\ntion set. We compare accuracy-parameters trade-off of recent\\nmodels RegNet [9], Swin Transformer [3], ConvNeXt [5],\\nDeiT [2], ResNet [6], PVTv2 [10] and our DMFormer.\\nTable 1. Favorable properties correspond to different mod-\\nules, including self-attention, multi-scale convolution, dilated\\nconvolution, and DMA.\\nProperties\\nSelf-\\nattention\\nMulti-scale\\nConvolution\\nDilated\\nConvolution\\nDMA\\nLocal inductive bias\\n\\x17\\n\\x13\\n\\x13\\n\\x13\\nLarge receptive ﬁeld\\n\\x13\\n\\x17\\n\\x13\\n\\x13\\nMulti-scale Interaction\\n\\x17\\n\\x13\\n\\x13\\n\\x13\\nInput Adaptive\\n\\x13\\n\\x17\\n\\x17\\n\\x13\\ntention mechanism. For example, ConvNext [5] built a pure\\nCNN family based on ResNet [6], which performs on par or\\nslightly better than ViT by learning their training procedure\\nand macro/micro-level architecture designs. RepLKNet [7]\\nadopts as large as 31 × 31 kernel size to enlarge effective\\nreceptive ﬁelds following the design in ViT. Although en-\\ncouraging performance has been achieved by the above meth-\\nods, their computation costs are relatively large. [8] com-', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='receptive ﬁelds following the design in ViT. Although en-\\ncouraging performance has been achieved by the above meth-\\nods, their computation costs are relatively large. [8] com-\\npetes favorably with Swin transformer [3] by replacing the\\nlocal self-attention layer with the dynamic depth-wise convo-\\nlution layer, while keeping the overall structure unchanged.\\nHowever, the lack of multi-level features limits its capacity to\\nachieve better performance.\\nIn this paper, we propose a new dynamic multi-level at-\\narXiv:2209.07738v3  [cs.CV]  29 Nov 2022\\n1x1 Conv\\n1x1 Conv\\n1x1 Conv\\nSoftmax\\n1x1 Conv\\n(a) Self-Attention\\n1x1 Conv\\n1x1 Conv\\nGAP\\nFC\\nFC\\n(d) DMA\\n1x1 Conv\\n1x1 Conv\\n1x1 Conv\\n(b) Multi-scale Convolution\\n3x3 DwConv 5x5 DwConv 7x7 DwConv\\n1x1 Conv\\n1x1 Conv\\n3x3 DwConv', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='5x5 DwConv\\nDilate rate=2\\n\\n7x7 DwConv\\nDilate rate=3\\nGating Mechnism\\n3x3 DwConv\\n\\n5x5 DwConv\\nDilate rate=2', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='7x7 DwConv\\nDilate rate=3\\n(c) Dilated Convolution\\nFig. 2. The structure comparison between related modules: self-attention (a), multi-scale convolution (b), dilated convolution\\n(c), and DMA (d). DwConv, GAP, σ, and FC refer to depth-wise convolution, global average pool, sigmoid function, and fully\\nconnected layer, respectively. C refers to the number of channels, while r is the expansion ratio in the DMA.\\ntention (DMA) mechanism (see Table 1 and Fig. 2 for more\\ndetails). Firstly, DMA is characterized by applying multi-\\nple kernel sizes for different resolution patterns. As men-\\ntioned in [11], multiple kernel sizes can effectively improve\\nthe model’s performance. Secondly, DMA involves dilated\\nconvolutions to efﬁciently enlarge the receptive ﬁeld. Thirdly,\\nwe design a lightweight gating mechanism in DMA to provide\\ninput adaptability, with which the channel-wise relationships\\nare captured and the representational power of the network\\nis enhanced. Based on DMA, we extend the architecture of\\nSwin Transformer [3] and propose a new framework named\\nDMFormer. The experimental results show that DMFormer\\nachieves state-of-the-art performance on ImageNet classiﬁca-\\ntion (see Fig. 1) and semantic segmentation tasks.\\nIn summary, the contributions of this paper have two\\naspects: 1) We propose a new attention mechanism named', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='tion (see Fig. 1) and semantic segmentation tasks.\\nIn summary, the contributions of this paper have two\\naspects: 1) We propose a new attention mechanism named\\nDMA, which combines the advantages of convolution and\\nself-attention.\\n2) Based on DMA, we design a backbone\\nmodel named DMFormer.\\nExtensive experiments on Ima-\\ngeNet classiﬁcation and semantic segmentation tasks show\\nthe superior of DMFormer over other existing models.\\n2. APPROACHES\\nIn this section, we ﬁrst present details of DMA. Then we show\\nthe overall structure and present different architecture designs\\nin DMFormer family.\\n2.1. DMA\\nWe illustrate the structure of DMA in Fig. 2 (d). The two\\nkey components in DMA are the multi-scale dilated convolu-\\ntion and gating mechanism. Multi-scale dilated convolution\\ncaptures different patterns with various resolutions of input\\nimages, while the gating mechanism learns to selectively em-\\nphasize informative features by re-calibration.\\nAssuming that the input of DMA is X. As depicted in\\nFig. 2 (d), a 1 × 1 convolution layer (Convexp r1 × 1) is\\napplied to expand the number of channels by r times. Then, a\\nparallel design of 3 × 3, 5 × 5, 7 × 7 depth-wise convolution\\nis introduced to learn multi-scale features. BatchNorm and\\nReLU are followed to prevent over-ﬁt when training. Next, in', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='is introduced to learn multi-scale features. BatchNorm and\\nReLU are followed to prevent over-ﬁt when training. Next, in\\norder to apply a residual connection for better optimization,\\nwe apply a 1 × 1 convolution layer to reduce the number of\\nchannels as the original input X. The above operators can be\\nexpressed as follows:\\nXE = Convexp r1 × 1 (X) ,\\nX1, X2,X3 = Parallel3×3,5×5,7×7 (XE) ,\\nXP = ReLU (BN (X1 + X2 + X3)) ,\\nX′ = X + Conv1 × 1 (XP ) ,\\n(1)\\nwhere Parallel3×3,5×5,7×7 contains multi-branch of 3 ×\\n3, 5 × 5, 7 × 7 convolution layers. Speciﬁcally, for branches\\nwith kernel size 5 × 5 and 7 × 7, we set the corresponding\\ndilated rates as 2, 3 to obtain larger receptive ﬁeld.\\nFor the gating mechanism, we apply a global average\\npooling (GAP) layer to obtain global information, followed\\nby two successive fully connected layers. At last, a sigmoid\\nfunction is applied to compute the attention vector. The oper-\\nations in the gating mechanism can be formulated as follows:\\nV = ReLU (FC (GAP (X))) ,\\nAttn = Sigmoid (FC (V )) ,\\n(2)', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='ations in the gating mechanism can be formulated as follows:\\nV = ReLU (FC (GAP (X))) ,\\nAttn = Sigmoid (FC (V )) ,\\n(2)\\nFinally, the output of DMA is obtained by re-calibrating\\nthe fused feature X′ with the gating mechanism as follows:\\nOutput = Attn ⊗Conv1 × 1\\n\\x00X′\\x01\\n,\\n(3)\\nwhere ⊗means the element-wise matrix multiplication.\\nIn the following section, we will introduce the basic\\nblock, overall framework, and detailed architectural design in\\nDMFormer family.\\n2.2. DMFormer\\nWe build DMFormer with a hierarchical design similar to tra-\\nditional CNNs [6] and recent local vision transformers [3].\\nTable 2. The detailed setting for different versions of DM-\\nFormer. ER, r represent the expansion ratio in the MLP mod-\\nule and DMA module, respectively.\\nstage\\noutput size\\nER\\nr\\nDMFormer-S\\nDMFormer-L\\n1\\nH\\n4 × W\\n4 × C1\\n8\\n4\\nC1 = 64\\nN1 = 2\\nC1 = 64\\nN1 = 3\\n2\\nH\\n8 × W\\n8 × C2\\n8\\n4\\nC2 = 128\\nN2 = 2\\nC2 = 128\\nN2 = 3\\n3\\nH\\n16 × W\\n16 × C3\\n4\\n4\\nC3 = 320\\nN3 = 6\\nC3 = 320\\nN3 = 12\\n4\\nH', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='N2 = 3\\n3\\nH\\n16 × W\\n16 × C3\\n4\\n4\\nC3 = 320\\nN3 = 6\\nC3 = 320\\nN3 = 12\\n4\\nH\\n32 × W\\n32 × C4\\n4\\n4\\nC4 = 512\\nN4 = 2\\nC4 = 512\\nN4 = 3\\nParameters (M)\\n26.7\\n45.0\\nFLOPs (G)\\n5.0\\n8.7\\nConv Stem\\nBlock\\nInput\\nStage 1\\nConv 3x3, Stride=2\\nConv 3x3, Stride=2\\nConv 3x3, Stride=2\\nStage 2\\nStage 3\\nStage 4\\nLinear\\n(b) Block\\nBlock\\nBlock\\nBlock\\n(a) Overall Framework\\nMLP\\nBatch Norm\\nDMA\\nBatch Norm\\nFig. 3. (a) The overall framework of DMFormer. Follow-\\ning [3], DMFormer adopts the hierarchical architecture with\\n4 stages, and each stage consists of multiple blocks. Ci, Ni\\nrefer to the feature dimension and block number in stage i,\\nrespectively. (b) The basic block in DMFormer. We apply the\\nmodular design in vision transformers, while replacing the\\nself-attention layer with DMA.\\nFig. 3 (a) presents the overall framework of DMFormer and\\nFig. 3 (b) shows the basic block in DMFormer.\\nThe input image I is ﬁrst processed by the convolution', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='Fig. 3 (a) presents the overall framework of DMFormer and\\nFig. 3 (b) shows the basic block in DMFormer.\\nThe input image I is ﬁrst processed by the convolution\\nstem module, which consists of a 7×7 convolution layer with\\na stride of 2, a 3 × 3 convolution layer with a stride of 1, and\\na non-overlapping 2 × 2 convolution layer with a stride of 2.\\nThen the spatial size of output features after the convolution\\nstem module is H\\n4 × W\\n4 .\\nX = ConvStem(I),\\n(4)\\nwhere X ∈RN×C1× H\\n4 × W\\n4 is the output feature of the con-\\nvolution stem module. N, C1 is the batch size and number\\nof channels. Then X is fed to repeated DMFormer blocks,\\neach of which consists of two sub-blocks. Speciﬁcally, the\\nmain components of the ﬁrst sub-block include DMA and the\\nBatchNorm module, which we present as\\nY = DMA(BN(X)) + X,\\n(5)\\nwhere BN(·) denotes a Batch Normalization.\\nDetails of\\nDMA(·) are depicted in Section 2.1. The second sub-block\\nconsists of two fully-connected layers and a non-linear activa-\\ntion GELU [12]. The output of the MLP module is formulated\\nas follows:\\nZ = GELU(W1(BN(Y ))W2 + Y,', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='tion GELU [12]. The output of the MLP module is formulated\\nas follows:\\nZ = GELU(W1(BN(Y ))W2 + Y,\\n(6)\\nTable 3. Compare with the state-of-the-art methods on the\\nImageNet validation set. Params means parameter. GFLOPs\\ndonates ﬂoating point operations.\\nMethod\\nParams. (M)\\nGFLOPs\\nTop-1 Acc (%)\\nPVT-Small [18]\\n24.5\\n3.8\\n79.8\\nSwin-T [3]\\n28.3\\n4.5\\n81.3\\nPoolFormer-S36 [19]\\n31.0\\n5.2\\n81.4\\nTwins-SVT-S [20]\\n24.0\\n2.8\\n81.7\\nFocal-T [14]\\n29.1\\n4.9\\n82.2\\nConvNeXt-T [5]\\n28.6\\n4.5\\n82.1\\nDMFormer-S\\n26.7\\n5.0\\n82.8\\nPVT-Medium [18]\\n44.2\\n6.7\\n81.2\\nFocal-S [14]\\n51.1\\n9.1\\n83.5\\nSwin-S [3]\\n49.6\\n8.7\\n83.0\\nConvNeXt-S [5]\\n50.0\\n8.7\\n83.1\\nDMFormer-L\\n45.0\\n8.7', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='49.6\\n8.7\\n83.0\\nConvNeXt-S [5]\\n50.0\\n8.7\\n83.1\\nDMFormer-L\\n45.0\\n8.7\\n83.6\\nwhere W1 ∈RCi×eCi and W2 ∈ReCi×Ci are learnable\\nparameters in fully connected layers. e is the MLP expansion\\nratio for the number of channels; Ci is the number of channels\\nin the corresponding stage.\\nBased on the above DMFormer block, we formulate\\nDMFormer-S and DMFormer-L with different model sizes.\\nThe numbers of channels corresponding to the four stages are\\nidentical for both DMFormer-S and DMFormer-L, which are\\nset as 64, 128, 320, and 512. Differently, DMFormer-L is\\nlarger with more block numbers. Speciﬁcally, stages 1, 2, 3,\\nand 4 of DMFormer-S contain 2, 2, 6, 2 blocks, while that\\nfor DMFormer-L are 3, 3, 12, 3, respectively. Their detailed\\narchitecture designs are shown in Table 2.\\n3. EXPERIMENTS\\n3.1. Image classiﬁcation\\nWe conduct image classiﬁcation experiments on ImageNet-\\n1K [13]. Each model is trained for 300 epochs with AdamW\\noptimizer, and a total batch size of 1024 on 8 GPUs. The\\ninitial learning rate is set as 1e −3. We adopt the cosine', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='1K [13]. Each model is trained for 300 epochs with AdamW\\noptimizer, and a total batch size of 1024 on 8 GPUs. The\\ninitial learning rate is set as 1e −3. We adopt the cosine\\ndecay schedule to adjust the learning rate during training.\\nTable 3 summarizes the experimental results on ImageNet-\\n1K classiﬁcation. Comparing with recently well-established\\nViTs like Swin-S [3] and Focal-S [14], DMFormer consis-\\ntently shows better performance. Speciﬁcally, DMFormer-L\\nsurpasses Swin-S, Focal-S by 0.6%, 0.1% top-1 accuracy with\\n9%, 12% fewer parameters. Comparatively, ConvNeXt [5] is\\nan excellent CNN that learns architecture designs and training\\nschedules from ViTs for better performance. DMFormer-L\\noutperforms ConvNeXt-S by 0.5%, while reducing the model\\nsize by 10%. Moreover, in Fig. 4 (a), we utilize Grad-CAM\\n[15] to localize discriminative regions generated by Swin-\\nT [16], ResNet50 [6] and DMFormer-S. It can be observed\\nthat the highlighted class activation area of DMFormer-S is\\nmore accurate.\\nThe experimental results demonstrate the\\nsuperiority of DMFormer.\\nResNet 50\\nSwin-T\\nOurs\\nInput\\n（a）\\n（b）', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='more accurate.\\nThe experimental results demonstrate the\\nsuperiority of DMFormer.\\nResNet 50\\nSwin-T\\nOurs\\nInput\\n（a）\\n（b）\\nFig. 4. (a) Grad-CAM [15] results generated by Swin-T [3],\\nResNet50 [6] and ours. The images are randomly selected\\nfrom the ImageNet validation dataset. Lighter colors in Grad-\\nCAM results refer to stronger attention regions. (b) Visualiza-\\ntion results of semantic segmentation on ADE20K [17] with\\nImageNet pre-trained DMFormer-S as the backbone.\\n3.2. Semantic segmentation\\nWe evaluate models for the semantic segmentation task on\\nADE20K [17]. mIoU (mean Intersection Over Union) is ap-\\nplied to measure the model performance. Semantic FPN [21]\\nand UperNet [22] are used as main frameworks to evaluate the\\nDMFormer-S backbone. Pre-trained weights on ImageNet-\\n1K are utilized to initialize our backbone.\\nWe train each\\nmodel with AdamW optimizer, a total batch size of 16 on\\n8 GPUs. When equipped with Semantic FPN [21], we use the\\n40K-iteration training scheme in [18, 19]. The learning rate\\nis set as 2 × 10−4 and decays by polynomial schedule with\\na power of 0.9. Comparatively, we adopt the 160K-iteration', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='is set as 2 × 10−4 and decays by polynomial schedule with\\na power of 0.9. Comparatively, we adopt the 160K-iteration\\ntraining scheme in [3] for UperNet. Speciﬁcally, the learning\\nrate is set as 6 × 10−5 with 1500 iteration warmup and linear\\nlearning rate decay.\\nTable 4 lists the performance of different backbones us-\\ning FPN and UperNet. Generally, DMFormer consistently\\noutperforms other state-of-the-art backbones.\\nWhen using\\nSemantic FPN for semantic segmentation, DMFormer-S sur-\\npasses VAN-B2 [24] by 0.5 mIoU with similar computation\\ncost. Moreover, although the parameter of DMFormer-S is\\n37% smaller than that of PVT-Medium [18], the mIoU is still\\n5.6 points higher (47.2 vs 41.6). When applying UperNet as\\nthe framework, our model outperforms ConvNeXt-T [5] by\\n0.9 mIOU, with about 6% model size reduction. Addition-\\nally, DMFormer-S is 1.8 mIOU higher than Focal-T [14] with\\n9% decreased parameters. In Fig. 4 (b), we present visualiza-\\ntion of semantic segmentation results applying UperNet [22].\\nThe results indicate the powerful capacity of DMFormer for\\nthe semantic segmentation task.\\nTable 4. Results of semantic segmentation on ADE20K [17]', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='tion of semantic segmentation results applying UperNet [22].\\nThe results indicate the powerful capacity of DMFormer for\\nthe semantic segmentation task.\\nTable 4. Results of semantic segmentation on ADE20K [17]\\nvalidation set.\\nThe pre-trained DMFormer-S is applied as\\nthe backbone and plugged in Semantic FPN [21] and Uper-\\nNet [22] frameworks. Note that the difference in the number\\nof parameters between two DMFormer-S is due to the usage\\nof semantic FPN and UperNet.\\nMethod\\nBackbone\\n#Param (M)\\nmIoU (%)\\nSemantic FPN [21]\\nResNet101 [6]\\n47.5\\n38.8\\nResNeXt101-32x4d [23]\\n47.1\\n39.7\\nPoolFormer-S36 [19]\\n34.6\\n42.0\\nPVT-Medium [18]\\n48.0\\n41.6\\nTwinP-S [20]\\n28.4\\n44.3\\nVAN-B2 [24]\\n30.0\\n46.7\\nDMFormer-S\\n30.4\\n47.2\\nUperNet [22]\\nConvNeXt-T [5]\\n60.0\\n46.7\\nSwin-T [3]\\n60.0\\n46.1\\nTwinP-S [20]\\n54.6\\n46.2\\nFocal-T [14]\\n62.0\\n45.8\\nDMFormer-S\\n56.6', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='60.0\\n46.1\\nTwinP-S [20]\\n54.6\\n46.2\\nFocal-T [14]\\n62.0\\n45.8\\nDMFormer-S\\n56.6\\n47.6\\nTable 5. Ablation study of different modules in DMA. Block\\nnumbers are modiﬁed to achieve comparable computation\\ncomplexity. We adopt Semantic FPN [21] as main frame-\\nwork, and use ImageNet pre-trained DMFormer-S variants as\\nbackbone for semantic segmentation. Parameters and FLOPs\\nare calculated under the ImageNet classiﬁcation setting.\\nVariants\\nBlocks\\nParams.\\nFLOPs\\nImageNet\\nADE20k\\n(M)\\n(G)\\nTop-1 Acc\\nmIoU\\nw/o Expansion rate r\\n2, 2, 12, 2\\n26.9\\n5.0\\n82.8\\n46.3\\nw/o Multi-scale Conv\\n2, 2, 6, 2\\n26.2\\n4.9\\n82.5\\n46.0\\nw/o Dilation\\n2, 2, 6, 2\\n26.7\\n5.0\\n82.4\\n46.4\\nw/o Gating Mechanism\\n2, 2, 7, 2\\n26.2\\n5.4\\n82.7\\n45.7\\nDMFormer-S\\n2, 2, 6, 2\\n26.7\\n5.0\\n82.8\\n47.2\\n3.3. Ablation studies', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='5.4\\n82.7\\n45.7\\nDMFormer-S\\n2, 2, 6, 2\\n26.7\\n5.0\\n82.8\\n47.2\\n3.3. Ablation studies\\nWe conduct ablation studies on each component of DMA\\nmodule to shed light on various architecture designs. DMFormer-\\nS is adopted as the baseline model. In the w/o multi-scale\\nConv variant, we only keep a single convolution branch with\\n7 × 7 kernel size. For w/o Expansion rate variant, we set r in\\nthe DMA (see Table 2 and Fig. 2) as 1. To achieve compa-\\nrable model complexity, we modify block numbers in some\\nvariants. The experimental results in Table 5 indicate that all\\ncomponents in DMA are non-trivial to improve performance.\\n4. CONCLUSION\\nIn this paper, we have introduced DMFormer, a novel efﬁcient\\nvision backbone that outperforms most similar-sized state-of-\\nthe-art models on ImageNet classiﬁcation and semantic seg-\\nmentation tasks. We have also presented DMA, the key com-\\nponent of DMFormer which shows much efﬁciency over the\\nexisting self-attention mechanisms. Future work may include\\nadapting DMFormer to other tasks such as video processing\\nand object detection, or extending DMA to a hybrid design\\nwith existing excellent CNN or self-attention mechanisms to\\nfurther enhance performance.\\n5. REFERENCES\\n[1] Alexey', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='and object detection, or extending DMA to a hybrid design\\nwith existing excellent CNN or self-attention mechanisms to\\nfurther enhance performance.\\n5. REFERENCES\\n[1] Alexey\\nDosovitskiy,\\nLucas\\nBeyer,\\nAlexander\\nKolesnikov, and Dirk Weissenborn,\\n“An image is\\nworth 16x16 words: Transformers for image recogni-\\ntion at scale,” in International Conference on Learning\\nRepresentations, 2020.\\n[2] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\\nand Herv´\\ne J´\\negou, “Training data-efﬁcient image trans-\\nformers & distillation through attention,”\\nin Interna-\\ntional Conference on Machine Learning. PMLR, 2021,\\npp. 10347–10357.\\n[3] Ze Liu, Yutong Lin, Yue Cao, and Baining Guo,\\n“Swin transformer: Hierarchical vision transformer us-\\ning shifted windows,” in Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision (ICCV),\\nOctober 2021, pp. 10012–10022.\\n[4] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei\\nCheng, Gang Yu, and Bin Fu, “Shufﬂe transformer: Re-\\nthinking spatial shufﬂe for vision transformer,” arXiv\\npreprint arXiv:2106.03650, 2021.', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='thinking spatial shufﬂe for vision transformer,” arXiv\\npreprint arXiv:2106.03650, 2021.\\n[5] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Fe-\\nichtenhofer, Trevor Darrell, and Saining Xie, “A convnet\\nfor the 2020s,” arXiv preprint arXiv:2201.03545, 2022.\\n[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\\nSun, “Deep residual learning for image recognition,” in\\nCVPR, 2016, pp. 770–778.\\n[7] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and\\nGuiguang Ding, “Scaling up your kernels to 31x31: Re-\\nvisiting large kernel design in cnns,” in CVPR, 2022, pp.\\n11963–11975.\\n[8] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng,\\nJiaying Liu, and Jingdong Wang, “On the connection\\nbetween local attention and dynamic depth-wise convo-\\nlution,” in International Conference on Learning Rep-\\nresentations, 2021.\\n[9] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\\nKaiming He, and Piotr Doll´\\nar, “Designing network de-\\nsign spaces,” 2020, pp. 10428–10436.', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='Kaiming He, and Piotr Doll´\\nar, “Designing network de-\\nsign spaces,” 2020, pp. 10428–10436.\\n[10] Wenhai Wang, Enze Xie, Tong Lu, Ping Luo, and Ling\\nShao, “Pvtv2: Improved baselines with pyramid vision\\ntransformer,” arXiv preprint arXiv:2106.13797, 2021.\\n[11] Mingxing Tan and Quoc V Le,\\n“Mixconv: Mixed\\ndepthwise convolutional kernels,”\\narXiv preprint\\narXiv:1907.09595, 2019.\\n[12] Dan Hendrycks and Kevin Gimpel,\\n“Gaussian error\\nlinear units (gelus),” arXiv preprint arXiv:1606.08415,\\n2016.\\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei, “Imagenet: A large-scale hierarchical\\nimage database,” in 2009 IEEE conference on computer\\nvision and pattern recognition. Ieee, 2009, pp. 248–255.\\n[14] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang\\nDai, Bin Xiao, Lu Yuan, and Jianfeng Gao, “Focal self-\\nattention for local-global interactions in vision trans-\\nformers,” arXiv preprint arXiv:2107.00641, 2021.', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='attention for local-global interactions in vision trans-\\nformers,” arXiv preprint arXiv:2107.00641, 2021.\\n[15] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek\\nDas, Ramakrishna Vedantam, Devi Parikh, and Dhruv\\nBatra, “Grad-cam: Visual explanations from deep net-\\nworks via gradient-based localization,” 2017, pp. 618–\\n626.\\n[16] Ze Liu, Yutong Lin, Yue Cao, Stephen Lin, and Bain-\\ning Guo, “Swin transformer: Hierarchical vision trans-\\nformer using shifted windows,” 2021.\\n[17] Bolei Zhou, Hang Zhao, and Antonio Torralba, “Scene\\nparsing through ade20k dataset,” in CVPR, 2017, pp.\\n633–641.\\n[18] Wenhai Wang, Enze Xie, Ping Song, and Ling Shao,\\n“Pyramid vision transformer: A versatile backbone for\\ndense prediction without convolutions,” in ICCV, Octo-\\nber 2021, pp. 568–578.\\n[19] Weihao Yu, Mi Luo, and Shuicheng Yan, “Metaformer\\nis actually what you need for vision,” in CVPR, 2022,\\npp. 10819–10829.\\n[20] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang,', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='is actually what you need for vision,” in CVPR, 2022,\\npp. 10819–10829.\\n[20] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang,\\nHaibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua\\nShen, “Twins: Revisiting the design of spatial attention\\nin vision transformers,” vol. 34, 2021.\\n[21] Alexander Kirillov, Ross Girshick, Kaiming He, and Pi-\\notr Doll´\\nar, “Panoptic feature pyramid networks,” 2019,\\npp. 6399–6408.\\n[22] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang,\\nand Jian Sun, “Uniﬁed perceptual parsing for scene un-\\nderstanding,” 2018, pp. 418–434.\\n[23] Saining Xie, Ross Girshick, Piotr Doll´\\nar, Zhuowen Tu,\\nand Kaiming He, “Aggregated residual transformations\\nfor deep neural networks,” in CVPR, 2017, pp. 1492–\\n1500.\\n[24] Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-\\nMing Cheng, and Shi-Min Hu, “Visual attention net-\\nwork,” arXiv preprint arXiv:2202.09741, 2022.', metadata={'Published': '2022-11-29', 'Title': 'DMFormer: Closing the Gap Between CNN and Vision Transformers', 'Authors': 'Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li', 'Summary': 'Vision transformers have shown excellent performance in computer vision\\ntasks. As the computation cost of their self-attention mechanism is expensive,\\nrecent works tried to replace the self-attention mechanism in vision\\ntransformers with convolutional operations, which is more efficient with\\nbuilt-in inductive bias. However, these efforts either ignore multi-level\\nfeatures or lack dynamic prosperity, leading to sub-optimal performance. In\\nthis paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which\\ncaptures different patterns of input images by multiple kernel sizes and\\nenables input-adaptive weights with a gating mechanism. Based on DMA, we\\npresent an efficient backbone network named DMFormer. DMFormer adopts the\\noverall architecture of vision transformers, while replacing the self-attention\\nmechanism with our proposed DMA. Extensive experimental results on ImageNet-1K\\nand ADE20K datasets demonstrated that DMFormer achieves state-of-the-art\\nperformance, which outperforms similar-sized vision transformers(ViTs) and\\nconvolutional neural networks (CNNs).'}), Document(page_content='JOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n1\\nSpeech Emotion Recognition Via CNN-Transforemr\\nand Multidimensional Attention Mechanism\\nXiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng\\nAbstract—Speech Emotion Recognition (SER) is crucial in\\nhuman-machine interactions. Mainstream approaches utilize\\nConvolutional Neural Networks or Recurrent Neural Networks\\nto learn local energy feature representations of speech segments\\nfrom speech information, but struggle with capturing global\\ninformation such as the duration of energy in speech. Some use\\nTransformers to capture global information, but there is room\\nfor improvement in terms of parameter count and performance.\\nFurthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal\\ninformation in speech. In this paper, to model local and global\\ninformation at different levels of granularity in speech and cap-\\nture temporal, spatial and channel dependencies in speech signals,\\nwe propose a Speech Emotion Recognition network based on\\nCNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing\\nlocal information in speech from a time-frequency perspective.\\nIn addition, a time-channel-space attention mechanism is used to\\nenhance features across three dimensions. Moreover, we model\\nlocal and global dependencies of feature sequences using large\\nconvolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='enhance features across three dimensions. Moreover, we model\\nlocal and global dependencies of feature sequences using large\\nconvolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed\\nmethod on IEMOCAP and Emo-DB datasets and show our\\napproach significantly improves the performance over the state-\\nof-the-art methods1.\\nIndex Terms—Speech emotion recognition, temporal-channel-\\nspatial attention, lightweight convolution transformer, local global\\nfeature fusion.\\nI. INTRODUCTION\\nE\\nMOTION recognition has significant importance in var-\\nious fields, especially in increasingly common human-\\ncomputer interaction systems [1], and speech emotion recogni-\\ntion (SER) has promising applications in areas such as mental\\nhealth monitoring [2], educational assistance, personalized\\ncontent recommendation, and customer service quality mon-\\nitoring. Speech contains rich emotional information, and as\\nCorresponding author: Xiaoyu Tang. E-mail address: tangxy@scnu.edu.cn.\\nXiaoyu Tang is with the School of Electronic and Information Engineering,\\nFaculty of Engineering, South China Normal University, Foshan, Guangdong\\n528225, China, and also with the School of Physics and Telecommunica-\\ntions Engineering, South China Normal University, Guangzhou, Guangdong\\n510000, China.\\nYixin Lin is with the School of Electronic and Information Engineering,\\nFaculty of Engineering, South China Normal University, Foshan, Guangdong\\n528225, China.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='510000, China.\\nYixin Lin is with the School of Electronic and Information Engineering,\\nFaculty of Engineering, South China Normal University, Foshan, Guangdong\\n528225, China.\\nTing Dang is with the Nokia Bell Labs, Cambridge, UK.\\nYuanfang Zhang is with the Autocity (Shenzhen) Autonomous Driving\\nCo.,ltd.\\nJintao Cheng is with the School of Physics and Telecommunications En-\\ngineering, South China Normal University, Guangzhou, Guangdong 510000,\\nChina.\\n1Our\\ncode\\nis\\navailable\\non\\nhttps://github.com/SCNU-RISLAB/CNN-\\nTransforemr-and-Multidimensional-Attention-Mechanism\\none of the most basic human communication methods, speech\\nemotion recognition is particularly important for computers to\\nanalyze and respond to the emotional state of human users and\\nrespond to them accordingly. With the rapid development of\\nartificial intelligence, speech emotion recognition has received\\nextensive research attention. Human speech contains a wealth\\nof information, including not only the language content but\\nalso attributes such as gender and emotions of the speaker.\\nIt is of great significance to accurately identify emotional\\ninformation from speech signals.\\nFeature extraction of speech is a rather important and\\nchallenging task in speech emotion recognition, and the ex-\\ntraction of features directly affects the effectiveness of subse-\\nquent model training and the accuracy of the final algorithm\\nfor emotion recognition. Speech features can be categorized\\nas acoustic-based features and deep learning-based features', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='traction of features directly affects the effectiveness of subse-\\nquent model training and the accuracy of the final algorithm\\nfor emotion recognition. Speech features can be categorized\\nas acoustic-based features and deep learning-based features\\nwhere acoustic-based features can be broadly classified into\\nrhythmic features [3], spectral-based correlation features [4],\\nand phonetic features [5]. Among them, spectral-based corre-\\nlation features reflect the characteristics of the signal in the\\nfrequency domain, where there are differences in the perfor-\\nmance of different emotions in the frequency domain. Based\\non the spectral correlation features include linear spectrum\\n[6] and inverse spectrum [7], Linear Prediction Cofficients\\n(LPC), Log Frequency Power Coefficients (LFPC), etc.; In-\\nverse spectrum includes Mel-Frequency Cepstrum Coefficients\\n(MFCC), Linear Prediction Cepstrum Cofficients (LPCC), etc.\\nAmong them, MFCC is regarded as a low-level feature based\\non human knowledge, which is widely used in the field of\\nspeech.\\nEarly SER algorithms mainly used acoustic-based features\\nand combined with traditional machine learning algorithms,\\nincluding hidden Markov models [8], Gaussian mixture mod-\\nels [9], and support vector machines [10]. In recent years,\\ndeep learning-based neural networks have gradually become\\nactive in the field of speech emotion recognition [11], [12], and\\ncompared with traditional models, deep learning-based models\\nhave shown better performance in speech emotion recognition.\\nDeep learning-based features use neural networks to learn', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='active in the field of speech emotion recognition [11], [12], and\\ncompared with traditional models, deep learning-based models\\nhave shown better performance in speech emotion recognition.\\nDeep learning-based features use neural networks to learn\\nmore advanced features from the original signal of speech or\\nsome low-dimensional acoustic features. Convolutional neural\\nnetworks (CNNs) are effective in capturing local acoustic\\ndetails in speech, while long short-term memory networks\\n(LSTMs) are widely used in speech emotion recognition for\\nmodeling dynamic information and temporal dependencies in\\nspeech. Additionally, attention mechanisms are also a key\\n0000–0000/00$00.00 © 2021 IEEE\\narXiv:2403.04743v1  [eess.AS]  7 Mar 2024\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n2\\nfactor in improving model performance, as they can adaptively\\nfocus on the importance of different features to obtain better\\nspeech features at the discourse level. For example, Qi et\\nal. [13] proposed a hierarchical network based on static and\\ndynamic features, which uses LSTM to encode dynamic and\\nstatic features of speech and designs a gating model to fuse\\nthe features, an attention mechanism is used to acquire the\\ndiscourse-level speech features. Liu et al. [14] proposed a\\nlocal-global perceptual depth representation learning system.\\nOne module contains a multiscale CNN and a time-frequency\\nCNN (TFCNN) to learn the local representation, and in the', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='local-global perceptual depth representation learning system.\\nOne module contains a multiscale CNN and a time-frequency\\nCNN (TFCNN) to learn the local representation, and in the\\nother module, a Capsule network with an improved routing\\nalgorithm is utilized to design a multi-block dense connection\\nstructure, which can learn both shallow and deep global\\ninformation.\\nAlthough speech emotion recognition (SER) models com-\\nposed of CNNs exhibit better performance than traditional\\nmodels, these networks can only extract local information in\\nspeech, such as the energy and rhythm of a particular segment\\nof speech, while struggling to learn global information in\\nspeech features, such as the overall volume and speaking rate\\nof the speaker, and the duration of energy, thus neglecting the\\nglobal correlation of features [15]. In recent years, the trans-\\nformer [16] based on the self-attentive mechanism has been\\nwidely used in major deep learning tasks. Tarantino et al. [17]\\nused transformer in combination with global windowing for\\nspeech emotion recognition and achieved better performance,\\nbut transformer is weak for local feature extraction. Some\\nrecent work has attempted to combine CNN and transformer\\nto alleviate the limitations of using CNN and transformer\\nalone. Wang et al. [18] stacked the transformer blocks after\\nthe CNN model to improve the global features of aggregation.\\nA model combining the transformer and CNN is proposed in\\n[19], enabling it to learn local information while capturing\\nglobal dependencies. The original transformer tends to have a\\nhigh number of parameters for computing multi-headed self-\\nattention, which requires a lot of resources and poses some', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='[19], enabling it to learn local information while capturing\\nglobal dependencies. The original transformer tends to have a\\nhigh number of parameters for computing multi-headed self-\\nattention, which requires a lot of resources and poses some\\ndifficulties in the training of the network. In addition, this\\nkind of work tends to stack transformers in the last part of\\nthe model or a simple combination of transformer and CNN,\\nwhich makes it hard to obtain better local information of the\\nspeech.\\nIn addition, attention mechanisms improve the effectiveness\\nof task processing by selectively attending to features that are\\nmost relevant to the current task, and have received widespread\\nattention in major fields. In recent years, several researchers\\nhave utilized deep learning methods for feature extraction\\nand used attention mechanisms to improve performance. A\\nlightweight self-attention module is proposed in [20], which\\nuses MLP to extract channel information and a large perceptual\\nfield extended CNN to extract spatial contextual information.\\nGuo et al. [21] proposed an attention mechanism based on\\ntime, frequency, and CNN channels to improve representa-\\ntion learning ability. However, temporal information is often\\nembedded in speech, which reflects the dynamic changes of\\nspeech, such as pitch and energy variations over time. Tem-\\nporal features can reflect the temporal context and evolution\\nof emotion expression in speech. However, it falls short in\\ncapturing the temporal information present in speech, which\\nrepresents the dynamics of speech and plays a crucial role in\\nemotion recognition. This limitation is a common issue in both\\nMLPs and CNNs.\\nIn this paper, we investigate how to effectively combine', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='represents the dynamics of speech and plays a crucial role in\\nemotion recognition. This limitation is a common issue in both\\nMLPs and CNNs.\\nIn this paper, we investigate how to effectively combine\\ntransformer and CNN and apply them to SER to characterize\\nlocal features and global features in speech signals, and\\npropose the temporal-channel-space attention mechanism in\\nthe model for multiple dimensions of feature enhancement.\\nSpecifically, we first use a set of stacked CNNs to capture local\\ninformation in speech and learn shallow features of speech for\\nthe transformer module for better training of the transformer\\nmodule. In the stacked CNN module, two sets of convolutional\\nfilters of different shapes are used to capture both temporal\\nand frequency domain contextual information. Specifically,\\nafter stacking the CNN modules, we introduced a temporal-\\nchannel-space attention mechanism that models the contextual\\nemotional expression of features over time, and efficiently\\nfuses the attention of the spatial and channel dimensions of\\nthe speech feature map through the Shuffle unit. Furthermore,\\na combination of transformer and CNN is used to model\\nthe local and global dependencies of feature sequences by a\\ndeep separable convolution with residuals and a lightweight\\ntransformer module. The main contributions of this work are\\nsummarized as follows:\\n• A framework based on CNN and transformer is proposed\\nfor speech emotion recognition. Our framework uses\\ntime-frequency domain convolution and stacked convolu-\\ntion blocks to extract initial local features of speech and\\nstacked CNN and transformer blocks are used to enhance\\nlocal and global features.\\n• To enhance the finiteness of the feature map and model', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='tion blocks to extract initial local features of speech and\\nstacked CNN and transformer blocks are used to enhance\\nlocal and global features.\\n• To enhance the finiteness of the feature map and model\\nthe temporal information of speech, a temporal-channel-\\nspace attention mechanism called Time-Shuffle Attention\\n(T-Sa) is used in our model. T-Sa enhances the feature\\nmap in multi-dimensions.\\n• We propose a module based on deeply separable convo-\\nlution and a lightweight transformer called Lightweight\\nconvolution\\ntransformer(LCT).\\nThis\\nmodel\\nemploys\\nlightweight convolutional blocks to efficiently extract\\nlocal information from features, and incorporates Coor-\\ndinate Attention (CA) into the multi-head self-attention\\nmechanism to capture long-range dependencies among\\nfeatures while enhancing their temporal and spectral\\ninformation.\\n• Extensive experiments of our proposed model on IEMO-\\nCAP and EMO-DB datasets demonstrate the effectiveness\\nof the model in SER tasks.\\nThe rest of the paper is organized as follows. Section\\nII briefly reviews related work. Details of the system are\\npresented in Section II. In Section IV,we present experimental\\nresults to showcase the effectiveness of our model on two\\nwidely-used benchmark datasets. Section V concludes this\\nwork.\\nII. RELATED WORK\\nIn this section, we will briefly review the algorithms related\\nto speech emotion recognition, namely convolutional and\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n3', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='to speech emotion recognition, namely convolutional and\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n3\\nrecurrent neural networks, attention mechanism, and trans-\\nformer.\\nA. Convolutional neural networks and recurrent neural net-\\nworks\\nSpeech is a continuous time-series signal, and CNN and\\nRNN have been two main network structures for SER. Moti-\\nvated by the studies of CNN in computer vision, AlexNet [22]\\nand ResNet [23] show promising results in image classification\\ntasks and therefore have been studies in SER. Zhu et al.\\n[24] has proposed a new Global Aware Multi-scale (GLAM)\\nneural network that utilizes a global perception fusion module\\nto learn multi-scale feature representations, with a focus on\\nemotional information. The multi-time-scale (MTS) method\\nwas introduced in [25], which extends the CNNs by scaling\\nand resampling the convolutional kernel along the time axis\\nto increase temporal flexibility. Liu et al. [14] proposed a\\nlocal global-aware deep representation learning system that\\nuses CNNs and Capsule Networks to learn local and deep\\nglobal information.\\nRNN can model the temporal information in speech and\\ncapture long-term dependencies in the speech signal more\\neffectively. A new layered network HNSD was proposed [13]\\nthat can efficiently integrate static and dynamic features of\\nSER, which uses LSTM to encode static and dynamic features\\nand gated multi-features unit (GMU) for frame level feature', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='that can efficiently integrate static and dynamic features of\\nSER, which uses LSTM to encode static and dynamic features\\nand gated multi-features unit (GMU) for frame level feature\\nfusion for the emotional intermediate representation. Xu et\\nal. [26] proposed a hierarchical grained and feature model\\n(HGFM) that uses recurrent neural networks to process both\\ndiscourse-level and frame-level information of the speech.\\nSince convolutional neural networks can capture local in-\\nformation of features, while recurrent neural networks take\\nadvantage of modeling temporal information, many works\\nhave combined these two approaches and achieved outstanding\\nresults. Li [27] extracted location information from MFCC\\nfeatures and VGGish features by bi-direction long short time\\nmemory (BiLSTM) neural network, and then fused these two\\nfeatures to predict emotions. Liu et al. [28] combined triplet\\nloss and CNN-LSTM models to obtain more discriminative\\nsentiment information, and the proposed framework yielded\\nexcellent results in experiments. Zou [29] et al. used CNN,\\nBiLSTM, and wav2vec2 to extract different levels of speech\\ninformation, including MFCC, spectrogram, and acoustic in-\\nformation, and fused these three features by an attention\\nmechanism. Zhang [30] et al. used CNNs to learn segment-\\nlevel features in spectrograms, using a deep LSTM model to\\ncapture temporal dependencies between speech segments.\\nB. Attention mechanism\\nIn recent years, attention mechanisms have received a lot of', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='level features in spectrograms, using a deep LSTM model to\\ncapture temporal dependencies between speech segments.\\nB. Attention mechanism\\nIn recent years, attention mechanisms have received a lot of\\nattention in major fields to improve the effectiveness of task\\nprocessing by focusing on information that is more critical to\\nthe current task among the many inputs. A channel attention\\nmechanism called Squeeze-and-Excitation (SE) was proposed\\nin [31], which assigns weights to individual channels and adap-\\ntively recalibrates the feature responses of the channels. Woo\\net al. [32] proposed a convolutional block attention module\\nthat combines both spatial and channel dimensions to obtain\\nattention with better results. In addition, some researchers have\\nused deep learning methods for feature extraction of speech\\nand enhancement of feature maps using attention mechanisms.\\nAn attention pooling-based approach was proposed in [33],\\ncompared to existing average and maximum pooling, it can\\ncombine both class-agnostic bottom-up attention maps and\\nclass-specific top-down attention maps in an effective manner.\\nMustaqeem et al. [20] proposed a self-attentive module (SAM)\\nfor SER systems,which uses a multilayer perceptron (MLP)\\nto recognize global information of the channels and identifies\\nspatial information using a special dilated CNN to generate\\nan attentional map for both channels. SAM significantly re-\\nduces the computational and parametric overhead. A spectro-\\ntemporal-channel (STC) attention mechanism was proposed in', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='an attentional map for both channels. SAM significantly re-\\nduces the computational and parametric overhead. A spectro-\\ntemporal-channel (STC) attention mechanism was proposed in\\n[21], which acquires attention feature maps along three dimen-\\nsions: time, frequency, and channel. Xi et al. [34] employed an\\nattention mechanism based on the time and frequency domain\\nto introduce long-distance contextual information.\\nThe current attention mechanisms typically focus more\\non spatial or channel information in feature maps, often\\nneglecting the temporal characteristics in speech. However,\\ntemporal features in speech are equally important for emotion\\nrecognition. Therefore, it is necessary to pay more attention to\\ntemporal information in attention mechanisms to better explore\\nand utilize the temporal characteristics in speech signals.\\nC. Transformer\\nTransformer has been rapidly developing in the field of\\nnatural language processing (NLP) in recent years and has\\nachieved great success. Due to its powerful ability to obtain\\nglobal information, the transformer has been gradually ex-\\ntended to the fields of speech. An end-to-end speech emotion\\nrecognition model [18] was proposed to enhance the global\\nfeature representation of the model by using stacked trans-\\nformer blocks at the end of the model. Hu et al. [19] took\\nadvantage of multiple models, improved the learning ability\\nof the model by residual BLSTM, and proposed a convolu-\\ntional neural network and E-transformer module to learn both\\nlocal and global information. Recently, transformer-based self-\\nsupervised methods have also been applied to speech, and', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='tional neural network and E-transformer module to learn both\\nlocal and global information. Recently, transformer-based self-\\nsupervised methods have also been applied to speech, and\\nsome transformer-based models have achieved great success in\\nautomatic speech recognition (ASR), including wav2vec [35],\\nVQ-wav2vec [36], and wav2vec2.0 [37]. There is also some\\nwork in speech emotion recognition that employs these models\\nfor migration learning. A pre-trained wav2vec2.0 model [38]\\nis used as the input to the network and the outputs of multiple\\nnetwork layers of the pre-trained model are combined to\\nproduce a richer representation of speech features. Cai et\\nal. [39] proposed a multi-task learning (MTL) framework\\nthat uses the wav2vec2.0 model for feature extraction and\\nsimultaneously training for speech emotion classification and\\ntext recognition. Among computer vision tasks, ViT [40] first\\napplied transformer directly to image patch sequences which is\\ngroundbreaking in applying transformer structure to computer\\nvision. ViT has a superior structure and reduced computa-\\ntional resource consumption compared to convolutional neural\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n4\\nFig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='EX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n4\\nFig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in\\nspeech; ii) T-Sa attention module enhances speech information in three dimensions: time-space-channel; iii) LCT Block combines local and global information\\nin speech.\\nnetworks. There are many similar approaches in the field of\\nspeech. ViT was introduced to speech and improved based\\non the properties of spectrograms in [41], which proposed\\na separable transformer (SepTr) that uses the transformer to\\nprocess tokens at the same time and the same frequency\\ninterval, respectively. In [42], a method to improve ViT was\\napplied to infant cry recognition by combining the original\\nlog-Mel spectrogram, first-order time-frame and frequency-\\nbin differential log-Mels 3D features into ViT for infant cry\\nrecognition.\\nIII. METHOD\\nIn this section, we describe the proposed model in detail.\\nOur proposed model is shown in Fig. 1, which consists of three\\nparts, namely CNN Block, T-Sa attention mechanism module,\\nand LCT Block, these three modules will be introduced in\\ndetail next.\\nA. Overview of the model\\nTo take full advantage of convolutional neural networks and\\ntransformer to model the speech sequences and use attention\\nmechanism to enhance the features in time, space and channel,\\nbased on which our model is designed. As shown in Fig. 1, for', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='transformer to model the speech sequences and use attention\\nmechanism to enhance the features in time, space and channel,\\nbased on which our model is designed. As shown in Fig. 1, for\\na given input speech sequence, a series of preprocessing steps\\nare performed. Specifically, we uniformly process different\\nlengths of speech sequences into 1.8s, the longer sequences\\nwill be cropped into subsegments, and for shorter sequences,\\nwe process them using loop filling, after which MFCC features\\nare extracted of speech as the input to the model. The local\\nfeatures of the speech first are obtained by a CNN block, where\\nthe irregular-sized time-frequency domain convolution is used\\nto obtain the features in the time and frequency domains of the\\nspeech. The features are then enhanceed using a T-Sa attention\\nmechanism block, which contains a bilstm attention module\\nto model the features in the temporal order, followed by a\\nspatial-channel attention mechanism to focus on the spatial and\\nchannel information. Finally, the global and local information\\nof speech is learned interactively by an LCT Block, which\\nenables the model to learn information at different scales. The\\nthree parts of the model in this paper are described in detail\\nin the following sections.\\nB. CNN Block\\nFor a large model such as transformer, if MFCC features\\nare directly input into transformer, it will bring a large number\\nof parameters. And since the dataset of speech emotion\\nrecognition is generally small, using transformer directly for\\nfeature learning will make the model difficult to converge.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='are directly input into transformer, it will bring a large number\\nof parameters. And since the dataset of speech emotion\\nrecognition is generally small, using transformer directly for\\nfeature learning will make the model difficult to converge.\\nTherefore, we introduce a CNN Block to pre-learn the local\\nfeatures in speech. As shown in Figure 1, the CNN Block\\nconsists of a series of convolutional and pooling layers.\\nFor the input feature MFCC, the two dimensions of MFCC\\ncorrespond to two dimensions in the temporal and frequency\\ndomains, respectively, for which we first use a pair of irregular\\nconvolutions to obtain the perceptual field in a specific range.\\nFor a convolution of size 3 × 1, we set the perceptual field\\nin the time domain to 1, thus minimizing the effect in the\\ntime domain to learn information in the frequency domain,\\nand for a convolution of size 1×3 which is a similar process,\\nwhich in turn allows capturing a multi-scale representation in\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n5\\nFig. 2. An illustration of our proposed T-Sa attention module consisting of two modules: Timing attention which enhances the current features temporally\\nthrough BiLSTM, and Space-channel attention which enhances the features from the spatial and channel dimensions.\\nthe temporal-frequency domain. The results are then fed to\\nsuccessive convolutional layers and maxpooling layers, which\\nare used to further capture the local representation in speech,\\nthe batch normalization (BN), and relu activation function are', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='successive convolutional layers and maxpooling layers, which\\nare used to further capture the local representation in speech,\\nthe batch normalization (BN), and relu activation function are\\napplied after each convolutional layer.\\nC. T-Sa attention module\\nIn this paper, inspired by [43], we adopt a novel attention\\nmodule to combine temporal attention with spatial-channel\\nattention, focusing on the temporal dynamics of speech fea-\\ntures and spatial-channel information in the feature map, as\\nshown in Fig. 2, which is divided into two parts before and\\nafter. In the former part, the attention model enhances the\\ntemporal information in the features by Bilstm to model the\\ncurrent features temporally, based on the fact that speech\\ninformation is temporal information and the order of features\\nin time is of importance. The latter part follows the spatial\\nand channel attention, which is a common concern in existing\\nattention, and efficiently combines the attention of the spatial\\nand channel dimensions of speech feature maps through the\\nShuffle unit. T-Sa attention module enhances the features in\\nthe model through three dimensions and with a small number\\nof parameters and achieves better results in SER.\\n1) Timing attention: After Pre-processing and CNN Block\\nfor feature extraction of speech, giving the feature map size\\nof X ∈RC×H×W for the input T-Sa attention module, where\\nC, H and W denote the number of channels, spatial height\\nand width, respectively. To model the temporal attention in\\nspeech features, a recurrent neural network is used to model', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='C, H and W denote the number of channels, spatial height\\nand width, respectively. To model the temporal attention in\\nspeech features, a recurrent neural network is used to model\\nthe speech information which is bilstm. The input of bilstm is\\na two-dimensional sequence while our speech feature map is\\nthree-dimensional, so what we need is to process the feature\\nmap X. If we directly reshape the feature map, the input bilstm\\nwill bring a great number of parameters number. Therefore,\\naverage pooling is used to reduce the dimensionality of the\\nchannels, and the specific calculation is:\\nXCAvgP ool = 1\\nC\\nC\\nX\\ni=1\\nX(i)\\n(1)\\nAfter the feature passes through average pooling, the size\\nof the feature map is XCAvgP ool ∈RH×W . Feature map\\nis adjusted to XCAvgP ool ∈RW ×H by reshaping operation\\nand then feeding it to the bilstm layer. By encoding long\\ndistances from front to back and from back to front, bilstm can\\nbetter capture bidirectional feature dependencies. XCAvgP ool\\nis encoded by bilstm as follows:\\n−\\n→\\nh bilstm = −\\n−\\n−\\n−\\n−\\n−\\n−\\n→\\nBILSTM(XCAvgP ool)\\n(2)\\n←\\n−\\nh bilstm = ←\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\nBILSTM(XCAvgP ool)', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='(2)\\n←\\n−\\nh bilstm = ←\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\nBILSTM(XCAvgP ool)\\n(3)\\nTwo LSTMs in bilstm process the sequence forward and\\nbackward, respectively, and then concate the outputs of the\\ntwo LSTMs together:\\nHbilstm = Concatenate\\n\\x10−\\n→\\nh bilstm; ←\\n−\\nh bilstm\\x11\\n(4)\\nHbilstm is then applied sigmoid activation and multiplied with\\nthe input feature map using the residual scheme to output\\ntemporal attention:\\nXtime = σ\\n\\x00Hbilstm\\x01\\n· X\\n(5)\\n2) Space-channel attention: Spatial attention and channel\\nattention are widely used in computer vision. Most methods\\ntransform and aggregate features in these two directions, such\\nas SE [31], CBAM [32], BAM [44], GCNet [45], but these\\nmethods do not make full use of the correlation between space\\nand channel which are not efficient. Therefore, we adopted\\nSA-Net [43] as our spatial-channel attention module.\\nGiven the output Xtime of the temporal attention module,\\nthe input is first divided into G groups, and the size of\\neach sub-feature map is Xtime′ ∈RC/G×H×W . Then, each\\ngroup is split into two sub-branches in the channel dimension.,\\nXspatial and Xchannel, one of which obtains spatial attention', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='group is split into two sub-branches in the channel dimension.,\\nXspatial and Xchannel, one of which obtains spatial attention\\nand the other obtains channel attention.\\nFor the channel attention branch, firstly, the global average\\npooling (GAP) operation is performed on the input of the\\nbranch to embed the global information. Then, a simple\\ngating mechanism and sigmoid activation are used to perform\\nadaptive learning of spatial features. A residual scheme is used\\nto multiply the input channel branch feature map. The specific\\noperation of the spatial attention module is as follows:\\nXchannel′ = σ\\n\\x00W1 · GAP(Xchannel) + b1\\n\\x01\\n· Xchannel (6)\\nW1 and b1 are learnable parameters.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n6\\nFig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the\\nfeatures through a lightweight convolution module; ii) CA-LMAM, which captures the long-range dependencies in the features and enhance the time-frequency\\ndomain features of speech through a CA module; iii) SE-IBFFN, which introduces a nonlinear part through a feedforward network with inverse residuals to\\nenhance the model the expressiveness of the model.\\nFor the spatial attention branch, GN [46] operation is\\nfirst performed on the input of the branch to embed spatial\\nstatistical information, and then a simple gating mechanism', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='enhance the model the expressiveness of the model.\\nFor the spatial attention branch, GN [46] operation is\\nfirst performed on the input of the branch to embed spatial\\nstatistical information, and then a simple gating mechanism\\nand sigmoid activation are used to perform adaptive learning\\nof features on the channel, and the residual scheme is used to\\nmultiply the input channel branch feature map. The specific\\noperation of the channel attention module is as follows:\\nXspatial′ = σ\\n\\x00W2 · GN(Xspatial) + b2\\n\\x01\\n· Xspatial\\n(7)\\nBoth W2 and b2 are learnable parameters, and then the outputs\\nof the two branches are merged by concatenating them along\\nthe channel dimension:\\nXattention = Concatenate\\n\\x10\\nXchannel′; Xspatial′\\x11\\n(8)\\nThe attention weights on space and channel are learned sep-\\narately through two branches, and the corresponding residual\\nscheme is multiplied with the respective input feature map to\\nenhance the representations of space and channel. Finally, the\\nchannel shuffle [47] is employed to facilitate communication\\nof information between different groups along the channel\\ndimension. The information of the features interacts in the\\nchannel dimension, and the size of the output feature map is\\nthe same as the initial input.\\nD. LCT Block\\nInspired by [48], we proposed an LCT Block shown in Fig.\\n3, which consists of three modules: lightweight local con-\\nvolution (LLC), coordinate attention-lightweight multi-head', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Inspired by [48], we proposed an LCT Block shown in Fig.\\n3, which consists of three modules: lightweight local con-\\nvolution (LLC), coordinate attention-lightweight multi-head\\nattention mechanism (CA-LMAM) and SE Inverted Bottleneck\\nfeed forward network (SE-IBFFN). Through the combination\\nof convolution and transformer, LCT Block obtains the in-\\nformation of features from the local and the whole, which is\\nmore efficient and has fewer parameters than the traditional\\ntransformer. LLC is a lightweight convolution module, which\\nefficiently obtains local information of features. The CA-\\nLMAM module can capture the long-distance dependencies\\nof features and enhance the information in the time-frequency\\ndomain through the Coordinate Attention (CA) [49] module.\\nSE-IBFFN introduces a nonlinear part through a feedforward\\nnetwork with inverted residuals, which enhances the perfor-\\nmance of the model and can further capture the local infor-\\nmation of features. These three modules will be introduced in\\ndetail below\\n1) LLC: In order to make up for the lack of local infor-\\nmation in trasnformer, a convolution module is used to obtain\\nlocal information in speech which called LLC. Some work\\nsuch as CMT [48] also adopted a similar architecture, but the\\nconvolution module in CMT is relatively simple, only using\\na Depthwise convolution with residuals, which can not obtain\\neffective local information. Inspired by [50], we used a large', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='convolution module in CMT is relatively simple, only using\\na Depthwise convolution with residuals, which can not obtain\\neffective local information. Inspired by [50], we used a large\\nconvolution kernel Depthwise convolution with residuals and\\na Pointwise convolution in the local feature extraction module,\\ngiven an input feature X ∈RC×H×W as follows:\\nLLC (X) = PWConv (DWConv (X) + X)\\n(9)\\nThe activation layer and batch normalization are omitted, and\\nthe same omission will be in subsequent formulas. PWConv\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n7\\nrepresents Pointwise convolution and DWConv represents\\nDepthwise. In PWConv we used a large 7 × 7 convolution\\nkernel is used to get a larger receptive field than a 3 × 3\\nconvolution kernel. In addition, PWConv has smaller param-\\neters than DWConv and ordinary convolution. Additionally,\\na residual structure is incorporated to address the issue of\\ngradient dispersion.\\n2) CA-LMAM: In transformer, multi-head attention is usu-\\nally used to make the model pay more attention to the more\\nnoteworthy part of itself, which can obtain long-distance de-\\npendence in speech features. Given the output X ∈RC×H×W\\nin the LLC module, the input of multi-head attention is query\\nQ, key K, and value V , respectively. If the original features', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='pendence in speech features. Given the output X ∈RC×H×W\\nin the LLC module, the input of multi-head attention is query\\nQ, key K, and value V , respectively. If the original features\\nare directly input into multi-head attention, it will often bring\\na large amount of calculation. The amount of calculation is\\nrelated to the size of the input features. Therefore, 2 × 2\\nDWConv is used to downsample the parts of K and V , as\\nshown below:\\nK = DWConv (X)\\n(10)\\nV = DWConv (X)\\n(11)\\nWhere K ∈RC× H\\n2 × W\\n2 , V ∈RC× H\\n2 × W\\n2 , then the H and W\\ndimensions are merged and input a linear layer respectively.\\nFinally get K′ ∈RN ′×C, V ′ ∈RN ′×C, where N ′ = H\\n2 × W\\n2 ,\\nC is the dimension of the linear layer output.\\nFor Q, a CA module is used to enhance the time-frequency\\ndomain representation of speech features. This attention mech-\\nanism can obtain long-distance feature dependence along one\\ndirection and spatial dependence information in another direc-\\ntion. However, in speech, speech features have more special\\nsignificance in the spatial dimension. The abscissa of speech\\nfeatures is the time axis, which represents the time domain\\ninformation of speech while the ordinate of the speech feature\\nrepresents the frequency information of the speech, so CA', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='features is the time axis, which represents the time domain\\ninformation of speech while the ordinate of the speech feature\\nrepresents the frequency information of the speech, so CA\\ncan better aggregate the dependent information in the time\\ndomain and frequency domain for speech features, which is\\nmore suitable for SER tasks.\\nThe specific operation of CA is shown in Fig. 3. Given\\nthe input X ∈RC×H×W , pooling is performed in the time\\ndomain and frequency domain respectively:\\nXT AvgP ool = 1\\nW\\nW\\nX\\ni=1\\nX(i)\\n(12)\\nXF AvgP ool = 1\\nH\\nH\\nX\\nj=1\\nX(j)\\n(13)\\nXT AvgP ool ∈RC×H×1 and XF AvgP ool ∈RC×1×W are\\nthe feature maps after aggregation in two directions of time-\\nfrequency domain. Then, they concatenate and perform con-\\nvolution operations to encode the information:\\nXtf = Conv\\n\\x00concatenate\\n\\x00XT AvgP ool; XF AvgP ool\\x01\\x01\\n(14)\\nWhere Xtf ∈RC/r×1×(W +H), r is the reduction rate of the\\nchannel, and then separated into two separate features Xt, Xf\\nalong the spatial direction, where Xt ∈RC/r×H×1, Xf ∈\\nRC/r×1×W . The extraction of features is then performed by\\ntwo separate convolutions, followed by activation using the σ', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='RC/r×1×W . The extraction of features is then performed by\\ntwo separate convolutions, followed by activation using the σ\\nactivation function, and then multiplied by the initial output\\nto learn the more critical information in the feature:\\nst = σ\\n\\x00Conv\\n\\x00Xt\\x01\\x01\\n(15)\\nsf = σ\\n\\x00Conv\\n\\x00Xf\\x01\\x01\\n(16)\\nQ = X · st · sf\\n(17)\\nWe adjust the dimension of Q, eventually Q′ ∈RN×C, where\\nN = H × W.\\nEventually learning information about the model itself\\nthrough multi-headed self-attention:\\nLMAM(Q, K, V ) = Softmax\\n\\x12Q′K′T\\n√\\nC\\n+ B\\n\\x13\\nV ′\\n(18)\\nWhere B is a learnable parameter, representing the relative\\nposition coding of multi-head self-attention, which is used to\\ncharacterize the relative position relationship between tokens.\\nIt is more flexible than the traditional absolute position coding,\\nmaking transformer better model the relative position informa-\\ntion of speech features.\\n3) SE-IBFFN: In the original transformer, FFN is generally\\ncomposed of two linear layers. In this paper, inspired by [51],\\na series of Pointwise convolution and Depthwise convolution\\nmake up our SE-IBFFN. Compared with the Transformer\\nmodel traditionally composed of linear layers, this module can\\ncapture local information while learning channel information,\\nand has a smaller number of convolution parameters than\\nordinary ones.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='model traditionally composed of linear layers, this module can\\ncapture local information while learning channel information,\\nand has a smaller number of convolution parameters than\\nordinary ones.\\nThe structure of SE-IBFFN is shown in Fig. 3. Given the\\ninput X ∈RC×H×W , the specific calculation process is as\\nfollows:\\nXconv = PWConv (DWConv (PWConv (X)))\\n(19)\\nSE-IBFFN (X) = SE (Xconv) + X\\n(20)\\nFirstly, the Pointwise convolution operation is performed on\\nthe input, and the number of channels is expanded to 4 times of\\nthe original to increase the feature size of the channels. Then\\na 7 × 7 large convolution kernel DWConv is used to obtain\\nthe local information in the feature, and the large convolution\\nkernel can provide a larger receptive field without increasing\\ntoo many parameters. The feature map is then restored to its\\noriginal size using PWConv.\\nThen, the SE module is to obtain the attention of the channel\\ndimension of the feature map. Given the input X ∈RC×H×W\\nof SE, the specific calculation process is as follows:\\nSE (X) = σ (GAP (FC (FC (X)))) · X\\n(21)\\nCompared with [51], we put the SE module after PWConv,\\nwhich makes the parameters of the model smaller. A residual\\nstructure is used to solve the problem of gradient dispersion.\\nJOURNAL OF L\\nAT', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='which makes the parameters of the model smaller. A residual\\nstructure is used to solve the problem of gradient dispersion.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n8\\nIV. EXPERIMENTS SETUP\\nIn this section, we will introduce the dataset and experimen-\\ntal details used in our study, as well as the evaluation metrics\\nused to assess the performance of our algorithm.\\nA. Corpus Description\\nTo verify the performance of our proposed algorithm, per-\\nformance tests on two benchmark databases are conducted, on\\nwhich we will evaluate our algorithm.\\nActually, Interactive Emotional Dyadic Motion Capture\\n(IEMOCAP) [52] is an action, multimodal, and multimodal\\ndatabase that contains data from 10 actors and actresses\\nduring an emotional binary interaction, with two speakers\\n(one male and one female) speaking in each session. The\\nIEMOCAP database has been annotated by several annotators\\nwith categorical labels and dimensional labels. The database\\ncombines discrete and dimensional sentiment models. In our\\nwork, the method used improvisational and scripted data,\\nchoose anger, happiness, neutral, sadness and excitement as\\nbasic emotions, and merge happy and excited into happy. We\\npartitioned the IEMOCAP dataset into training and testing sets\\nby randomly selecting 80% and 20% of the data, respectively.\\nTo valid our method robustness, we tested our method\\nin other datasets. The Berlin Emotional Database (Emo-DB)\\n[53] is a German emotional speech database recorded by', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='To valid our method robustness, we tested our method\\nin other datasets. The Berlin Emotional Database (Emo-DB)\\n[53] is a German emotional speech database recorded by\\nthe Technical University of Berlin. The database includes\\nrecordings of ten actors, comprising of five male and five\\nfemale, who simulate seven emotions, including neutral, anger,\\nfear, joy, sadness, disgust, and boredom, on ten sentences\\n(five short and five long), resulting in a total of 535 speech\\nrecordings (233 male and 302 female). It has high emotional\\nfreedom, adopts 16 kHz sampling, and 16-bit quantization, and\\nsaves files in WAV format. It is a discrete emotional language\\ndatabase, and the excitation method is performance type.\\nB. Implementation Details\\nIn the feature generation phase, the method uses the mel-\\nfrequency cepstrum coefficients (MFCCs) extracted by 26 Mel\\nfilters as the feature input. Meanwhile it divided each input\\nspeech into 1.8 seconds of speech segments, and the overlap\\nbetween segments is 1.6 seconds, which can generate a large\\nnumber of speech samples to solve the problem of scarcity of\\ndata set samples in SER. To obtain the prediction result for a\\nsentence in the test set, we take the average of the prediction\\nresults of all speech segments within that sentence. Our model\\ntrained a total of 150 epochs, used the cross entropy criterion\\nas the objective function, and used the Adam optimizer. The\\nweight decay rate is 10−6, the learning rate and the mini-', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='trained a total of 150 epochs, used the cross entropy criterion\\nas the objective function, and used the Adam optimizer. The\\nweight decay rate is 10−6, the learning rate and the mini-\\nbatch size are set to 0.001 and 128, respectively, and the\\nmultiplication factor 0.95 is exponentially decayed until the\\nvalue reaches 10−6. Our experiment is carried out on Ubuntu\\n18.04 with a GeForce RTX 2080ti GPU, and we utilized\\nPytorch 1.7 as the training framework.\\nIn addition, we use the method of mixup [54] to train, so\\nas to improve the generalization ability of the system. This\\nmethod constructs new training samples and labels by linear\\ninterpolation, and effectively smoothes the discrete data space\\ninto continuous space. In our proposed model, we set α to 0.2\\nfor best performance.\\nC. Evaluation Metrics\\nIn this section, we’ll describe in detail the criteria we use\\nto evaluate the performance of our algorithms. For various\\ncategories of performance in the dataset, Precision, Recall, and\\nF1-score are the general metrics to measure their performance.\\nFirst of all, four concepts will be introduced: True Positive\\n(TP), False Positive (FP), True Negative (TN), and False\\nNegative (FN), where TP means actual positive and predicted\\npositive, FP means actual positive and predicted negative, TN\\nmeans actual negative and predicted positive, and FN means\\nactual negative and predicted negative. The Precision, Recall,\\nand F1-score can be expressed as:\\nPrecision =\\nTP', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='means actual negative and predicted positive, and FN means\\nactual negative and predicted negative. The Precision, Recall,\\nand F1-score can be expressed as:\\nPrecision =\\nTP\\nTP + FP\\n(22)\\nRecall =\\nTP\\nTP + FN\\n(23)\\nF1 −score = precisio × recall × 2\\nprecision + recall\\n(24)\\nTo evaluate the overall performance of our model, weighted\\naverage accuracy (WA) and unweighted average accuracy\\n(UA) will be used as evaluation metrics, where WA is the\\nweighted average accuracy of different sentiment categories,\\nand its weight is related to the number of sentiment categories,\\nand UA is the average accuracy of different sentiment cate-\\ngories. The validity of the model is better evaluated in the\\ncontext of unbalanced SER dataset samples. The calculation\\nmethods for these two metrics are as follows:\\nAcci =\\nTPi\\nTPi + FPi\\n(25)\\nWA =\\nPC\\ni=1 Ni × Acci\\nPC\\ni=1 Ni\\n(26)\\nUA = 1\\nC\\nC\\nX\\ni=1\\nAcci\\n(27)\\nAmong them, C represents the number of emotional cate-\\ngories, and Ni represents the number of samples of class i.\\nV. RESULTS\\nIn this section, we conduct extensive experiments to evaluate\\nthe performance of our method on two datasets. The section\\nmainly compares our proposed model with state-of-the-art\\nbaselines, and then verify the effectiveness of our proposed\\nmodules through ablation experiments.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='the performance of our method on two datasets. The section\\nmainly compares our proposed model with state-of-the-art\\nbaselines, and then verify the effectiveness of our proposed\\nmodules through ablation experiments.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n9\\nA. Comparison with State-of-the-Art\\nTo compare with our proposed model, we evaluate the\\nperformance of the algorithm from the WA and UA perspective\\nwith a series of existing methods in IEMOCAP and Emo-DB\\ndatasets, as shown in Table I and Table II. The proposed model\\nis compared with several commonly used speech emotion\\nrecognition models, including algorithms based on the combi-\\nnation of CNN and transformer [19], CNN-based algorithms\\n[14] [55], LSTM-based algorithms [56] [57], attention-based\\nmethods [21] [58], and some other algorithms.\\nTABLE I\\nCOMPARISON ON IEMOCAP\\nComparative Methods\\nWA\\nUA\\nLatif et al. [59]\\n-\\n68.8\\nHu et al. [19]\\n69.73\\n70.11\\nGuo et al. [21]\\n61.32\\n60.43\\nGao et al. [60]\\n70.34\\n70.82\\nWang et al. [57]\\n69.40\\n69.50\\nLatif et al. [56]\\n-\\n64.10\\nLiu et al. [14]\\n70.34\\n70.78', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Wang et al. [57]\\n69.40\\n69.50\\nLatif et al. [56]\\n-\\n64.10\\nLiu et al. [14]\\n70.34\\n70.78\\nDai et al. [61]\\n65.40\\n66.90\\nProposed\\n71.64\\n72.72\\nTABLE II\\nCOMPARISON ON EMO-DB\\nComparative Methods\\nWA\\nUA\\nTuncer et al. [62]\\n90.09\\n89.47\\nLi et al. [58]\\n83.30\\n82.10\\nZhong et al. [63]\\n85.76\\n86.12\\nKerkeni et al. [64]\\n-\\n86.22\\nSuganya et al. [55]\\n-\\n85.62\\nProposed\\n90.65\\n89.51\\nTo verify the performance of our proposed method, we\\ncompare it with other speech emotion recognition algorithms\\nin IEMOCAP and Emo-DB datasets. The results are shown in\\nTable I and Table II.\\nFor IEMOCAP dataset, as shown in Table I, our proposed\\nmethod outperforms other methods. In addition, compared\\nwith the simple splicing of transformer and CNN [19], our\\nmethod achieves the best results through the ingenious design\\nof local and global feature extraction. Compared to tradi-\\ntional spatial and channel attention [21], temporal attention\\ninformation makes it more competitive. In addition, for the\\nCNN or LSTM-based method [14] [57] [56] [61], our method', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='tional spatial and channel attention [21], temporal attention\\ninformation makes it more competitive. In addition, for the\\nCNN or LSTM-based method [14] [57] [56] [61], our method\\nintroduces global information through a lightweight trans-\\nformer module to bring more comprehensive features to the\\nsystem, and also shows that our transformer module has a\\nstronger ability to obtain global information than some capsule\\nnetworks. Finally, our method is as competitive as the method\\nusing semi-supervised methods [59] and pre-trained models\\n[60].\\nFor Emo-DB dataset, as shown in Table II, our proposed\\nmethod outperforms several methods. Similar to the perfor-\\nmance in IEMOCAP dataset, our attention mechanism and\\nlocal-global model have significant advantages over traditional\\nattention and CNN-based models [58] [63] [55]. In addition,\\nEmo-DB dataset is a smaller dataset. Compared with non-deep\\nlearning feature extraction and feature learning methods [62]\\n[64], our overall end-to-end network based on deep learning\\nalso has a relatively better performance on this small dataset,\\nindicating that our method still has excellent robustness on\\nsmall datasets.\\nIn summary, combining the performance of these two\\ndatasets proves the superiority of our proposed method.\\nB. Results and Analysis\\nIn this section, Table III and Table IV list the Precision,\\nRecall, F1-score, and overall WA and UA for each sentiment\\ncategory in IEMOCAP dataset and Emo-DB dataset, respec-', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='In this section, Table III and Table IV list the Precision,\\nRecall, F1-score, and overall WA and UA for each sentiment\\ncategory in IEMOCAP dataset and Emo-DB dataset, respec-\\ntively. In addition, the confusion matrices are visualized of the\\ntwo datasets in Fig. 4 and Fig. 5, where the diagonal indicates\\nthat the sentiment is correctly classified, and other locations\\nindicate that the sentiment is misclassified as other sentiments.\\nThe darker the color in the grid while higher the accuracy.\\nTABLE III\\nCONFUSION MATRIX OF THE PROPOSED MODEL ON IEMOCAP\\nEmotion\\nPrecision\\nRecall\\nF1-score\\nNeural\\n72.88\\n62.32\\n67.19\\nSad\\n70.51\\n75.00\\n72.68\\nAngry\\n74.43\\n79.13\\n76.71\\nHappy\\n69.68\\n74.43\\n71.98\\nWA\\n71.64\\nUA\\n72.72\\nTABLE IV\\nCONFUSION MATRIX OF THE PROPOSED MODEL ON EMO-DB\\nEmotion\\nPrecision\\nRecall\\nF1-score\\nNeural\\n86.67\\n100.00\\n92.86\\nSad\\n94.44\\n100.00\\n97.14\\nAngry\\n88.00\\n95.65\\n91.67\\nHappy\\n83.33\\n83.33\\n83.33\\nBoredom\\n100.00\\n93.75\\n96.77\\nDisgust\\n100.00\\n76.92', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='95.65\\n91.67\\nHappy\\n83.33\\n83.33\\n83.33\\nBoredom\\n100.00\\n93.75\\n96.77\\nDisgust\\n100.00\\n76.92\\n86.96\\nFear\\n83.33\\n76.92\\n80.00\\nWA\\n90.65\\nUA\\n89.51\\nFor IEMOCAP dataset, as shown in Table IV and Fig. 4,\\nour proposed model achieves good results on this dataset and\\nhas good accuracy for all four emotions, especially sadness,\\nanger and happiness. Among these four emotions, anger has\\nthe highest recognition accuracy, while neutral has the lowest\\nrecognition accuracy. Sadness, anger and happiness will be\\nmisjudged as neutral in some cases. The result is similar\\nto that in [65], which also verifies that neutral emotion is\\na defect of expression. This emotion is easily expressed as\\nother emotions, which makes the model misjudged. Therefore,\\nneutral emotions are easily confused with other emotions.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n10\\n(a)\\n(b)\\nFig. 4. Visualization the confusion matrices of the proposed method: (a) Confusion matrix on IEMOCAP; (b) Confusion matrix on Emo-DB.\\nFor Emo-DB dataset, as shown in Table IV and Fig.\\n5, our proposed model also achieved good results on this\\ndataset, and has good accuracy for seven emotions. The three', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='For Emo-DB dataset, as shown in Table IV and Fig.\\n5, our proposed model also achieved good results on this\\ndataset, and has good accuracy for seven emotions. The three\\nemotions achieved 100% accuracy, and Angry also achieved\\n96% accuracy. In addition, the wrong judgment in happiness is\\nanger. We believe that this is because these two emotions have\\nsimilar arousal, while the wrong judgment in disgust is fear.\\nFinally, the accuracy of fear is lower than that of other emotion\\ncategories, but it also achieves relatively good results. In some\\ncases, fear is easily misjudged as sad, angry and happy, this\\nis because these four emotions have a high degree of arousal.\\nTABLE V\\nPERFORMANCE WITH DIFFERENT EXPERIMENTAL SETTINGS\\nModels\\nWA\\nUA\\nW/o T-Sa\\n69.92\\n71.18\\nW/o lstm attention\\n67.84\\n69.62\\nW/o LCT\\n67.12\\n68.93\\nW/ Conv-LCT\\n68.29\\n69.98\\nW/o CA\\n69.29\\n70.12\\nW/o SE\\n69.92\\n71.31\\nProposed\\n71.64\\n72.72\\nC. Ablation Study\\nTo explore the role of each part of our proposed model,\\nTable V shows the results of a series of ablation experiments.\\nThe following are the modules for comparison.\\n• W/o T-Sa: This module removes the T-Sa module from', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Table V shows the results of a series of ablation experiments.\\nThe following are the modules for comparison.\\n• W/o T-Sa: This module removes the T-Sa module from\\nour model, using only the CNN Block and LCT Block\\nsections.\\n• W/o lstm attention: This module removes the temporal\\nattention lstm attention part in our T-Sa module, and the\\nother parts are retained.\\n• W/o LCT: This module removes the LCT module in our\\nmodel and only uses the CNN Block and T-Sa parts.\\n• W/ Conv-LCT: This module replaces the LLC module in\\nour LCT module with a 3 × 3 convolution, and the rest\\nis preserved.\\n• W/o CA: This module removes the CA portion of our\\nLCT module and preserves the rest.\\n• W/o SE: This module removes the SE Attention part of\\nour LCT module, and the other parts are retained.\\nIt can be seen from the table that when using the T-Sa\\nmodule, our method has an absolute improvement of 1.2%\\nand 1.54% in WA and UA, indicating that our attention\\nmechanism module has a very significant effect and can\\naggregate the noteworthy parts of the features. In addition,\\nwhen the temporal attention is removed, the model effect is\\nalso greatly reduced, indicating that temporal attention plays\\na non-important role in our model to enhance the temporal\\ninformation in speech. We also directly removed the LCT\\nmodule, which caused the performance of the model to be', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='also greatly reduced, indicating that temporal attention plays\\na non-important role in our model to enhance the temporal\\ninformation in speech. We also directly removed the LCT\\nmodule, which caused the performance of the model to be\\nreduced by 4.52% and 3.79% on WA and UA, it clearly shows\\nthe importance of introducing global information into our LCT\\nmodule.\\nFor our LCT part, our experiments also verified the role\\nof different modules within the LCT. Firstly, the LLC part is\\nreplaced with a 3×3 ordinary convolution, which reduces the\\nperformance of the model by 3.35% and 2.74% on WA and\\nUA, and the number of parameters has also been improved.\\nThis shows that the wider receptive field brought by our large\\nconvolution kernel LLC module is very important, and it does\\nnot bring a larger number of parameters. In order to verify\\nthe role of our CA module, we removed the CA module,\\nwhich caused the performance of the model to decrease\\nby 2.35% and 2.6% on WA and UA, indicating that the\\ntime-frequency domain representation of the speech features\\nenhanced by the CA module is of vital importance. Finally,\\nthe SE Attention part in our SE-IBFFN is removed, which\\nreduces the performance of the model by 1.72% and 1.41%\\non WA and UA. It also proves that using the SE module to\\nobtain the attention of the feature map channel dimension can\\nimprove the performance of the model to recognize emotions.\\nD. Model Efficiency Analysis', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='on WA and UA. It also proves that using the SE module to\\nobtain the attention of the feature map channel dimension can\\nimprove the performance of the model to recognize emotions.\\nD. Model Efficiency Analysis\\nIn order to explore the size and efficiency of the proposed\\nmodel, Table VI presents a comparison of the parameter\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n11\\nTABLE VI\\nCOMPARISON OF MODEL PARAMETERS AND ACCURACY\\nModels\\nParams\\nWA\\nUA\\nW/ Conv-LCT\\n1,404,922\\n68.29\\n69.98\\nW/ConvFFN-LCT-L\\n10,390,522\\n70.46\\n72.14\\nW/ConvFFN-LCT-S\\n2,526,202\\n68.83\\n69.71\\nW/Transformer\\n1,357,810\\n69.38\\n70.43\\nW/MobileVitv1 block-depth2 [66]\\n2,036,458\\n70.01\\n70.98\\nW/MobileVitv1 block-depth3 [66]\\n2,726,506\\n66.58\\n67.30\\nW/MobileVitv2 block-depth2 [67]\\n937,898\\n68.29\\n69.68\\nW/MobileVitv2 block-depth3 [67]\\n1,086,634\\n67.12\\n69.79', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='937,898\\n68.29\\n69.68\\nW/MobileVitv2 block-depth3 [67]\\n1,086,634\\n67.12\\n69.79\\nW/MobileVitv3 block-depth2 [68]\\n661,162\\n69.20\\n70.14\\nW/MobileVitv3 block-depth3 [68]\\n884,010\\n68.47\\n69.73\\nProposed\\n1,031,674\\n71.64\\n72.72\\ncounts and accuracy between our model and other models.\\nThe following are the modules we designed for comparison.\\n• W/ Conv-LCT: This module replaces the LLC module in\\nour LCT module with a 3 × 3 convolution and the other\\nparts are retained.\\n• w/ ConvFFN-LCT-L: This module replaces the first PW\\nconvolution and DW convolution in the SE-IBFFN mod-\\nule of our LCT module with a 7 × 7 convolution and the\\nother parts are retained.\\n• W/ ConvFFN-LCT-S: This module replaces the first PW\\nconvolution and DW convolution in the SE-IBFFN mod-\\nule of our LCT module with a 3 × 3 convolution and the\\nother parts are retained.\\n• W/Transformer: This module replaces CA-LMAM and\\nSE-IBFFN with traditional transformer, which is the same\\nas the setting of LCT.\\n• W/ MobileVit block: This module replaces our LCT', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='SE-IBFFN with traditional transformer, which is the same\\nas the setting of LCT.\\n• W/ MobileVit block: This module replaces our LCT\\nmodule with the MobileVit block of various series of\\nMobile Vit, with an input channel size of 128 and a depth\\nof 2 or 3. The intermediate layer size of the MLP is set\\nto 4 times of the input channel size for all versions. For\\nv1 and v2, the number of intermediate layers is 192, and\\nfor v3, the number of intermediate layers is 128.\\nFrom the Table VI, it can be seen that when the LLC\\nmodule in LCT is replaced with a regular convolution, the\\nnumber of parameters increases and the accuracy is lower\\nthan that of the LLC module. This indicates that LLC has\\nbetter accuracy with fewer parameters. Additionally, when the\\nfirst PW and DW convolutions in SE-IBFFN are replaced\\nwith regular convolutions, the number of parameters increases\\nsignificantly, and the accuracy is still lower than the original\\nmodel. We also reduced the size of the convolution kernel to\\n3×3, which improved the number of parameters, but it is still\\nlarger than the original SE-IBFFN, and the accuracy decreased.\\nThis suggests that the SE-IBFFN module did not bring a huge\\nnumber of parameters while using a larger convolution kernel\\nto achieve a larger receptive field, and the accuracy is still\\nexcellent.\\nAdditionally, we attempted to replace CA-LMAM and SE-\\nIBFFN in LCT with traditional transformers, and experimental', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='to achieve a larger receptive field, and the accuracy is still\\nexcellent.\\nAdditionally, we attempted to replace CA-LMAM and SE-\\nIBFFN in LCT with traditional transformers, and experimental\\nresults showed that the traditional transformer has a larger\\nnumber of parameters and a decrease in accuracy compared\\nto LCT, with a decrease of 2.23% and 2.29%, respectively. The\\nentire LCT module was also replaced with other transformer\\nmodels, including the MobileVit series which combines CNN\\nand Vit and is a lightweight transformer model. The MobileVit\\nblock in each series was used to replace LCT, and the results\\nshowed that the parameter size of the MobileVitv3 block is\\nsmaller than LCT, but there is still a gap in accuracy. The\\nperformance of the two-layer MobileVitv1 block was the best,\\nbut it still did not reach the accuracy of LCT. Additionally,\\nwe found that models with a depth of 2 performed better than\\nthose with a depth of 3, due to the difficulty of fitting to small\\nSER datasets as the number of parameters increases.\\nVI. CONCLUSION\\nIn this study, to better model local and global features\\nof speech signals at different levels of granularity in SER\\nand capture temporal, spatial and channel dependencies in\\nspeech signals, we propose a Speech Emotion Recognition\\nnetwork based on CNN-Transformer and multi-dimensional\\nattention mechanisms. The network consists of three modules.\\nFirst, a CNN block is used to model time-frequency domain\\ninformation in speech, capturing preliminary local information', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='network based on CNN-Transformer and multi-dimensional\\nattention mechanisms. The network consists of three modules.\\nFirst, a CNN block is used to model time-frequency domain\\ninformation in speech, capturing preliminary local information\\nin speech. Second, we propose a T-Sa network to model\\nthe emotional expression context of features over time and\\nefficiently fuse the spatial and channel dimensions of speech\\nfeature maps through Shuffle units. Finally, to efficiently fuse\\nlocal information and long-distance dependencies in speech,\\nwe propose an LCT module that uses lightweight convolu-\\ntional modules and introduces Coordinate Attention into multi-\\nhead self-attention. This allows for the fusion of features at\\ndifferent levels of granularity while enhancing information in\\nthe time-frequency domain of features without introducing a\\nhigh number of parameters.\\nIn future work, in addition to MFCC features, we will\\ntry more hierarchical speech features and combine current\\nCNN and transformer structures to improve the performance\\nof Speech Emotion Recognition from multiple feature dimen-\\nsions.\\nACKNOWLEDGMENTS\\nThis work was supported by the National Natural Science\\nFoundation of China (Grant No. 62001173), the Project of\\nSpecial Funds for the Cultivation of Guangdong College\\nStudents’ Scientific and Technological Innovation (”Climb-\\ning Program” Special Funds) (Grant No. pdjh2022a0131,\\npdjh2023b0141).\\nREFERENCES\\n[1] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech emotion', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='pdjh2023b0141).\\nREFERENCES\\n[1] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech emotion\\nrecognition: Features, classification schemes, and databases,” Pattern\\nrecognition, vol. 44, no. 3, pp. 572–587, 2011.\\n[2] L.-S. A. Low, N. C. Maddage, M. Lech, L. B. Sheeber, and N. B. Allen,\\n“Detection of clinical depression in adolescents’ speech during family\\ninteractions,” IEEE Transactions on Biomedical Engineering, vol. 58,\\nno. 3, pp. 574–586, 2010.\\n[3] A. Mahdhaoui, M. Chetouani, and C. Zong, “Motherese detection based\\non segmental and supra-segmental features,” in 2008 19th International\\nConference on Pattern Recognition.\\nIEEE, 2008, pp. 1–4.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n12\\n[4] S. E. Bou-Ghazale and J. H. Hansen, “A comparative study of traditional\\nand newly proposed features for recognition of speech under stress,”\\nIEEE Transactions on speech and audio processing, vol. 8, no. 4, pp.\\n429–442, 2000.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='and newly proposed features for recognition of speech under stress,”\\nIEEE Transactions on speech and audio processing, vol. 8, no. 4, pp.\\n429–442, 2000.\\n[5] C. Gobl and A. N. Chasaide, “The role of voice quality in communi-\\ncating emotion, mood and attitude,” Speech communication, vol. 40, no.\\n1-2, pp. 189–212, 2003.\\n[6] J. Hernando and C. Nadeu, “Linear prediction of the one-sided autocor-\\nrelation sequence for noisy speech recognition,” IEEE Transactions on\\nSpeech and Audio Processing, vol. 5, no. 1, pp. 80–84, 1997.\\n[7] S. S. Barpanda, B. Majhi, P. K. Sa, A. K. Sangaiah, and S. Bakshi, “Iris\\nfeature extraction through wavelet mel-frequency cepstrum coefficients,”\\nOptics & Laser Technology, vol. 110, pp. 13–23, 2019.\\n[8] Y. Q. Qin and X. Y. Zhang, “Hmm-based speaker emotional recognition\\ntechnology for speech signal,” in Advanced Materials Research, vol.\\n230.\\nTrans Tech Publ, 2011, pp. 261–265.\\n[9] J. Pribil, A. Pribilova, and J. Matousek, “Artefact determination by gmm-\\nbased continuous detection of emotional changes in synthetic speech,” in', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='[9] J. Pribil, A. Pribilova, and J. Matousek, “Artefact determination by gmm-\\nbased continuous detection of emotional changes in synthetic speech,” in\\n2019 42nd International Conference on Telecommunications and Signal\\nProcessing (TSP).\\nIEEE, 2019, pp. 45–48.\\n[10] B. Schuller, B. Vlasenko, F. Eyben, M. W¨\\nollmer, A. Stuhlsatz, A. Wen-\\ndemuth, and G. Rigoll, “Cross-corpus acoustic emotion recognition:\\nVariances and strategies,” IEEE Transactions on Affective Computing,\\nvol. 1, no. 2, pp. 119–131, 2010.\\n[11] B. Gupta and S. Dhawan, “Deep learning research: Scientometric\\nassessment of global publications output during 2004-17,” Emerging\\nScience Journal, vol. 3, no. 1, pp. 23–32, 2019.\\n[12] S. Kumar, T. Roshni, and D. Himayoun, “A comparison of emotional\\nneural network (enn) and artificial neural network (ann) approach for\\nrainfall-runoff modelling,” Civil Engineering Journal, vol. 5, no. 10, pp.\\n2120–2130, 2019.\\n[13] Q. Cao, M. Hou, B. Chen, Z. Zhang, and G. Lu, “Hierarchical network\\nbased on the fusion of static and dynamic features for speech emotion', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='[13] Q. Cao, M. Hou, B. Chen, Z. Zhang, and G. Lu, “Hierarchical network\\nbased on the fusion of static and dynamic features for speech emotion\\nrecognition,” in ICASSP 2021-2021 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP).\\nIEEE, 2021, pp.\\n6334–6338.\\n[14] J. Liu, Z. Liu, L. Wang, L. Guo, and J. Dang, “Speech emotion\\nrecognition with local-global aware deep representation learning,” in\\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP).\\nIEEE, 2020, pp. 7174–7178.\\n[15] R. A. Khalil, E. Jones, M. I. Babar, T. Jan, M. H. Zafar, and T. Alhussain,\\n“Speech emotion recognition using deep learning techniques: A review,”\\nIEEE Access, vol. 7, pp. 117 327–117 345, 2019.\\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\\nneural information processing systems, vol. 30, 2017.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\\nneural information processing systems, vol. 30, 2017.\\n[17] L. Tarantino, P. N. Garner, A. Lazaridis et al., “Self-attention for speech\\nemotion recognition,” in Interspeech, 2019, pp. 2578–2582.\\n[18] X. Wang, M. Wang, W. Qi, W. Su, X. Wang, and H. Zhou, “A novel\\nend-to-end speech emotion recognition network with stacked transformer\\nlayers,” in ICASSP 2021-2021 IEEE International Conference on Acous-\\ntics, Speech and Signal Processing (ICASSP).\\nIEEE, 2021, pp. 6289–\\n6293.\\n[19] D. Hu, X. Hu, and X. Xu, “Multiple enhancements to lstm for learning\\nemotion-salient features in speech emotion recognition,” Proc. Inter-\\nspeech 2022, pp. 4720–4724, 2022.\\n[20] S. Kwon et al., “Att-net: Enhanced emotion recognition system using\\nlightweight self-attention module,” Applied Soft Computing, vol. 102, p.\\n107101, 2021.\\n[21] L. Guo, L. Wang, C. Xu, J. Dang, E. S. Chng, and H. Li, “Representation\\nlearning with spectro-temporal-channel attention for speech emotion', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='learning with spectro-temporal-channel attention for speech emotion\\nrecognition,” in ICASSP 2021-2021 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP).\\nIEEE, 2021, pp.\\n6304–6308.\\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification\\nwith deep convolutional neural networks,” Communications of the ACM,\\nvol. 60, no. 6, pp. 84–90, 2017.\\n[23] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 2016, pp. 770–778.\\n[24] W. Zhu and X. Li, “Speech emotion recognition with global-aware fusion\\non multi-scale feature representation,” in ICASSP 2022-2022 IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing\\n(ICASSP).\\nIEEE, 2022, pp. 6437–6441.\\n[25] E. Guizzo, T. Weyde, and J. B. Leveson, “Multi-time-scale convolution\\nfor emotion recognition from speech audio signals,” in ICASSP 2020-\\n2020 IEEE International Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP).\\nIEEE, 2020, pp. 6489–6493.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='2020 IEEE International Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP).\\nIEEE, 2020, pp. 6489–6493.\\n[26] Y. Xu, H. Xu, and J. Zou, “Hgfm: A hierarchical grained and feature\\nmodel for acoustic emotion recognition,” in ICASSP 2020-2020 IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing\\n(ICASSP).\\nIEEE, 2020, pp. 6499–6503.\\n[27] C. Li, “Robotic emotion recognition using two-level features fusion in\\naudio signals of speech,” IEEE Sensors Journal, 2021.\\n[28] J. Liu and H. Wang, “A speech emotion recognition framework for better\\ndiscrimination of confusions,” in Interspeech, 2021, pp. 4483–4487.\\n[29] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emotion\\nrecognition with co-attention based multi-level acoustic information,” in\\nICASSP 2022-2022 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP).\\nIEEE, 2022, pp. 7367–7371.\\n[30] S. Zhang, X. Zhao, and Q. Tian, “Spontaneous speech emotion recog-\\nnition using multiscale deep convolutional lstm,” IEEE Transactions on\\nAffective Computing, 2019.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='nition using multiscale deep convolutional lstm,” IEEE Transactions on\\nAffective Computing, 2019.\\n[31] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” 2018.\\n[32] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, “Cbam: Convolutional\\nblock attention module,” in Proceedings of the European conference on\\ncomputer vision (ECCV), 2018, pp. 3–19.\\n[33] P. Li, Y. Song, I. V. McLoughlin, W. Guo, and L.-R. Dai, “An atten-\\ntion pooling based representation learning method for speech emotion\\nrecognition,” 2018.\\n[34] Y.-X. Xi, Y. Song, L.-R. Dai, I. McLoughlin, and L. Liu, “Frontend\\nattributes disentanglement for speech emotion recognition,” in ICASSP\\n2022-2022 IEEE International Conference on Acoustics, Speech and\\nSignal Processing (ICASSP).\\nIEEE, 2022, pp. 7712–7716.\\n[35] S. Schneider, A. Baevski, R. Collobert, and M. Auli, “wav2vec:\\nUnsupervised pre-training for speech recognition,” arXiv preprint\\narXiv:1904.05862, 2019.\\n[36] A.\\nBaevski,', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Unsupervised pre-training for speech recognition,” arXiv preprint\\narXiv:1904.05862, 2019.\\n[36] A.\\nBaevski,\\nS.\\nSchneider,\\nand\\nM.\\nAuli,\\n“vq-wav2vec:\\nSelf-\\nsupervised learning of discrete speech representations,” arXiv preprint\\narXiv:1910.05453, 2019.\\n[37] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A frame-\\nwork for self-supervised learning of speech representations,” Advances\\nin Neural Information Processing Systems, vol. 33, pp. 12 449–12 460,\\n2020.\\n[38] L. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from speech\\nusing wav2vec 2.0 embeddings,” arXiv preprint arXiv:2104.03502,\\n2021.\\n[39] X. Cai, J. Yuan, R. Zheng, L. Huang, and K. Church, “Speech emotion\\nrecognition with multi-task learning,” in Interspeech, vol. 2021, 2021,\\npp. 4508–4512.\\n[40] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='pp. 4508–4512.\\n[40] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\\n“An image is worth 16x16 words: Transformers for image recognition\\nat scale,” arXiv preprint arXiv:2010.11929, 2020.\\n[41] N.\\nRistea,\\nR.\\nIonescu,\\nand\\nF.\\nKhan,\\n“Septr:\\nSeparable\\ntrans-\\nformer for audio spectrogram processing. arxiv 2022,” arXiv preprint\\narXiv:2203.09581.\\n[42] H.-t. Xu, J. Zhang, and L.-r. Dai, “Differential time-frequency log-mel\\nspectrogram features for vision transformer based infant cry recogni-\\ntion,” Proc. Interspeech 2022, pp. 1963–1967, 2022.\\n[43] Q.-L. Zhang and Y.-B. Yang, “Sa-net: Shuffle attention for deep con-\\nvolutional neural networks,” in ICASSP 2021-2021 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP).\\nIEEE, 2021, pp. 2235–2239.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='volutional neural networks,” in ICASSP 2021-2021 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP).\\nIEEE, 2021, pp. 2235–2239.\\n[44] J. Park, S. Woo, J.-Y. Lee, and I. S. Kweon, “Bam: Bottleneck attention\\nmodule,” arXiv preprint arXiv:1807.06514, 2018.\\n[45] Y. Cao, J. Xu, S. Lin, F. Wei, and H. Hu, “Gcnet: Non-local networks\\nmeet squeeze-excitation networks and beyond,” in Proceedings of the\\nIEEE/CVF international conference on computer vision workshops,\\n2019, pp. 0–0.\\n[46] Y. Wu and K. He, “Group normalization,” in Proceedings of the\\nEuropean conference on computer vision (ECCV), 2018, pp. 3–19.\\n[47] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “Shufflenet v2: Practical\\nguidelines for efficient cnn architecture design,” in Proceedings of the\\nEuropean conference on computer vision (ECCV), 2018, pp. 116–131.\\n[48] J. Guo, K. Han, H. Wu, Y. Tang, X. Chen, Y. Wang, and C. Xu,\\n“Cmt: Convolutional neural networks meet vision transformers,” in', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='“Cmt: Convolutional neural networks meet vision transformers,” in\\nProceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, 2022, pp. 12 175–12 185.\\n[49] Q. Hou, D. Zhou, and J. Feng, “Coordinate attention for efficient\\nmobile network design,” in Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, 2021, pp. 13 713–13 722.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n13\\n[50] A. Trockman and J. Z. Kolter, “Patches are all you need?” arXiv preprint\\narXiv:2201.09792, 2022.\\n[51] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang,\\nY. Zhu, R. Pang, V. Vasudevan et al., “Searching for mobilenetv3,”\\nin Proceedings of the IEEE/CVF international conference on computer\\nvision, 2019, pp. 1314–1324.\\n[52] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N.\\nChang, S. Lee, and S. S. Narayanan, “Iemocap: Interactive emotional', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Chang, S. Lee, and S. S. Narayanan, “Iemocap: Interactive emotional\\ndyadic motion capture database,” Language resources and evaluation,\\nvol. 42, no. 4, pp. 335–359, 2008.\\n[53] F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, B. Weiss et al.,\\n“A database of german emotional speech.” in Interspeech, vol. 5, 2005,\\npp. 1517–1520.\\n[54] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond\\nempirical risk minimization,” arXiv preprint arXiv:1710.09412, 2017.\\n[55] S. Suganya and E. Charles, “Speech emotion recognition using deep\\nlearning on audio recordings,” in 2019 19th International Conference\\non Advances in ICT for Emerging Regions (ICTer), vol. 250.\\nIEEE,\\n2019, pp. 1–6.\\n[56] S. Latif, R. Rana, S. Khalifa, R. Jurdak, and B. W. Schuller, “Deep\\narchitecture enhancing robustness to noise, adversarial attacks, and\\ncross-corpus setting for speech emotion recognition,” arXiv preprint\\narXiv:2005.08453, 2020.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='architecture enhancing robustness to noise, adversarial attacks, and\\ncross-corpus setting for speech emotion recognition,” arXiv preprint\\narXiv:2005.08453, 2020.\\n[57] J. Wang, M. Xue, R. Culhane, E. Diao, J. Ding, and V. Tarokh, “Speech\\nemotion recognition with dual-sequence lstm architecture,” in ICASSP\\n2020-2020 IEEE International Conference on Acoustics, Speech and\\nSignal Processing (ICASSP).\\nIEEE, 2020, pp. 6474–6478.\\n[58] S. Li, X. Xing, W. Fan, B. Cai, P. Fordson, and X. Xu, “Spatiotem-\\nporal and frequential cascaded attention networks for speech emotion\\nrecognition,” Neurocomputing, vol. 448, pp. 238–248, 2021.\\n[59] S. Latif, R. Rana, S. Khalifa, R. Jurdak, J. Epps, and B. W. Schuller,\\n“Multi-task semi-supervised adversarial autoencoding for speech emo-\\ntion recognition,” IEEE Transactions on Affective Computing, vol. 13,\\nno. 2, pp. 992–1004, 2022.\\n[60] Y. Gao, J. Liu, L. Wang, and J. Dang, “Metric learning based feature\\nrepresentation with gated fusion model for speech emotion recognition.”', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='[60] Y. Gao, J. Liu, L. Wang, and J. Dang, “Metric learning based feature\\nrepresentation with gated fusion model for speech emotion recognition.”\\nin Interspeech, 2021, pp. 4503–4507.\\n[61] D. Dai, Z. Wu, R. Li, X. Wu, J. Jia, and H. Meng, “Learning discrimi-\\nnative features from spectrograms using center loss for speech emotion\\nrecognition,” in ICASSP 2019-2019 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP).\\nIEEE, 2019, pp.\\n7405–7409.\\n[62] T. Tuncer, S. Dogan, and U. R. Acharya, “Automated accurate speech\\nemotion recognition system using twine shuffle pattern and iterative\\nneighborhood component analysis techniques,” Knowledge-Based Sys-\\ntems, vol. 211, p. 106547, 2021.\\n[63] S. Zhong, B. Yu, and H. Zhang, “Exploration of an independent training\\nframework for speech emotion recognition,” IEEE Access, vol. 8, pp.\\n222 533–222 543, 2020.\\n[64] L. Kerkeni, Y. Serrestou, K. Raoof, M. Mbarki, M. A. Mahjoub, and\\nC. Cleder, “Automatic speech emotion recognition using an optimal', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='C. Cleder, “Automatic speech emotion recognition using an optimal\\ncombination of features based on emd-tkeo,” Speech Communication,\\nvol. 114, pp. 22–35, 2019.\\n[65] M. Hou, Z. Zhang, Q. Cao, D. Zhang, and G. Lu, “Multi-view speech\\nemotion recognition via collective relation construction,” IEEE/ACM\\nTransactions on Audio, Speech, and Language Processing, vol. 30, pp.\\n218–229, 2021.\\n[66] S.\\nMehta\\nand\\nM.\\nRastegari,\\n“Mobilevit:\\nlight-weight,\\ngeneral-\\npurpose,\\nand\\nmobile-friendly\\nvision\\ntransformer,”\\narXiv\\npreprint\\narXiv:2110.02178, 2021.\\n[67] Mehta, Sachin and Rastegari, Mohammad, “Separable self-attention for\\nmobile vision transformers,” arXiv preprint arXiv:2206.02680, 2022.\\n[68] S. N. Wadekar and A. Chaurasia, “Mobilevitv3: Mobile-friendly vision\\ntransformer with simple and effective fusion of local, global and input\\nfeatures,” arXiv preprint arXiv:2209.15159, 2022.\\nXiaoyu Tang (Member, IEEE) received the B.S.\\ndegree from South China Normal University in 2003', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='features,” arXiv preprint arXiv:2209.15159, 2022.\\nXiaoyu Tang (Member, IEEE) received the B.S.\\ndegree from South China Normal University in 2003\\nand the M.S. degree from Sun Yat-sen University\\nin 2011. He is currently pursuing the Ph.D. degree\\nwith South China Normal University. He is working\\nwith the School of Physics and Telecommunication\\nEngineering, South China Normal University, where\\nhe engaged in information system development. His\\nresearch interests include machine vision, intelligent\\ncontrol, and the Internet of Things. He is a member\\nof the IEEE ICICSP Technical Committee.\\nYixin Lin received the B.Eng. degree from the\\nSchool of Physics and Telecommunication Engi-\\nneering, South China Normal University, in 2021,\\nwhere he is currently pursuing the M.E. degree\\nwith the Department of Electronics and Information\\nEngineering. His research interests include artificial\\nintelligence and speech emotion recognition.\\nTing Dang is a Senior Research Scientist at Nokia\\nBell Labs, Cambridge, UK. Before joining Nokia,\\nshe worked as a Senior Research Associate at the\\nUniversity of Cambridge. Dr. Dang earned her Ph.D.\\ndegree from the University of New South Wales\\nin Sydney, Australia, and holds MEng and BEng\\ndegrees in Signal Processing from the Northwest-\\nern Polytechnical University in China. Her primary\\nresearch interests are on exploring the potential of\\naudio signals (e.g., speech) for mobile health, i.e.,\\nautomatic disease and mental state prediction and', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='ern Polytechnical University in China. Her primary\\nresearch interests are on exploring the potential of\\naudio signals (e.g., speech) for mobile health, i.e.,\\nautomatic disease and mental state prediction and\\nmonitoring (e.g., COVID-19, emotion) via mobile and wearable audio sensing.\\nHer work aims to develop generalised and interpretable machine learning\\nmodels to improve healthcare delivery. She served as the (senior) program\\ncommittee and invited reviewer for more than 30 top-tier conferences and\\njournals, such as AAAI, NeurIPS, ICASSP, IEEE TAC, IEEE TASL etc. She\\nhas won the Asian Dean’s Forum (ADF) Rising Star Women in Engineering\\nAward 2022, IEEE Early Career Writing Retreat Grant 2019 and ISCA Grant\\n2017.\\nYuanfang Zhang is currently principal researcher\\nat Autocity (Shenzhen) autonomous driving Co.,ltd.\\nHe got his dual Ph.D. degrees with the School\\nof Computer Science at Northwestern Polytechnical\\nUniversity, Shaanxi Province, China and Faculty\\nof Engineering and IT, University of Technology\\nSydney, Australia. His current research focuses on\\nunmanned vehicle sensing technology, multimodal\\nlearning methodology, comprehensive computer vi-\\nsion.\\nJOURNAL OF L\\nAT\\nEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n14\\nJintao Cheng received his bachelor’s degree from\\nthe school of Physics and Telecommunications En-\\ngineering, South China Normal University in 2021.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='14\\nJintao Cheng received his bachelor’s degree from\\nthe school of Physics and Telecommunications En-\\ngineering, South China Normal University in 2021.\\nHis research is computer vision, SLAM and deep\\nlearning.', metadata={'Published': '2024-03-07', 'Title': 'Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism', 'Authors': 'Xiaoyu Tang, Yixin Lin, Ting Dang, Yuanfang Zhang, Jintao Cheng', 'Summary': 'Speech Emotion Recognition (SER) is crucial in human-machine interactions.\\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\\nNetworks to learn local energy feature representations of speech segments from\\nspeech information, but struggle with capturing global information such as the\\nduration of energy in speech. Some use Transformers to capture global\\ninformation, but there is room for improvement in terms of parameter count and\\nperformance. Furthermore, existing attention mechanisms focus on spatial or\\nchannel dimensions, hindering learning of important temporal information in\\nspeech. In this paper, to model local and global information at different\\nlevels of granularity in speech and capture temporal, spatial and channel\\ndependencies in speech signals, we propose a Speech Emotion Recognition network\\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\\nin speech from a time-frequency perspective. In addition, a time-channel-space\\nattention mechanism is used to enhance features across three dimensions.\\nMoreover, we model local and global dependencies of feature sequences using\\nlarge convolutional kernels with depthwise separable convolutions and\\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\\nEmo-DB datasets and show our approach significantly improves the performance\\nover the state-of-the-art methods. Our code is available on\\nhttps://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism'}), Document(page_content='Vision Transformer: Vit and its Derivatives\\nZujun Fu\\nMay 25, 2022\\nAbstract\\nTransformer, an attention-based encoder-decoder architecture, has not only revolutionized the\\nﬁeld of natural language processing (NLP), but has also done some pioneering work in the ﬁeld\\nof computer vision (CV). Compared to convolutional neural networks (CNNs), the Vision Trans-\\nformer (ViT) relies on excellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the self-attention mecha-\\nnism in natural language processing, where word embeddings are replaced with patch embeddings.\\nThis paper reviews the derivatives in the ﬁeld of ViT and the cross-applications of ViT with\\nother ﬁelds.\\n1\\nPyramid Vision Transformer\\nTo overcome the quadratic complexity of the attention mechanism, the Pyramid Vision Trans-\\nformer (PVT) uses a variant of self-attention called Spatial-Reduced Attention (SRA). It is character-\\nized by spatial reduction of keys and values, similar to Linformer attention in the NLP ﬁeld.\\nBy applying SRA, the feature space dimension of the whole model is slowly reduced and the\\nconcept of order is enhanced by applying positional embedding in all transformer blocks.PVT has\\nbeen used as a backbone network for object detection and semantic segmentation to process high\\nresolution images.', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='concept of order is enhanced by applying positional embedding in all transformer blocks.PVT has\\nbeen used as a backbone network for object detection and semantic segmentation to process high\\nresolution images.\\nLater on, the research team further improved their PVT model named PVT-v2, with the following\\nmajor improvements.\\n• overlapping patch embedding\\n• convolutional feedforward networks\\n• linear-complexity self-attention layers.\\nFigure 1: Overall architecture of the proposed Pyramid Vision Transformer (PVT).\\n1\\narXiv:2205.11239v2  [cs.CV]  24 May 2022\\nFigure 2: PVT-v2.\\nFigure 3: Swin-transformer\\nOverlapping patches is a simple and general idea to improve ViT, especially for dense tasks\\n(e.g. semantic segmentation).By exploiting overlapping regions/patch, PVT-v2 can obtain more local\\ncontinuity of image representations.\\nConvolution between fully connected layers (FC) eliminates the need for ﬁxed size positional\\nencoding in each layer. The 3x3 deep convolution with zero padding (p=1) is designed to compensate\\nfor the removal of positional encoding from the model (they are still present, but only in the input).\\nThis process allows more ﬂexibility to handle multiple image resolutions.\\nFinally, using key and value pooling(p=7), the self-attentive layer is reduced to a complexity\\nsimilar to that of a CNN.\\n2\\nSwin Transformer: Hierarchical Vision Transformer using\\nShifted Windows', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='similar to that of a CNN.\\n2\\nSwin Transformer: Hierarchical Vision Transformer using\\nShifted Windows\\nSwin Transformer aims to build the idea of locality from the standard NLP transformer, i.e. local\\nor window attention:\\nIn the Swin Transformer, local self-attention is used for non-overlapping windows. The next layer\\nof window-to-window communication produces hierarchical representation by progressively merging\\nwindows.\\nAs shown in Figure 3, the left shows the regular window partitioning scheme in the ﬁrst layer,\\nwhere self-attention is computed within each window. The window partitioning in the second layer on\\nthe right is shifted by 2 image patches, resulting in crossing the boundary of the previous window.\\n2\\nFigure 4: local attention\\nFigure 5: scaling on jft data\\nThe local self-attention scales linearly with image size O(M ∗N) instead of O(N 2) in the window\\nsize used for sequence length N and M.\\nBy merging and adding many local layers, there is a global representation. In addition, the spatial\\ndimensions of the feature maps has been signiﬁcantly reduced. The authors claim to have achieved\\npromising results on both ImageNet-1K and ImageNet-21K.\\n3\\nScaling Vision Transformer\\nDeep learning and scale are related. In fact, scale is a key component in pushing the state-of-the-\\nart. In this study, the authors from Google Brain Research trained a slightly modiﬁed ViT model with', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='art. In this study, the authors from Google Brain Research trained a slightly modiﬁed ViT model with\\n2 billion parameters and achieved a top-1 accuracy of 90.45% on ImageNet. This over-parameterized\\ngeneralized model was tested on few-shot learning, with only 10 examples per class. A top-1 accuracy\\nof 84.86% was achieved on ImageNet.\\nFew-shot learning refers to ﬁne-tuning a model with an extremely limited number of samples. The\\ngoal of few-shot learning is to motivate generalization by slightly adapting the acquired pre-trained\\nknowledge to a speciﬁc task. If large models are successfully pre-trained, it makes sense to perform\\nwell with a very limited understanding of the downstream task (provided by only a few examples).\\nThe following are some of the core contributions and main results of this paper.\\n• Representation quality can be bottlenecked by model size, given that you have enough data to\\nfeed it;\\n• Large models beneﬁt from additional supervised data, even over 1B images.\\nFigure 5 depicts the eﬀect of switching from a 300M image dataset (JFT-300M) to 3 billion\\nimages (JFT-3B) without any further scaling.\\nBoth the medium (B/32) and large (L/16) models\\nbeneﬁt from adding data, roughly by a constant factor.\\nResults are obtained by few-shot(linear)\\nevaluation throughout the training process.\\n3\\nFigure 6: Weight decay decoupling eﬀect', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='Results are obtained by few-shot(linear)\\nevaluation throughout the training process.\\n3\\nFigure 6: Weight decay decoupling eﬀect\\n• Larger models are more sample eﬃcient, achieving the same level of error rate with fewer visible\\nimages.\\n• To save memory, they remove class tokens (cls). Instead, they evaluated global average pooling\\nand multi-head attention pooling to aggregate the representations of all patch tokens.\\n• They use diﬀerent weight decay for the head and the rest of the layers called ’body’. The authors\\ndemonstrate this well in the Figure 6. The box values are few-shot accuracy, while the horizontal\\nand vertical axes indicate the weight decay for the body and the head, respectively. Surprisingly,\\nthe stronger decay of the head produces the best results. The authors speculate that a strong\\nweight decay of the head leads to representations with a larger margin between classes.\\nThis is perhaps the most interesting ﬁnding that can be more widely applied to pre-training ViT.\\nThey used a warm-up phase at the beginning of training and a cool-down phase at the end\\nof training, where the learning rate linearly anneals to zero. In addition, they used the Adafactor\\noptimizer, which has a memory overhead of 50% compared to traditional Adam.\\n4\\nReplacing self-attention: independent token + channel mix-\\ning methods\\nIt is well known that self-attention can be used as an information routing mechanism with fast', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='4\\nReplacing self-attention: independent token + channel mix-\\ning methods\\nIt is well known that self-attention can be used as an information routing mechanism with fast\\nweights. So far, 3 papers tell the same story: replacing self-attention with 2 information mixing layers;\\none for mixing token (projected patch vector) and one for mixing channel/feature information.\\n4.1\\nMLP-Mixer\\nThe MLP-Mixer contains two MLP layers: the ﬁrst applied independently to the image patches\\n(i.e., ” mixing” the features at each location) and the other across the patches (i.e., ” mixing” the\\nspatial information).MLP Mixer architecture is shown in Figure 7.\\n4\\nFigure 7: MLP Mixer architecture\\nFigure 8: XCiT architecture\\n4.2\\nXCiT: Cross-Covariance Image Transformers\\nThe other is the recent architecture XCiT, which aims to modify the core building block of ViT:\\nself-attention applied to the token dimension.XCiT architecture is shown in Figure 8.\\nXCA: For information mixing, the authors propose a cross-covariance attention (XCA) function\\nthat operates on the feature dimension of a token rather than on its own. Importantly, this method is\\nonly applicable to the L2-normalized set of queries, keys, and values. the L2 norm is denoted by the', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='that operates on the feature dimension of a token rather than on its own. Importantly, this method is\\nonly applicable to the L2-normalized set of queries, keys, and values. the L2 norm is denoted by the\\nhat above the letters K and Q. The result of multiplication is also normalized to [-1,1] before softmax.\\nLocal Patch Interaction: To achieve explicit communication between the patches, the re-\\nsearchers added two depth-wise 3×3 convolutional layers with Batch Normalization and GELU non-\\nlinearity in between, as shown in Figure 9. Depth-wise convolution was applied to each channel (here\\nthe patch) independently.\\nFigure 9: depthwise convolutions\\n5\\nFigure 10: ConvMixer architecture\\nFigure 11: depthwise convolution with pointwise convolution\\n4.3\\nConvMixer\\nSelf-attention and MLP are theoretically more general modeling mechanisms, as they allow for\\nlarger receptive ﬁelds and content-aware behaviour. Nevertheless, the inductive bias of convolution\\nhas undeniable results in computer vision tasks.\\nMotivated by this, researchers have proposed another variant based on convolutional networks\\ncalled ConvMixer, as shown in Figure 10. the main idea is that it operates directly on the patches\\nas input, separating the mixing of spatial and channel dimensions and maintaining the same size and\\nresolution throughout the network.\\nMore speciﬁcally, depthwise convolution is responsible for mixing spatial locations, while pointwise', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='as input, separating the mixing of spatial and channel dimensions and maintaining the same size and\\nresolution throughout the network.\\nMore speciﬁcally, depthwise convolution is responsible for mixing spatial locations, while pointwise\\nconvolution (1x1x channel kernel) for mixing channel locations, as shown in the Figure 11.\\nMixing of distant spatial locations can be achieved by selecting a larger kernel size to create a\\nlarger receptive ﬁeld.\\n5\\nMultiscale Vision Transformers\\nThe CNN backbone architecture beneﬁts from the gradual increase of channels while reducing the\\nspatial dimension of the feature map. Similarly, the Multiscale Vision Transformer (MViT) exploits\\nthe idea of combining a multi-scale feature hierarchies with a Vision Transformer model. In practice,\\nthe authors start with an initial image size of 3 channels and gradually expand (hierarchically) the\\nchannel capacity while reducing the spatial resolution.\\nThus, a multi-scale feature pyramid is created, as shown in Figure 12. Intuitively, the early layers\\nwill learn high-spatial with simple low-level visual information, while the deeper layers are responsible\\nfor complex high-dimensional features.\\n6\\nFigure 12: Multi-scale Vit\\nFigure 13: Block-based and architecture-based / module-based space-time attention architectures for\\nvideo recognition\\n6\\nVideo classiﬁcation: Timesformer\\nAfter a successful image task, the Vision Transformer is applied to video recognition. Two archi-\\ntectures are presented here,as shown in Figure 13.\\n• Right: Reducing the architecture level.', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='After a successful image task, the Vision Transformer is applied to video recognition. Two archi-\\ntectures are presented here,as shown in Figure 13.\\n• Right: Reducing the architecture level.\\nThe proposed method applies a spatial Transformer\\nto the projection image patches and then has another network responsible for capturing time\\ncorrelations. This is similar to the winning strategy of CNN+LSTM based on video processing.\\n• Left: Space-time attention that can be implemented at the self-attention level, with the best\\ncombination in the red box. Attention is applied sequentially in the time domain by ﬁrst treating\\nthe image frames as tokens. Then, the combined space attention of the two spatial dimensions\\nis applied before the MLP projection. Figure 14 is the t-SNE visualization of the method.\\nIn Figure 14, each video is visualized as a point. Videos belonging to the same action category\\nhave the same color. A TimeSformer with split space-time attention learns more separable features\\nsemantically than a TimeSformer with only space attention or ViT.\\n7\\nFigure 14: Feature visualization with t-SNE of Timesformer\\nFigure 15: Segformer architecture\\n7\\nViT in semantic segmentation: SegFormer\\nNVIDIA has proposed a well-conﬁgured setup called SegFormer. SegFormer has an interesting\\ndesign component. First, it consists of a hierarchical Transformer encoder that outputs multi-scale\\nfeatures. Second, it does not require positional encoding, which can deteriorate performance when the\\ntest resolution is diﬀerent from the training.', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='features. Second, it does not require positional encoding, which can deteriorate performance when the\\ntest resolution is diﬀerent from the training.\\nSegFormer ,as shown in Figure 15, uses a very simple MLP decoder to aggregate the multi-scale\\nfeatures of the encoder. Contrary to ViT, SegFormer uses small image patches, such as 4 x 4, which are\\nknown to favor intensive prediction tasks. The proposed Transformer encoder outputs 1/4, 1/8, 1/16,\\n1/32 multi-scale features at the original image resolution. These multi-level features are provided to\\nthe MLP decoder to predict the segmentation mask.\\nMix-FFN in Figure 15: In order to mitigate the impact of positional encoding, the researchers\\nuse zero-padding 3 × 3 convolutional layers to leak location information.Mix-FFN can be expressed\\nas follows.\\nxout = MLP(GELU(Conv(MLP(xin)))) + xin\\nEﬃcient self-attention is proposed in PVT, which uses a reduction ratio to reduce the length of\\nthe sequence. The results can be measured qualitatively by visualizing the eﬀective receptive ﬁeld\\n(ERF) as shown in Figure 16.\\n8\\nFigure 16: SegFormer’s encoder naturally produces local attention, similar to the convolution of lower\\nstages, while being able to output highly non-local attention, eﬀectively capturing the context of Stage-', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='Figure 16: SegFormer’s encoder naturally produces local attention, similar to the convolution of lower\\nstages, while being able to output highly non-local attention, eﬀectively capturing the context of Stage-\\n4. As shown in the enlarged patch, the ERF in the MLP header (blue box) diﬀers from Stage-4 (red\\nbox) in that local attention is signiﬁcantly stronger in addition to non-local attention.\\nFigure 17: Unetr architecture\\n8\\nVision Transformers in Medical imaging: Unet + ViT =\\nUNETR\\nAlthough there are other attempts in medical imaging, UNETR provides the most convincing\\nresults. In this approach, ViT is applied to 3D medical image segmentation. It was shown that a\\nsimple adaptation is suﬃcient to improve the baselines for several 3D segmentation tasks.\\nEssentially, UNETR uses the Transformer as an encoder to learn the sequence representation of\\nthe input audio, as in Figure 17. Similar to the Unet model, it aims to eﬃciently capture global\\nmulti-scale information that can be passed to the decoder through long skip connections, forming skip\\nconnections at diﬀerent resolutions to compute the ﬁnal semantic segmentation output.\\n9\\nReferences\\n[1] Wang, W., Xie, E., Li, X., Fan, D. P., Song, K., Liang, D., ... & Shao, L. (2021). Pyramid', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='9\\nReferences\\n[1] Wang, W., Xie, E., Li, X., Fan, D. P., Song, K., Liang, D., ... & Shao, L. (2021). Pyramid\\nvision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint\\narXiv:2102.12122.\\n[2] Wang, S., Li, B. Z., Khabsa, M., Fang, H., & Ma, H. (2020). Linformer: Self-attention with linear\\ncomplexity. arXiv preprint arXiv:2006.04768.\\n[3] Wang, W., Xie, E., Li, X., Fan, D. P., Song, K., Liang, D., ... & Shao, L. (2021). Pvtv2: Improved\\nbaselines with pyramid vision transformer. arXiv preprint arXiv:2106.13797.\\n[4] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer:\\nHierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030.\\n[5] Zhai, X., Kolesnikov, A., Houlsby, N., & Beyer, L. (2021). Scaling vision transformers. arXiv\\npreprint arXiv:2106.04560.', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='preprint arXiv:2106.04560.\\n[6] Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., ... & Dosovitskiy,\\nA. (2021). Mlp-mixer: An all-mlp architecture for vision. arXiv preprint arXiv:2105.01601.\\n[7] El-Nouby, A., Touvron, H., Caron, M., Bojanowski, P., Douze, M., Joulin, A., ... & Jegou, H.\\n(2021). XCiT: Cross-Covariance Image Transformers. arXiv preprint arXiv:2106.09681.\\n[8] Patches Are All You Need? Anonymous ICLR 2021 submission\\n[9] Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., & Feichtenhofer, C. (2021). Multiscale\\nvision transformers. arXiv preprint arXiv:2104.11227.\\n[10] Bertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for Video\\nUnderstanding?. arXiv preprint arXiv:2102.05095.', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'}), Document(page_content='Understanding?. arXiv preprint arXiv:2102.05095.\\n[11] Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M., & Luo, P. (2021). SegFormer:\\nSimple and Eﬃcient Design for Semantic Segmentation with Transformers. arXiv preprint\\narXiv:2105.15203.\\n[12] Hatamizadeh, A., Yang, D., Roth, H., & Xu, D. (2021). Unetr: Transformers for 3d medical\\nimage segmentation. arXiv preprint arXiv:2103.10504.\\n10', metadata={'Published': '2022-05-24', 'Title': 'Vision Transformer: Vit and its Derivatives', 'Authors': 'Zujun Fu', 'Summary': 'Transformer, an attention-based encoder-decoder architecture, has not only\\nrevolutionized the field of natural language processing (NLP), but has also\\ndone some pioneering work in the field of computer vision (CV). Compared to\\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\\nexcellent modeling capabilities to achieve very good performance on several\\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\\nself-attention mechanism in natural language processing, where word embeddings\\nare replaced with patch embeddings.\\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\\nwith other fields.'})]\n"
     ]
    }
   ],
   "source": [
    "# Load the document pertaining to a particular topic\n",
    "docs = ArxivLoader(query=\"\"\" all:\"attention mechanisms\" AND (all:\"convolutional neural networks\" OR all:\"CNN\") AND NOT all:\"transformer\" \"\"\", load_max_docs=5).load()\n",
    "\n",
    "# Split the dpocument into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=350, chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunked_documents = text_splitter.split_documents(docs)\n",
    "print(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "5\n",
      "<class 'str'>\n",
      "<class 'list'>\n",
      "199\n",
      "page_content='1\\nPulmonary Disease Classiﬁcation Using Globally\\nCorrelated Maximum Likelihood:\\nan Auxiliary Attention mechanism for\\nConvolutional Neural Networks\\nEdward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, and Faraz Hussain\\nAbstract—Convolutional neural networks (CNN) are now being\\nwidely used for classiﬁying and detecting pulmonary abnormal-\\nities in chest radiographs. Two complementary generalization\\nproperties of CNNs, translation invariance and equivariance,\\nare particularly useful in detecting manifested abnormalities\\nassociated with pulmonary disease, regardless of their spatial\\nlocations within the image. However, these properties also come\\nwith the loss of exact spatial information and global relative\\npositions of abnormalities detected in local regions. Global\\nrelative positions of such abnormalities may help distinguish\\nsimilar conditions, such as COVID-19 and viral pneumonia. In\\nsuch instances, a global attention mechanism is needed, which\\nCNNs do not support in their traditional architectures that\\naim for generalization afforded by translation invariance and\\nequivariance. Vision Transformers provide a global attention\\nmechanism, but lack translation invariance and equivariance,\\nrequiring signiﬁcantly more training data samples to match\\ngeneralization of CNNs. To address the loss of spatial information\\nand global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that\\nserves as an auxiliary attention mechanism to existing CNN\\narchitectures, in order to extract global correlations between\\nsalient features.\\nImpact Statement—We improve sensitivity of Convolutional' metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}\n"
     ]
    }
   ],
   "source": [
    "print(type(docs))\n",
    "print(len(docs))\n",
    "print(type(docs[0].page_content))\n",
    "\n",
    "print(type(chunked_documents))\n",
    "print(len(chunked_documents))\n",
    "print(chunked_documents[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat is a small, domesticated carnivorous mammal that is commonly kept as a pet. They are known for their independent and curious nature, as well as their agility and ability to hunt. Cats have a wide variety of breeds, sizes, colors, and coat patterns. They are known for their sharp retractable claws, keen senses, including excellent night vision, and their ability to purr. Cats are also known for their grooming behavior, as they clean themselves by licking their fur.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "response = completion(\n",
    "    api_key=apikey,\n",
    "    base_url=\"https://drchat.xyz\",\n",
    "    model = \"gpt-3.5-turbo-16k\",\n",
    "    custom_llm_provider=\"openai\",\n",
    "    messages = [{ \"content\": \"What is a cat?\",\"role\": \"user\"}],\n",
    "    temperature=0.5\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response recieved\n",
      "all:\"high protein foods\" OR all:\"protein-rich foods\" AND (\"growth\" OR \"muscle growth\" OR \"body growth\")\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"Ask a research question!\")\n",
    "\n",
    "# Create multiple search queries\n",
    "search_split_prompt = f\"\"\"\n",
    "Your role is that of a researcher attempting to answer a question. Given a question from the user,\n",
    "your job is to come up with an ArXiv query that searches for the exact information needed to answer the question.\n",
    "\n",
    "You can include all syntax that involves including multiple terms, search by abstract, title, etc.\n",
    "\n",
    "Example 1:\n",
    "Given question: What are the ethical concerns associated with the use of facial recognition technology?\n",
    "Your Answer: (\"facial recognition technology\" OR \"facial recognition systems\" OR \"facial recognition software\") AND (\"ethical concerns\" OR \"ethical implications\" OR \"ethical issues\")\n",
    "\n",
    "Example 2:\n",
    "Given question: What are some prominent attention mechanisms for convolutional neural networks, and how are they used in the autonomous vehicle industry?\n",
    "Your Answer: all:\"attention mechanisms\" AND (\"convolutional neural networks\" OR \"CNN\") AND all:\"attention mechanisms\" AND (\"autonomous vehicles\" OR \"self-driving cars\")\n",
    "\n",
    "Question: {user_query},\n",
    "\n",
    "As shown above, your response should solely be an ArXiv Query, and nothing else.\n",
    "\n",
    "\"\"\"\n",
    "response = completion(\n",
    "    api_key=apikey,\n",
    "    base_url=\"https://drchat.xyz\",\n",
    "    model = \"gpt4-1106-preview\",\n",
    "    custom_llm_provider=\"openai\",\n",
    "    messages = [{ \"content\": search_split_prompt,\"role\": \"user\"}],\n",
    "    temperature=0.5\n",
    ")\n",
    "print(\"Response recieved\")\n",
    "print(response.choices[0].message.content)\n",
    "arxiv_queries_list = response.choices[0].message.content.split(\"|\")\n",
    "\n",
    "\n",
    "# Each element contains vector stores for each search query developed by LLM\n",
    "chunks_for_queries = []\n",
    "for q in arxiv_queries_list:\n",
    "    docs = ArxivLoader(query=q, load_max_docs=5).load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=350, chunk_overlap=50\n",
    "    )\n",
    "    chunked_documents = text_splitter.split_documents(docs)\n",
    "    chunks_for_queries.append(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(type(chunks_for_queries))\n",
    "print(len(chunks_for_queries[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m vectorstore_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m chunks_for_queries:\n\u001b[1;32m----> 6\u001b[0m     faiss_vectorstore \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m      7\u001b[0m         documents\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m      8\u001b[0m         embedding\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m      9\u001b[0m     )\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(faiss_vectorstore))\n\u001b[0;32m     11\u001b[0m     vectorstore_list\u001b[38;5;241m.\u001b[39mappend(faiss_vectorstore)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\vectorstores.py:550\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    549\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\vectorstores\\faiss.py:931\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    930\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m--> 931\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m    932\u001b[0m     texts,\n\u001b[0;32m    933\u001b[0m     embeddings,\n\u001b[0;32m    934\u001b[0m     embedding,\n\u001b[0;32m    935\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    936\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    938\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\vectorstores\\faiss.py:888\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[0;32m    885\u001b[0m     index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;66;03m# Default to L2, currently other metric types not initialized.\u001b[39;00m\n\u001b[1;32m--> 888\u001b[0m     index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m    889\u001b[0m docstore \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocstore\u001b[39m\u001b[38;5;124m\"\u001b[39m, InMemoryDocstore())\n\u001b[0;32m    890\u001b[0m index_to_docstore_id \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_to_docstore_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Instantiate the Embedding Model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=apikey, base_url=\"https://drchat.xyz\")\n",
    "# Create Index- Load document chunks into the vectorstore\n",
    "vectorstore_list = []\n",
    "for x in chunks_for_queries:\n",
    "    faiss_vectorstore = FAISS.from_documents(\n",
    "        documents=x,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    print(type(faiss_vectorstore))\n",
    "    vectorstore_list.append(faiss_vectorstore)\n",
    "\n",
    "\n",
    "# Create a retriver and retrieve relevant documents for each vector store\n",
    "relevant_documents_list = []\n",
    "for x in vectorstore_list:\n",
    "    \n",
    "    relevant_documents = x.similarity_search(user_query, k = 5)\n",
    "    print(type(relevant_documents))\n",
    "    relevant_documents_list.append(relevant_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Document(page_content='months to execute on tens and even hundreds of thousands of com-\\npute nodes with CPUs. TVM provides an opportunity to improve\\nthe performance of these dense matrix factorizations on GPUs and\\nAI accelerators. In this paper, we propose a new autotuning frame-\\nwork using Bayesian Optimization in ytopt [9, 10] and use the TVM\\ntensor expression language to implement linear algebra kernels\\nsuch as LU, Cholesky, and 3mm from PolyBench 4.2 [12]. We use\\nthese scientific kernels to evaluate the effectiveness of our methods\\non a GPU cluster, called Swing [8], at Argonne National Laboratory.\\nIn this paper, we make the following contributions:\\n• We propose a new autotuning framework for TVM-based\\nscientific tensor applications using Bayesian Optimization.\\n• We use TVM to implement scientific kernels such as LU,\\nCholesky, and 3mm.\\n• We evaluate the effectiveness of the proposed autotuning\\nframework and compare its performance with AutoTVM.\\nThe remainder of this paper is organized as follows. Section 2\\ndescribes the backgrounds about Apache TVM and ytopt. Section\\n3 proposes a new autotuning framework using ytopt. Section 4\\ndiscusses linear algebra benchmarks and their TVM TE implemen-\\ntations. Section 5 presents the experimental results and analyzes\\nand compares the performance. Section 6 summarizes this paper\\nand briefly discusses some future work.\\n2\\nBACKGROUNDS\\nIn this section, we briefly discuss some backgrounds about Apache\\nTVM and ytopt.\\n2.1', metadata={'Published': '2023-09-13', 'Title': 'Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization', 'Authors': 'Xingfu Wu, Praveen Paramasivam, Valerie Taylor', 'Summary': 'Apache TVM (Tensor Virtual Machine), an open source machine learning compiler\\nframework designed to optimize computations across various hardware platforms,\\nprovides an opportunity to improve the performance of dense matrix\\nfactorizations such as LU (Lower Upper) decomposition and Cholesky\\ndecomposition on GPUs and AI (Artificial Intelligence) accelerators. In this\\npaper, we propose a new TVM autotuning framework using Bayesian Optimization\\nand use the TVM tensor expression language to implement linear algebra kernels\\nsuch as LU, Cholesky, and 3mm. We use these scientific computation kernels to\\nevaluate the effectiveness of our methods on a GPU cluster, called Swing, at\\nArgonne National Laboratory. We compare the proposed autotuning framework with\\nthe TVM autotuning framework AutoTVM with four tuners and find that our\\nframework outperforms AutoTVM in most cases.'}), Document(page_content=\"expression in TVM. The schedule part is to use the provided schedule primitives\\nto map from a tensor expression to low-level code while preserving the logical\\nequivalence of the program.\\nm, n, h = tvm.var('m'), tvm.var('n'), tvm.var('h')\\nA = tvm.placeholder((m, h)), name='A')\\nB = tvm.placeholder((n, h)), name='B')\\nk = tvm.reduce_axis((0, h)), name='k')\\nC = tvm.compute((m,n), lambda y,x: tvm.sum(A[y,k]*B[x,k],axis=k))\\nListing 1: Example of tensor expression in TVM\\n4\\nApproaches\\nAs introduced in Section 3, we ﬁrst implemented the block-sparse matrix multi-\\nplications with TVM tensor expressions DSL to deﬁne the computational seman-\\ntics. And then, we used the schedule primitives of TVM to explore the schedule\\nspace of the block-sparse operations on CUDA. After that, we integrated Au-\\ntoTVM, the machine-learning-based optimizer of TVM, to automatically tune\\nthe parameters of our schedules, such as the tiling size of the loops. We im-\\nplemented and tested on AWS one g4dn.xlarge instance which contains one\\nNVIDIA T4 GPU. We denote the operation we are optimizing as the following:\", metadata={'Published': '2020-07-26', 'Title': 'Optimizing Block-Sparse Matrix Multiplications on CUDA with TVM', 'Authors': 'Zijing Gu', 'Summary': 'We implemented and optimized matrix multiplications between dense and\\nblock-sparse matrices on CUDA. We leveraged TVM, a deep learning compiler, to\\nexplore the schedule space of the operation and generate efficient CUDA code.\\nWith the automatic parameter tuning in TVM, our cross-thread reduction based\\nimplementation achieved competitive or better performance compared with other\\nstate-of-the-art frameworks.'}), Document(page_content='Analyzing Quantization in TVM\\nMingfei Guo(mfguo@stanford.edu)\\nAugust 23, 2023\\n1\\nIntroduction\\n1.1\\nBackground\\n1.1.1\\nQuantization\\nQuantization is a technique commonly used in deep learning frameworks to reduce the precision of\\nneural network weights and activations from 32-bit floating-point numbers to 8-bit integers. The goal\\nof quantization is to reduce computational requirements and decrease memory usage while maintaining\\nacceptable model accuracy.\\n1.1.2\\nTVM (Tensor Virtual Machine)\\nTVM is an open-source compiler stack inspired by Halide that optimizes and deploys deep learning\\nmodels on various hardware platforms. It aims to enable machine learning engineers to optimize and\\nrun computations efficiently on any hardware backend.\\nTVM comprises two optimization layers. The first layer focuses on computation graph optimization,\\naddressing high-level dataflow rewriting. The second layer, the tensor optimization layer, introduces\\nnew schedule primitives to optimize memory reuse across threads, leverage tensorized compute in-\\ntrinsics, and improve latency hiding techniques.\\nFor example, optimization schedules involve loop\\nreordering, axis splitting, cache read/write definitions, and more. These schedules enable developers\\nto fine-tune computation execution, harness hardware-specific optimizations, and eliminate the need\\nfor manual design.\\n1.2\\nThe Problem\\nThere has been many papers in academic literature on quantizing weight tensors in deep learning\\nmodels to reduce inference latency and memory footprint.\\nTVM also has the ability to quantize', metadata={'Published': '2023-08-19', 'Title': 'Analyzing Quantization in TVM', 'Authors': 'Mingfei Guo', 'Summary': 'There has been many papers in academic literature on quantizing weight\\ntensors in deep learning models to reduce inference latency and memory\\nfootprint. TVM also has the ability to quantize weights and support low-bit\\ncomputations. Although quantization is typically expected to improve inference\\ntime, in TVM, the performance of 8-bit quantization does not meet the\\nexpectations. Typically, when applying 8-bit quantization to a deep learning\\nmodel, it is usually expected to achieve around 50% of the full-precision\\ninference time. However, in this particular case, not only does the quantized\\nversion fail to achieve the desired performance boost, but it actually performs\\nworse, resulting in an inference time that is about 2 times as slow as the\\nnon-quantized version. In this project, we thoroughly investigate the reasons\\nbehind the underperformance and assess the compatibility and optimization\\nopportunities of 8-bit quantization in TVM. We discuss the optimization of two\\ndifferent types of tasks: computation-bound and memory-bound, and provide a\\ndetailed comparison of various optimization techniques in TVM. Through the\\nidentification of performance issues, we have successfully improved\\nquantization by addressing a bug in graph building. Furthermore, we analyze\\nmultiple optimization strategies to achieve the optimal quantization result.\\nThe best experiment achieves 163.88% improvement compared with the TVM compiled\\nbaseline in inference time for the compute-bound task and 194.98% for the\\nmemory-bound task.'}), Document(page_content='provides program transformation primitives that generate different\\nversions of the program with various optimizations, supports an\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nConference acronym ’XX, ,\\n© 2023 Association for Computing Machinery.\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nautomated program optimization framework AutoTVM [3, 13] to\\nfind optimized tensor operators, and provides a graph rewriter to\\ntake full advantage of high- and operator-level optimizations.\\nDense matrix factorizations, such as LU and Cholesky, are widely\\nused for scientific applications that require solving systems of lin-\\near equations, eigenvalues, and linear least squares problems. Such\\nreal-world scientific applications often take days, weeks, and even\\nmonths to execute on tens and even hundreds of thousands of com-\\npute nodes with CPUs. TVM provides an opportunity to improve\\nthe performance of these dense matrix factorizations on GPUs and', metadata={'Published': '2023-09-13', 'Title': 'Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization', 'Authors': 'Xingfu Wu, Praveen Paramasivam, Valerie Taylor', 'Summary': 'Apache TVM (Tensor Virtual Machine), an open source machine learning compiler\\nframework designed to optimize computations across various hardware platforms,\\nprovides an opportunity to improve the performance of dense matrix\\nfactorizations such as LU (Lower Upper) decomposition and Cholesky\\ndecomposition on GPUs and AI (Artificial Intelligence) accelerators. In this\\npaper, we propose a new TVM autotuning framework using Bayesian Optimization\\nand use the TVM tensor expression language to implement linear algebra kernels\\nsuch as LU, Cholesky, and 3mm. We use these scientific computation kernels to\\nevaluate the effectiveness of our methods on a GPU cluster, called Swing, at\\nArgonne National Laboratory. We compare the proposed autotuning framework with\\nthe TVM autotuning framework AutoTVM with four tuners and find that our\\nframework outperforms AutoTVM in most cases.'}), Document(page_content='ARMPL, the TVM routine is consistently better. An analysis of the results taking into account the\\noperands’ dimensions shows that the TVM routine delivers higher performance for “rectangular”\\ncases, with 𝑚in the range 100,352–1,605,632, and it is competitive when 𝑚=25,088. In contrast,\\nBLIS is better choice for “square” problems, with 𝑚in the range of 6,000.4\\n8.4.2\\nWhy is TVM better? The superiority of the TVM routine is rooted in the fact that, by\\n(automatically) generating the micro-kernels of different dimensions, we can easily explore the\\n4As a side note, the actual processing cost of the Resnet50 v1.5 model is concentrated in those cases where 𝑚is in the range\\n100,352–1,605,632 (47.8% of the total time), followed by 𝑚=25,088 (35.6% of the total time). In terms of absolute cost, this\\nimplies that the execution of all layers employing the TVM routine would require 39.1 s compared with 48.0 s when using\\nBLIS (and higher for OpenBLAS and ARMPL).\\n, Vol. 1, No. 1, Article . Publication date: November 2023.\\nAlgorithm XXX: Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM\\n25\\nspace and select the one that is better suited to a particular problem dimension. This is illustrated in', metadata={'Published': '2023-10-31', 'Title': 'Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM', 'Authors': 'Guillermo Alaejos, Adrián Castelló, Pedro Alonso-Jordá, Francisco D. Igual, Héctor Martínez, Enrique S. Quintana-Ortí', 'Summary': 'We explore the utilization of the Apache TVM open source framework to\\nautomatically generate a family of algorithms that follow the approach taken by\\npopular linear algebra libraries, such as GotoBLAS2, BLIS and OpenBLAS, in\\norder to obtain high-performance blocked formulations of the general matrix\\nmultiplication (GEMM). % In addition, we fully automatize the generation\\nprocess, by also leveraging the Apache TVM framework to derive a complete\\nvariety of the processor-specific micro-kernels for GEMM. This is in contrast\\nwith the convention in high performance libraries, which hand-encode a single\\nmicro-kernel per architecture using Assembly code. % In global, the combination\\nof our TVM-generated blocked algorithms and micro-kernels for GEMM 1)~improves\\nportability, maintainability and, globally, streamlines the software life\\ncycle; 2)~provides high flexibility to easily tailor and optimize the solution\\nto different data types, processor architectures, and matrix operand shapes,\\nyielding performance on a par (or even superior for specific matrix shapes)\\nwith that of hand-tuned libraries; and 3)~features a small memory footprint.'})]]\n"
     ]
    }
   ],
   "source": [
    "print(relevant_documents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVM is an open-source machine learning compiler framework designed to optimize computations across various hardware platforms.\n"
     ]
    }
   ],
   "source": [
    "question_prompt = f\"\"\"\n",
    "Given the following context: {relevant_documents_list}\n",
    "\n",
    "Answer the following question: {user_query}\n",
    "\n",
    "Only answer the question if the answer is in the context. Otherwise, say that you don't know.\n",
    "\"\"\"\n",
    "\n",
    "response = completion(\n",
    "    api_key=apikey,\n",
    "    base_url=\"https://drchat.xyz\",\n",
    "    model = \"gpt-3.5-turbo-16k\",\n",
    "    custom_llm_provider=\"openai\",\n",
    "    messages = [{ \"content\": question_prompt,\"role\": \"user\"}],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
