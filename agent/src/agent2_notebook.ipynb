{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk--lxq9ReUmWxgxJhWPPwRNg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import litellm\n",
    "from litellm import completion\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load API keys\n",
    "load_dotenv(\".env\")\n",
    "apikey = os.getenv('OPENAI_API_KEY')\n",
    "print(apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Request Successful\n",
      "Response: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3ARetrieval%20Augmented%20Generation%26id_list%3D%26start%3D0%26max_results%3D5\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=all:Retrieval Augmented Generation&amp;id_list=&amp;start=0&amp;max_results=5</title>\n",
      "  <id>http://arxiv.org/api/UOKvU43HASfzk48jVrFyBsmll1A</id>\n",
      "  <updated>2024-05-23T00:00:00-04:00</updat\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Example query to the arXiv API directly\n",
    "url = \"http://export.arxiv.org/api/query\"\n",
    "params = {\n",
    "    \"search_query\": \"all:Retrieval Augmented Generation\",\n",
    "    \"start\": 0,\n",
    "    \"max_results\": 5\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"API Request Successful\")\n",
    "    print(\"Response:\", response.text[:500])  # Print first 500 characters of the response\n",
    "else:\n",
    "    print(\"Failed to fetch data from arXiv:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document pertaining to a particular topic\n",
    "docs = ArxivLoader(query=\"\"\" all:\"attention mechanisms\" AND (all:\"convolutional neural networks\" OR all:\"CNN\") AND NOT all:\"transformer\" \"\"\", load_max_docs=5).load()\n",
    "\n",
    "# Split the dpocument into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=350, chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunked_documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "5\n",
      "<class 'str'>\n",
      "<class 'list'>\n",
      "199\n",
      "page_content='1\\nPulmonary Disease ClassiÔ¨Åcation Using Globally\\nCorrelated Maximum Likelihood:\\nan Auxiliary Attention mechanism for\\nConvolutional Neural Networks\\nEdward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, and Faraz Hussain\\nAbstract‚ÄîConvolutional neural networks (CNN) are now being\\nwidely used for classiÔ¨Åying and detecting pulmonary abnormal-\\nities in chest radiographs. Two complementary generalization\\nproperties of CNNs, translation invariance and equivariance,\\nare particularly useful in detecting manifested abnormalities\\nassociated with pulmonary disease, regardless of their spatial\\nlocations within the image. However, these properties also come\\nwith the loss of exact spatial information and global relative\\npositions of abnormalities detected in local regions. Global\\nrelative positions of such abnormalities may help distinguish\\nsimilar conditions, such as COVID-19 and viral pneumonia. In\\nsuch instances, a global attention mechanism is needed, which\\nCNNs do not support in their traditional architectures that\\naim for generalization afforded by translation invariance and\\nequivariance. Vision Transformers provide a global attention\\nmechanism, but lack translation invariance and equivariance,\\nrequiring signiÔ¨Åcantly more training data samples to match\\ngeneralization of CNNs. To address the loss of spatial information\\nand global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that\\nserves as an auxiliary attention mechanism to existing CNN\\narchitectures, in order to extract global correlations between\\nsalient features.\\nImpact Statement‚ÄîWe improve sensitivity of Convolutional' metadata={'Published': '2021-09-01', 'Title': 'Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks', 'Authors': 'Edward Verenich, Tobias Martin, Alvaro Velasquez, Nazar Khan, Faraz Hussain', 'Summary': 'Convolutional neural networks (CNN) are now being widely used for classifying\\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\\ngeneralization properties of CNNs, translation invariance and equivariance, are\\nparticularly useful in detecting manifested abnormalities associated with\\npulmonary disease, regardless of their spatial locations within the image.\\nHowever, these properties also come with the loss of exact spatial information\\nand global relative positions of abnormalities detected in local regions.\\nGlobal relative positions of such abnormalities may help distinguish similar\\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\\nattention mechanism is needed, which CNNs do not support in their traditional\\narchitectures that aim for generalization afforded by translation invariance\\nand equivariance. Vision Transformers provide a global attention mechanism, but\\nlack translation invariance and equivariance, requiring significantly more\\ntraining data samples to match generalization of CNNs. To address the loss of\\nspatial information and global relations between features, while preserving the\\ninductive biases of CNNs, we present a novel technique that serves as an\\nauxiliary attention mechanism to existing CNN architectures, in order to\\nextract global correlations between salient features.'}\n"
     ]
    }
   ],
   "source": [
    "print(type(docs))\n",
    "print(len(docs))\n",
    "print(type(docs[0].page_content))\n",
    "\n",
    "print(type(chunked_documents))\n",
    "print(len(chunked_documents))\n",
    "print(chunked_documents[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat is a small carnivorous mammal that is often kept as a pet. It belongs to the Felidae family and is known for its furry coat, retractable claws, and sharp teeth. Cats are known for their agility, flexibility, and hunting skills. They come in various breeds and sizes, with domestic cats being the most common. Cats are popular pets due to their independent nature, companionship, and ability to catch pests.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "response = completion(\n",
    "    api_key=apikey,\n",
    "    base_url=\"https://drchat.xyz\",\n",
    "    model = \"gpt-3.5-turbo-16k\",\n",
    "    custom_llm_provider=\"openai\",\n",
    "    messages = [{ \"content\": \"What is a cat?\",\"role\": \"user\"}],\n",
    "    temperature=0.5\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response recieved\n",
      "title:(\"TVM\" OR \"tensor virtual machine\") OR abstract:(\"TVM\" OR \"tensor virtual machine\")\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"Ask a research question!\")\n",
    "\n",
    "# Create multiple search queries\n",
    "search_split_prompt = f\"\"\"\n",
    "Your role is that of a researcher attempting to answer a question. Given a question from the user,\n",
    "your job is to come up with an ArXiv query that searches for the exact information needed to answer the question.\n",
    "\n",
    "You can include all syntax that involves including multiple terms, search by abstract, title, etc.\n",
    "\n",
    "Example 1:\n",
    "Given question: What are the ethical concerns associated with the use of facial recognition technology?\n",
    "Your Answer: (\"facial recognition technology\" OR \"facial recognition systems\" OR \"facial recognition software\") AND (\"ethical concerns\" OR \"ethical implications\" OR \"ethical issues\")\n",
    "\n",
    "Example 2:\n",
    "Given question: What are some prominent attention mechanisms for convolutional neural networks, and how are they used in the autonomous vehicle industry?\n",
    "Your Answer: all:\"attention mechanisms\" AND (\"convolutional neural networks\" OR \"CNN\") AND all:\"attention mechanisms\" AND (\"autonomous vehicles\" OR \"self-driving cars\")\n",
    "\n",
    "Question: {user_query},\n",
    "\n",
    "As shown above, your response should solely be an ArXiv Query, and nothing else.\n",
    "\n",
    "\"\"\"\n",
    "response = completion(\n",
    "    api_key=apikey,\n",
    "    base_url=\"https://drchat.xyz\",\n",
    "    model = \"gpt4-1106-preview\",\n",
    "    custom_llm_provider=\"openai\",\n",
    "    messages = [{ \"content\": search_split_prompt,\"role\": \"user\"}],\n",
    "    temperature=0.5\n",
    ")\n",
    "print(\"Response recieved\")\n",
    "print(response.choices[0].message.content)\n",
    "arxiv_queries_list = response.choices[0].message.content.split(\"|\")\n",
    "\n",
    "\n",
    "# Each element contains vector stores for each search query developed by LLM\n",
    "chunks_for_queries = []\n",
    "for q in arxiv_queries_list:\n",
    "    docs = ArxivLoader(query=q, load_max_docs=5).load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=350, chunk_overlap=50\n",
    "    )\n",
    "    chunked_documents = text_splitter.split_documents(docs)\n",
    "    chunks_for_queries.append(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "247\n"
     ]
    }
   ],
   "source": [
    "print(type(chunks_for_queries))\n",
    "print(len(chunks_for_queries[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.vectorstores.faiss.FAISS'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Embedding Model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=apikey, base_url=\"https://drchat.xyz\")\n",
    "# Create Index- Load document chunks into the vectorstore\n",
    "vectorstore_list = []\n",
    "for x in chunks_for_queries:\n",
    "    faiss_vectorstore = FAISS.from_documents(\n",
    "        documents=x,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    print(type(faiss_vectorstore))\n",
    "    vectorstore_list.append(faiss_vectorstore)\n",
    "\n",
    "\n",
    "# Create a retriver and retrieve relevant documents for each vector store\n",
    "relevant_documents_list = []\n",
    "for x in vectorstore_list:\n",
    "    \n",
    "    relevant_documents = x.similarity_search(user_query, k = 5)\n",
    "    print(type(relevant_documents))\n",
    "    relevant_documents_list.append(relevant_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Document(page_content='months to execute on tens and even hundreds of thousands of com-\\npute nodes with CPUs. TVM provides an opportunity to improve\\nthe performance of these dense matrix factorizations on GPUs and\\nAI accelerators. In this paper, we propose a new autotuning frame-\\nwork using Bayesian Optimization in ytopt [9, 10] and use the TVM\\ntensor expression language to implement linear algebra kernels\\nsuch as LU, Cholesky, and 3mm from PolyBench 4.2 [12]. We use\\nthese scientific kernels to evaluate the effectiveness of our methods\\non a GPU cluster, called Swing [8], at Argonne National Laboratory.\\nIn this paper, we make the following contributions:\\n‚Ä¢ We propose a new autotuning framework for TVM-based\\nscientific tensor applications using Bayesian Optimization.\\n‚Ä¢ We use TVM to implement scientific kernels such as LU,\\nCholesky, and 3mm.\\n‚Ä¢ We evaluate the effectiveness of the proposed autotuning\\nframework and compare its performance with AutoTVM.\\nThe remainder of this paper is organized as follows. Section 2\\ndescribes the backgrounds about Apache TVM and ytopt. Section\\n3 proposes a new autotuning framework using ytopt. Section 4\\ndiscusses linear algebra benchmarks and their TVM TE implemen-\\ntations. Section 5 presents the experimental results and analyzes\\nand compares the performance. Section 6 summarizes this paper\\nand briefly discusses some future work.\\n2\\nBACKGROUNDS\\nIn this section, we briefly discuss some backgrounds about Apache\\nTVM and ytopt.\\n2.1', metadata={'Published': '2023-09-13', 'Title': 'Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization', 'Authors': 'Xingfu Wu, Praveen Paramasivam, Valerie Taylor', 'Summary': 'Apache TVM (Tensor Virtual Machine), an open source machine learning compiler\\nframework designed to optimize computations across various hardware platforms,\\nprovides an opportunity to improve the performance of dense matrix\\nfactorizations such as LU (Lower Upper) decomposition and Cholesky\\ndecomposition on GPUs and AI (Artificial Intelligence) accelerators. In this\\npaper, we propose a new TVM autotuning framework using Bayesian Optimization\\nand use the TVM tensor expression language to implement linear algebra kernels\\nsuch as LU, Cholesky, and 3mm. We use these scientific computation kernels to\\nevaluate the effectiveness of our methods on a GPU cluster, called Swing, at\\nArgonne National Laboratory. We compare the proposed autotuning framework with\\nthe TVM autotuning framework AutoTVM with four tuners and find that our\\nframework outperforms AutoTVM in most cases.'}), Document(page_content=\"expression in TVM. The schedule part is to use the provided schedule primitives\\nto map from a tensor expression to low-level code while preserving the logical\\nequivalence of the program.\\nm, n, h = tvm.var('m'), tvm.var('n'), tvm.var('h')\\nA = tvm.placeholder((m, h)), name='A')\\nB = tvm.placeholder((n, h)), name='B')\\nk = tvm.reduce_axis((0, h)), name='k')\\nC = tvm.compute((m,n), lambda y,x: tvm.sum(A[y,k]*B[x,k],axis=k))\\nListing 1: Example of tensor expression in TVM\\n4\\nApproaches\\nAs introduced in Section 3, we Ô¨Årst implemented the block-sparse matrix multi-\\nplications with TVM tensor expressions DSL to deÔ¨Åne the computational seman-\\ntics. And then, we used the schedule primitives of TVM to explore the schedule\\nspace of the block-sparse operations on CUDA. After that, we integrated Au-\\ntoTVM, the machine-learning-based optimizer of TVM, to automatically tune\\nthe parameters of our schedules, such as the tiling size of the loops. We im-\\nplemented and tested on AWS one g4dn.xlarge instance which contains one\\nNVIDIA T4 GPU. We denote the operation we are optimizing as the following:\", metadata={'Published': '2020-07-26', 'Title': 'Optimizing Block-Sparse Matrix Multiplications on CUDA with TVM', 'Authors': 'Zijing Gu', 'Summary': 'We implemented and optimized matrix multiplications between dense and\\nblock-sparse matrices on CUDA. We leveraged TVM, a deep learning compiler, to\\nexplore the schedule space of the operation and generate efficient CUDA code.\\nWith the automatic parameter tuning in TVM, our cross-thread reduction based\\nimplementation achieved competitive or better performance compared with other\\nstate-of-the-art frameworks.'}), Document(page_content='Analyzing Quantization in TVM\\nMingfei Guo(mfguo@stanford.edu)\\nAugust 23, 2023\\n1\\nIntroduction\\n1.1\\nBackground\\n1.1.1\\nQuantization\\nQuantization is a technique commonly used in deep learning frameworks to reduce the precision of\\nneural network weights and activations from 32-bit floating-point numbers to 8-bit integers. The goal\\nof quantization is to reduce computational requirements and decrease memory usage while maintaining\\nacceptable model accuracy.\\n1.1.2\\nTVM (Tensor Virtual Machine)\\nTVM is an open-source compiler stack inspired by Halide that optimizes and deploys deep learning\\nmodels on various hardware platforms. It aims to enable machine learning engineers to optimize and\\nrun computations efficiently on any hardware backend.\\nTVM comprises two optimization layers. The first layer focuses on computation graph optimization,\\naddressing high-level dataflow rewriting. The second layer, the tensor optimization layer, introduces\\nnew schedule primitives to optimize memory reuse across threads, leverage tensorized compute in-\\ntrinsics, and improve latency hiding techniques.\\nFor example, optimization schedules involve loop\\nreordering, axis splitting, cache read/write definitions, and more. These schedules enable developers\\nto fine-tune computation execution, harness hardware-specific optimizations, and eliminate the need\\nfor manual design.\\n1.2\\nThe Problem\\nThere has been many papers in academic literature on quantizing weight tensors in deep learning\\nmodels to reduce inference latency and memory footprint.\\nTVM also has the ability to quantize', metadata={'Published': '2023-08-19', 'Title': 'Analyzing Quantization in TVM', 'Authors': 'Mingfei Guo', 'Summary': 'There has been many papers in academic literature on quantizing weight\\ntensors in deep learning models to reduce inference latency and memory\\nfootprint. TVM also has the ability to quantize weights and support low-bit\\ncomputations. Although quantization is typically expected to improve inference\\ntime, in TVM, the performance of 8-bit quantization does not meet the\\nexpectations. Typically, when applying 8-bit quantization to a deep learning\\nmodel, it is usually expected to achieve around 50% of the full-precision\\ninference time. However, in this particular case, not only does the quantized\\nversion fail to achieve the desired performance boost, but it actually performs\\nworse, resulting in an inference time that is about 2 times as slow as the\\nnon-quantized version. In this project, we thoroughly investigate the reasons\\nbehind the underperformance and assess the compatibility and optimization\\nopportunities of 8-bit quantization in TVM. We discuss the optimization of two\\ndifferent types of tasks: computation-bound and memory-bound, and provide a\\ndetailed comparison of various optimization techniques in TVM. Through the\\nidentification of performance issues, we have successfully improved\\nquantization by addressing a bug in graph building. Furthermore, we analyze\\nmultiple optimization strategies to achieve the optimal quantization result.\\nThe best experiment achieves 163.88% improvement compared with the TVM compiled\\nbaseline in inference time for the compute-bound task and 194.98% for the\\nmemory-bound task.'}), Document(page_content='provides program transformation primitives that generate different\\nversions of the program with various optimizations, supports an\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nConference acronym ‚ÄôXX, ,\\n¬© 2023 Association for Computing Machinery.\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nautomated program optimization framework AutoTVM [3, 13] to\\nfind optimized tensor operators, and provides a graph rewriter to\\ntake full advantage of high- and operator-level optimizations.\\nDense matrix factorizations, such as LU and Cholesky, are widely\\nused for scientific applications that require solving systems of lin-\\near equations, eigenvalues, and linear least squares problems. Such\\nreal-world scientific applications often take days, weeks, and even\\nmonths to execute on tens and even hundreds of thousands of com-\\npute nodes with CPUs. TVM provides an opportunity to improve\\nthe performance of these dense matrix factorizations on GPUs and', metadata={'Published': '2023-09-13', 'Title': 'Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization', 'Authors': 'Xingfu Wu, Praveen Paramasivam, Valerie Taylor', 'Summary': 'Apache TVM (Tensor Virtual Machine), an open source machine learning compiler\\nframework designed to optimize computations across various hardware platforms,\\nprovides an opportunity to improve the performance of dense matrix\\nfactorizations such as LU (Lower Upper) decomposition and Cholesky\\ndecomposition on GPUs and AI (Artificial Intelligence) accelerators. In this\\npaper, we propose a new TVM autotuning framework using Bayesian Optimization\\nand use the TVM tensor expression language to implement linear algebra kernels\\nsuch as LU, Cholesky, and 3mm. We use these scientific computation kernels to\\nevaluate the effectiveness of our methods on a GPU cluster, called Swing, at\\nArgonne National Laboratory. We compare the proposed autotuning framework with\\nthe TVM autotuning framework AutoTVM with four tuners and find that our\\nframework outperforms AutoTVM in most cases.'}), Document(page_content='ARMPL, the TVM routine is consistently better. An analysis of the results taking into account the\\noperands‚Äô dimensions shows that the TVM routine delivers higher performance for ‚Äúrectangular‚Äù\\ncases, with ùëöin the range 100,352‚Äì1,605,632, and it is competitive when ùëö=25,088. In contrast,\\nBLIS is better choice for ‚Äúsquare‚Äù problems, with ùëöin the range of 6,000.4\\n8.4.2\\nWhy is TVM better? The superiority of the TVM routine is rooted in the fact that, by\\n(automatically) generating the micro-kernels of different dimensions, we can easily explore the\\n4As a side note, the actual processing cost of the Resnet50 v1.5 model is concentrated in those cases where ùëöis in the range\\n100,352‚Äì1,605,632 (47.8% of the total time), followed by ùëö=25,088 (35.6% of the total time). In terms of absolute cost, this\\nimplies that the execution of all layers employing the TVM routine would require 39.1 s compared with 48.0 s when using\\nBLIS (and higher for OpenBLAS and ARMPL).\\n, Vol. 1, No. 1, Article . Publication date: November 2023.\\nAlgorithm XXX: Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM\\n25\\nspace and select the one that is better suited to a particular problem dimension. This is illustrated in', metadata={'Published': '2023-10-31', 'Title': 'Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM', 'Authors': 'Guillermo Alaejos, Adri√°n Castell√≥, Pedro Alonso-Jord√°, Francisco D. Igual, H√©ctor Mart√≠nez, Enrique S. Quintana-Ort√≠', 'Summary': 'We explore the utilization of the Apache TVM open source framework to\\nautomatically generate a family of algorithms that follow the approach taken by\\npopular linear algebra libraries, such as GotoBLAS2, BLIS and OpenBLAS, in\\norder to obtain high-performance blocked formulations of the general matrix\\nmultiplication (GEMM). % In addition, we fully automatize the generation\\nprocess, by also leveraging the Apache TVM framework to derive a complete\\nvariety of the processor-specific micro-kernels for GEMM. This is in contrast\\nwith the convention in high performance libraries, which hand-encode a single\\nmicro-kernel per architecture using Assembly code. % In global, the combination\\nof our TVM-generated blocked algorithms and micro-kernels for GEMM 1)~improves\\nportability, maintainability and, globally, streamlines the software life\\ncycle; 2)~provides high flexibility to easily tailor and optimize the solution\\nto different data types, processor architectures, and matrix operand shapes,\\nyielding performance on a par (or even superior for specific matrix shapes)\\nwith that of hand-tuned libraries; and 3)~features a small memory footprint.'})]]\n"
     ]
    }
   ],
   "source": [
    "print(relevant_documents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVM stands for Tensor Virtual Machine. It is an open-source machine learning compiler framework designed to optimize computations across various hardware platforms. It provides an opportunity to improve the performance of dense matrix factorizations on GPUs and AI accelerators.\n"
     ]
    }
   ],
   "source": [
    "question_prompt = f\"\"\"\n",
    "Given the following context: {relevant_documents_list}\n",
    "\n",
    "Answer the following question: {user_query}\n",
    "\n",
    "Only answer the question if the answer is in the context. Otherwise, say that you don't know.\n",
    "\"\"\"\n",
    "\n",
    "response = completion(\n",
    "    api_key=apikey,\n",
    "    base_url=\"https://drchat.xyz\",\n",
    "    model = \"gpt-3.5-turbo-16k\",\n",
    "    custom_llm_provider=\"openai\",\n",
    "    messages = [{ \"content\": question_prompt,\"role\": \"user\"}],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
